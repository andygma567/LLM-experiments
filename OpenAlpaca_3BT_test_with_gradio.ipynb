{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "mount_file_id": "1wYq8QIIs_kVEX4Lo39t-uC5d5HP9kl8B",
      "authorship_tag": "ABX9TyOgOAvAwMlbqTJMx1wAq4DQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d402d2cf5cb410a910d7f63d6e238d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c41f44d9f1f4b12a9c8b052c76838f2",
              "IPY_MODEL_2a925df188c144f0b84d9cef07df597e",
              "IPY_MODEL_3606935955cd4101930fbf60f6fa2623"
            ],
            "layout": "IPY_MODEL_7182ab460ad3434196d7f8da7f22bb4c"
          }
        },
        "2c41f44d9f1f4b12a9c8b052c76838f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f996419900b24b71bb8e2688fa839eb2",
            "placeholder": "​",
            "style": "IPY_MODEL_4a62d44104de43ebbd000ce90632e434",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2a925df188c144f0b84d9cef07df597e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e731543074874f55b31585dedf5a52fa",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96cb4789337c40908401336c74facdd2",
            "value": 6
          }
        },
        "3606935955cd4101930fbf60f6fa2623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99fbbaf0796840eeab2a0202fd240523",
            "placeholder": "​",
            "style": "IPY_MODEL_71a2747163ee41e5bd35d844d83bb42b",
            "value": " 6/6 [09:59&lt;00:00, 89.69s/it]"
          }
        },
        "7182ab460ad3434196d7f8da7f22bb4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f996419900b24b71bb8e2688fa839eb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a62d44104de43ebbd000ce90632e434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e731543074874f55b31585dedf5a52fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96cb4789337c40908401336c74facdd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99fbbaf0796840eeab2a0202fd240523": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71a2747163ee41e5bd35d844d83bb42b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andygma567/LLM-experiments/blob/main/OpenAlpaca_3BT_test_with_gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This works kind of. It needs tuning or better prompting.\n",
        "\n",
        "High RAM:\n",
        "CPU 25 GB, GPU 16 GB, 5.45 credits/hour\n",
        "\n",
        "Standard RAM:\n",
        "CPU 12 GB, GPU 16 GB, 5.36 credits/hour\n",
        "\n"
      ],
      "metadata": {
        "id": "wI0QTX23OZJx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEWdKDp2oJjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ae2b561-0546-4e8c-ef85-6fa4b8cf1c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.1/288.1 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate\n",
        "# datasets\n",
        "!pip install -q gradio\n",
        "!pip install -q SentencePiece # required by Llama\n",
        "!pip install -q xformers # required to use pipelines with OpenAlpaca"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import infer_auto_device_map, init_empty_weights\n",
        "from transformers import LlamaConfig, LlamaForCausalLM, LlamaTokenizer, LlamaModel\n",
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# I want to time how long it takes to load everything\n",
        "from timeit import default_timer as timer"
      ],
      "metadata": {
        "id": "6F-cTr9PHmY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check the device map"
      ],
      "metadata": {
        "id": "-P4TGRhn-rav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # the previewed version of OpenAlpaca\n",
        "drive_checkpoint = r'/content/drive/MyDrive/openalpaca_7b_preview_3bt'\n",
        "HF_checkpoint = \"openllmplayground/openalpaca_7b_preview_3bt\"\n",
        "\n",
        "# config = AutoConfig.from_pretrained(drive_checkpoint)\n",
        "# # config = LlamaConfig()\n",
        "\n",
        "# with init_empty_weights():\n",
        "#     model = AutoModelForCausalLM.from_config(config)\n",
        "\n",
        "# # I think this is the method called by device_map='auto'\n",
        "# my_drive_device_map = infer_auto_device_map(model)"
      ],
      "metadata": {
        "id": "Fyh7DFwi-tOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config = AutoConfig.from_pretrained(HF_checkpoint)\n",
        "\n",
        "# with init_empty_weights():\n",
        "#     model = AutoModelForCausalLM.from_config(config)\n",
        "\n",
        "# hf_hub_device_map = infer_auto_device_map(model)"
      ],
      "metadata": {
        "id": "LF_qa6QfAav-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(my_drive_device_map == hf_hub_device_map)"
      ],
      "metadata": {
        "id": "XN53y28_XB4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An attempt at loading a model"
      ],
      "metadata": {
        "id": "j3YTOrGaI_Gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the tokenizer"
      ],
      "metadata": {
        "id": "3tlzeF89Cwyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes 5.21 minutes to load from drive\n",
        "# Loading from online takes 3.66 minutes\n",
        "start = timer()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    drive_checkpoint,\n",
        "    use_fast=True,\n",
        "    )\n",
        "# I tried testing this but it didn't work for me\n",
        "# tokenizer.bos_token_id, tokenizer.eos_token_id = 1,2 # see https://github.com/openlm-research/open_llama#preview-weights-release-and-usage\n",
        "\n",
        "end = timer()\n",
        "minutes = (end - start)/60\n",
        "print(f'Elapsed minutes: {minutes:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4YW8ByxU6rc",
        "outputId": "3fd6b363-3a41-4b12-ebc0-7c19eb94dcb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed minutes: 5.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model"
      ],
      "metadata": {
        "id": "rECxKcFvCy3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes 6 min to load the model from my drive\n",
        "start = timer()\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    drive_checkpoint,\n",
        "    device_map=\"auto\",\n",
        "    offload_folder=\"offload\",\n",
        "    offload_state_dict = True,\n",
        "    torch_dtype=torch.float16, # I don't think every Llama model supports half precision\n",
        "    )\n",
        "\n",
        "end = timer()\n",
        "minutes = (end - start)/60\n",
        "print(f'Elapsed minutes: {minutes:.2f}')"
      ],
      "metadata": {
        "id": "tKSgqwkxadD6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "3d402d2cf5cb410a910d7f63d6e238d5",
            "2c41f44d9f1f4b12a9c8b052c76838f2",
            "2a925df188c144f0b84d9cef07df597e",
            "3606935955cd4101930fbf60f6fa2623",
            "7182ab460ad3434196d7f8da7f22bb4c",
            "f996419900b24b71bb8e2688fa839eb2",
            "4a62d44104de43ebbd000ce90632e434",
            "e731543074874f55b31585dedf5a52fa",
            "96cb4789337c40908401336c74facdd2",
            "99fbbaf0796840eeab2a0202fd240523",
            "71a2747163ee41e5bd35d844d83bb42b"
          ]
        },
        "outputId": "5c19ce01-fcb6-4dcf-d9e8-ec59626c1615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d402d2cf5cb410a910d7f63d6e238d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed minutes: 10.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = r'What is an alpaca? How is it different from a llama?'\n",
        "prompt_no_input = f'### Instruction:\\n{input}\\n\\n### Response:'\n",
        "tokens = tokenizer(prompt_no_input)\n",
        "\n",
        "# Do I need to specify the return_tensors as pt?\n",
        "# I think so\n",
        "# The model can only accept tensors and if not specified then a dictionary (of numpy arrays?) is returned\n",
        "# I just need to pass the input_ids as tensors to the model\n",
        "# I think when there's only one sentence (no batching) then there's no need\n",
        "# for padding and therefore no need for passing in attention masks too"
      ],
      "metadata": {
        "id": "qzOUqohehVLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(output.keys())\n",
        "# I think we only decode the logits\n",
        "# output[0] == output['logits']"
      ],
      "metadata": {
        "id": "B0y9KIBBr8Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Maybe the max length is too small...\n",
        "# Maybe I need to send the tokens to cuda? - this helped but it didn't fix things\n",
        "\n",
        "# The issue was that it really needed the bos and eos tokens included in the input...\n",
        "# Strangely the other 7BT OpenAlpaca doens't need this token stuff\n",
        "\n",
        "# Everything works fine when it's not interpretting the input as being part-tensor\n",
        "# I think this has something to do with how colab is caching variables..."
      ],
      "metadata": {
        "id": "RsF7kjBhu7w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the variable to result to the same variable is somehow different\n",
        "input_ids = tokens[\"input_ids\"] + [2] + [1]"
      ],
      "metadata": {
        "id": "I328sOEmoYkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LongTensor means it's a long data type so the ints are correct...\n",
        "model_input = torch.IntTensor(input_ids).unsqueeze(0).to('cuda')"
      ],
      "metadata": {
        "id": "M7qaijgNopod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the input\n",
        "# If there is a tensor appearing in here then colab is caching variables that it shouldn't be\n",
        "print(tokenizer.decode(model_input.squeeze()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klCArKghviNB",
        "outputId": "207d1ef1-696c-4463-a746-1acb6ea8d976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> ### Instruction:\n",
            "What is an alpaca? How is it different from a llama?\n",
            "\n",
            "### Response:</s><s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_config = model.generation_config\n",
        "print(gen_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUN5TsBUh2zg",
        "outputId": "77d5beff-638b-4746-ef36-9709982a546a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.29.2\"\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  output = model.generate(\n",
        "      model_input,\n",
        "      max_length=512,\n",
        "      # top_p=0.9,\n",
        "      do_sample=True,\n",
        "      )"
      ],
      "metadata": {
        "id": "0SmL2DFDvmy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(output.squeeze()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFUgmnd5EBkx",
        "outputId": "7dd21128-2950-4299-9f9e-579d31796d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> ### Instruction:\n",
            "What is an alpaca? How is it different from a llama?\n",
            "\n",
            "### Response:</s><s> An Alpaca is a type of camelid native to the Andes mountains in South America. Alpacas have long legs, a long, curly mane, and a soft belly. They can weigh up to 130 pounds. Alpacas are sometimes referred to as “little lambs” because they have a wool that is very similar to sheep wool. There are many types of alpacas, including the suri breed which has unique longer legs and a single horn on their forehead.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def use_OpenAlpaca(input: str) -> str:\n",
        "  # I think this prompt format may be important.\n",
        "  # For example I get non-sense answers when I don't use this format\n",
        "  # This is the prompt for the 3B token model\n",
        "  prompt_no_input = f'### Instruction:\\n{input}\\n\\n### Response:'\n",
        "  tokens = tokenizer(prompt_no_input)\n",
        "\n",
        "  # I tried to set the bos, eos tokenizer values but it didn't work for me\n",
        "  bos_token_id, eos_token_id = 1, 2\n",
        "  tokens = tokens[\"input_ids\"] + [eos_token_id] + [bos_token_id]\n",
        "\n",
        "  # They use torch.unsqueeze along the 0 dim because they didn't form their\n",
        "  # tensor into batches for inputting to the model\n",
        "  tokens = torch.LongTensor(tokens[-1024:]).unsqueeze(0).cuda()\n",
        "\n",
        "  # They should have really just modified the generation_config\n",
        "  instance = {'input_ids': tokens,\n",
        "              'top_k': 50, # This is the default value\n",
        "              'top_p': 0.9,\n",
        "              'generate_len': 128}\n",
        "\n",
        "  length = len(tokens[0])\n",
        "  # This set up is for multinomial sampling according to the HF GenerationConfig docs\n",
        "  # Here they are overriding the GenerationConfig of the model by passing in kwargs\n",
        "  with torch.no_grad():\n",
        "      rest = model.generate(\n",
        "              input_ids=tokens,\n",
        "              max_length=length+instance['generate_len'],\n",
        "              use_cache=True,\n",
        "              do_sample=True,\n",
        "              top_p=instance['top_p'],\n",
        "              # top_k=instance['top_k']\n",
        "          )\n",
        "\n",
        "  output = rest[0][length:]\n",
        "  string = tokenizer.decode(output, skip_special_tokens=False)\n",
        "  string = string.replace('<s>', '').replace('</s>', '').strip()\n",
        "  return string"
      ],
      "metadata": {
        "id": "KA1l85JAJGZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# same prompt as provided in https://crfm.stanford.edu/2023/03/13/alpaca.html\n",
        "instruction = r'What is an alpaca? How is it different from a llama?'\n",
        "'''\n",
        "instruction = r'Write an e-mail to congratulate new Standford admits and mention that you are excited about meeting all of them in person.'\n",
        "instruction = r'What is the capital of Tanzania?'\n",
        "instruction = r'Write a well-thought out abstract for a machine learning paper that proves that 42 is the optimal seed for training neural networks.'\n",
        "'''\n",
        "\n",
        "output = use_OpenAlpaca(input=instruction)\n",
        "print(f'[!] Generation results: {output}')"
      ],
      "metadata": {
        "id": "E0m1JZAu75Th",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82a0320e-9497-49eb-e5f5-6be3eece21aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[!] Generation results: An alpaca is a small, warm blooded, herbivorous mammal closely related to the camel. Alpacas are native to the Andes Mountains in South America. Alpacas are smaller than llamas and are often called lanky. Their coats come in many colors including white, black, tan, silver and brown. They have large, distinctive eyes that give them a solemn, gentle expression. Alpacas are mostly quiet and are considered quite smart. Alpacas are usually quiet and often observed relaxing in groups. Alpacas are mostly quiet and often observed relaxing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An attempt at using a pipeline\n",
        "\n",
        "For some reason, the pipelines aren't passing the input to the tokenizer and the model"
      ],
      "metadata": {
        "id": "vatFdCQI72tL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Set up an inference pipeline...\n",
        "# from transformers import pipeline\n",
        "\n",
        "# # In a lower RAM situation I can try to use accelerate with the auto device mapping,\n",
        "# # offloading, and off load state dictionary\n",
        "# gen = pipeline(\n",
        "#     task=\"text-generation\",\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     # use_fast=True,\n",
        "#     device='cuda:0', # alternatively cuda:0\n",
        "#     )"
      ],
      "metadata": {
        "id": "ZiDsx6wpuqwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Does the textgeneration pipeline append bos and eos tokens? Idk\n",
        "\n",
        "# instruction=r'What is an alpaca?'\n",
        "\n",
        "# prompt_no_input = f'### Instruction:\\n{instruction}\\n\\n### Response:'\n",
        "\n",
        "# generated_text=gen(prompt_no_input)\n",
        "# print(generated_text)"
      ],
      "metadata": {
        "id": "K_7-QAabIIPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up a gradio demo"
      ],
      "metadata": {
        "id": "EHFiKfO4IxbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# We instantiate the Textbox class\n",
        "input_textbox = gr.Textbox(placeholder=\"Please enter an instruction\", lines=2)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    title=drive_checkpoint, # the path where the model is\n",
        "    fn=use_OpenAlpaca,\n",
        "    inputs=input_textbox,\n",
        "    outputs=\"text\",\n",
        "    examples=[[\"What is an alpaca? How is it different from a llama?\"],[\"How does a car work?\"]],\n",
        "    cache_examples=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "wBfBf6S3Waqk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91813213-fd2a-4521-e22f-2b49fb4e1cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching examples at: '/content/gradio_cached_examples/15'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(share=True) #, debug=True)"
      ],
      "metadata": {
        "id": "WFbXsIGnWl2j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "74ed9339-5a58-4a04-8b17-85ab5e3fc6ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://a8529628f21162fda3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a8529628f21162fda3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}