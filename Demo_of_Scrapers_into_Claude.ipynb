{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andygma567/LLM-experiments/blob/main/Demo_of_Scrapers_into_Claude.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbuN-8oPXnFh"
      },
      "source": [
        "- [Claude](https://www.anthropic.com/product) can work on 1hour videos (200k token input)\n",
        "\n",
        "- [Chat GPT 3.5] (https://chat.openai.com/) (4k token input)\n",
        "\n",
        "- ~~[Llama2](https://labs.perplexity.ai/) (3-4k characters / 512 words)~~\n",
        "\n",
        "- ~~[Mosaics's 30B MPT-chat](https://huggingface.co/spaces/mosaicml/mpt-30b-chat\n",
        ") (8k token input)~~\n",
        "\n",
        "- ~~[Bard](https://bard.google.com/?utm_source=welcome-email&utm_medium=email&utm_campaign=en) (I'll look up the context length later)~~\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQnbmB70zqon"
      },
      "source": [
        "## Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk2d90cCdF4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe4afa86-70ce-440f-cf2a-25d6768e2203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 803.6/803.6 kB 3.4 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 21.5 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.6/229.6 kB 10.4 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.3/49.3 kB 4.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.4/49.4 kB 1.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.6/57.6 kB 1.2 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 8.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 457.9/457.9 kB 34.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 274.7/274.7 kB 26.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 981.5/981.5 kB 51.4 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 48.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.6/16.6 MB 43.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.0/92.0 kB 9.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 305.2/305.2 kB 26.3 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.9/75.9 kB 7.4 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.8/139.8 kB 13.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 381.9/381.9 kB 27.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 2.5 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.5/7.5 MB 42.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.6/60.6 kB 6.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.9/129.9 kB 12.2 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 75.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 5.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.1/71.1 kB 6.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.9/76.9 kB 8.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.0/77.0 kB 1.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 11.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install -U -q langchain\n",
        "# needed for scraping youtube\n",
        "pip install -q youtube-transcript-api pytube\n",
        "# for reading urls with langchain\n",
        "pip install -q unstructured\n",
        "# for the demo\n",
        "pip install -q gradio\n",
        "# for openai\n",
        "pip install -q openai==0.28.1 tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "tmnNNc-DrrQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n75sbCqQN2PP"
      },
      "source": [
        "# Create a gradio interface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import (UnstructuredURLLoader, WebBaseLoader, YoutubeLoader)\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "import gradio as gr\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "from langchain.chains import (\n",
        "    StuffDocumentsChain,\n",
        "    LLMChain,\n",
        "    ReduceDocumentsChain,\n",
        "    MapReduceDocumentsChain,\n",
        ")\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Set up the MapReduceDocumentsChain\n",
        "# Instantiate the LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\")\n",
        "\n",
        "# Set up the text splitter and document loader\n",
        "text_splitter = TokenTextSplitter(chunk_size=16000, chunk_overlap=0)\n",
        "\n",
        "# Setting up the MapReduceDocumentsChain\n",
        "# Setting up the map step with LLMChain\n",
        "prompt_map = PromptTemplate.from_template(\"Summarize this content: {context}\")\n",
        "llm_chain_map = LLMChain(llm=llm, prompt=prompt_map)\n",
        "\n",
        "# Setting up the reduce step\n",
        "prompt_reduce = PromptTemplate.from_template(\"Combine these summaries into a bullet point summary: {context}\")\n",
        "llm_chain_reduce = LLMChain(llm=llm, prompt=prompt_reduce)\n",
        "combine_documents_chain = StuffDocumentsChain(llm_chain=llm_chain_reduce)\n",
        "reduce_documents_chain = ReduceDocumentsChain(combine_documents_chain=combine_documents_chain, token_max=4000)\n",
        "\n",
        "# Creating the MapReduceDocumentsChain\n",
        "map_reduce_chain = MapReduceDocumentsChain(llm_chain=llm_chain_map, reduce_documents_chain=reduce_documents_chain)\n",
        "\n",
        "def scrape(url, document_loader, use_api):\n",
        "    # Load the documents based on the selected document loader\n",
        "    if document_loader == \"YoutubeLoader\":\n",
        "        loader = YoutubeLoader.from_youtube_url(url.strip(), add_video_info=True)\n",
        "        prompt = PromptTemplate.from_template('Summarize this youtube transcript with bullet points: \"{text}\"')\n",
        "    elif document_loader == \"UnstructuredURLLoader\":\n",
        "        loader = UnstructuredURLLoader(urls=[url.strip()])\n",
        "        prompt = PromptTemplate.from_template('Summarize this website article with bullet points: \"{text}\"')\n",
        "    else:\n",
        "        loader = WebBaseLoader(web_path=[url.strip()])\n",
        "        prompt = PromptTemplate.from_template('Summarize this website article with bullet points: \"{text}\"')\n",
        "\n",
        "    docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "\n",
        "    # If use_api is True, use MapReduceDocumentsChain to get the summary\n",
        "    if use_api:\n",
        "        print(len(docs))\n",
        "        # Run the MapReduceDocumentsChain\n",
        "        outputs = map_reduce_chain.run(docs)\n",
        "        # Return the summary\n",
        "        return outputs\n",
        "\n",
        "    # If use_api is False, return the scraped content as is\n",
        "    else:\n",
        "        return prompt.format(text=docs[0].page_content.strip())\n",
        "\n",
        "# Start the gradio demo\n",
        "interface = gr.Interface(fn=scrape,\n",
        "                        inputs=[gr.Textbox(placeholder=\"https://sites.google.com/view/mnovackmath/home\", label=\"Website url\"),\n",
        "                                gr.Radio(label=\"Webscraper\", choices=[\"YoutubeLoader\",\"UnstructuredURLLoader\", \"WebBaseLoader\"], value=\"YoutubeLoader\"),\n",
        "                                gr.Checkbox(label=\"Use API\"),  # Adding the checkbox here\n",
        "                                ],\n",
        "                        outputs=[gr.Textbox(label=\"Scrape results\", show_copy_button=True, max_lines=10)],\n",
        "                        title=f\"URL scraper and a MapReduceDocumentsChain\",\n",
        "                        description=\"Works best with Claude2 or ChatGPT\")\n",
        "\n",
        "# Launch the interface\n",
        "# Enabling the queue is required for inference times > 60 seconds: https://gradio.app/key-features/#queuing\n",
        "interface.queue()\n",
        "interface.launch(share=True)"
      ],
      "metadata": {
        "id": "WEjH0vOzo624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "5031950a-27b5-46fa-c875-2b85838221fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://89d717533d49461545.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://89d717533d49461545.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Summarize this youtube transcript with bullet points: \"welcome to computer vision decoded where we discuss topics in the quickly evolving world of computer vision in today's episode we're going to dive into one of the hottest Topics in computer vision that's neural Radiance Fields whereas most people know it Nerfs as always I'm joined by our chief scientist of everypoint Jared heinle and today I'm extremely excited to have our first guest on the show Matt tancic Matthew is currently pursuing a PhD in the computer science and electrical engineering department at UC Berkeley last but not least he's building Nerf Studio a collaboration friendly studio for nurse with a few other students at UC Berkeley thank you two for joining uh just start off um I just want to get a little bit of a background Matt what what set you on the Journey of of pursuing computer vision and um you know your whole background that led you up to all this this Nerf research uh yeah that's a good question so um I guess I first got a computer when I was in maybe Middle School an old hand-me-down laptop from my parents and wasn't allowed to play or download it couldn't download games right but the one thing that it was able to run was blender so that was sort of like the first program I actually had access to and started playing around with and I'd say that's kind of where it started um first internship I got was using blender full time lots of reconstructing scenes from point clouds captured from lidar and yeah and then going throughout undergrad sort of switched to the more academic route taking classes wasn't really sure what I'd do after that but realized that you could actually contribute to these Cool Tools like blender and build off of them so that kind of got me interested in what you actually needed to do to contribute which is learn how to program they're in these various 3D Graphics methods and ultimately ended up here at Berkeley working on my PhD where worked with a great group of folks on building out Nerf and the various variants sets all right I mean look at your background prior to Berkeley you were at MIT was that correct yep all right that's a huge difference uh East Coast Cold versus you know the warm weather in Berkeley I guess kind of warm depending what time of year um all right and it sounds it's interesting I don't come from the computer science background that you two come from but it sounds like you know you have similar backgrounds that Jared what can you can you talk about yours too because I think that's kind of interesting how people end up in this this really it's kind of a hot field right now but you know you don't just fall into computer vision as a research yeah yeah no it's similar like with me it was for me it was high school getting my first graphing calculator you know the TI-83 or whatever and so there uh I did put games on it you know Tetris and you know Space Invaders or whatever it was you know and then very quickly was like okay how can I make games and then trying to figure out programming oh how can I make 3D games and then that led me on the path of oh can I do 3D programming and then I you know did my computer science degree got interested in Ray tracing research just to you know render you know really nice looking images and then from there was like okay well I can render images and how can you take images and turn that into computer vision so that was a big game changer for me realizing that there was a whole world of computer vision that uh I just I'd never seen before but yeah I love loving the the you know that that Graphics that Imaging that blender uh background that's really cool yeah there's a lot of different paths to get where you are yeah um all right so Jared do you want to tee up tee up one of the first questions or I can take it from here no I was gonna say I mean so for me um I guess you know so my background I deal with 3D reconstruction you know a lot of image based um 3D reconstruction but Nerfs you know that's not my I haven't I haven't done research into Nerfs you know like you have and so I guess just to start that off can you explain to us at the highest level you know what is a Nerf and how is it different from something that I'm familiar with more like a photogrammetry based output you know whether that's a point cloud or a textured mesh um yeah what what is a Nerf what does that even mean yeah so I'm going to start by explaining what a Nerf isn't um so what a derf isn't is a mesh for people who are not super familiar with Graphics it's basically the way that almost everything that you see in 3d is represented so the world video games TV shows when they want to use 3D rendering they'll use a mesh which is just a scene that is made up of a bunch of triangles where each triangle has a different color and if you think about why we use meshes it's because it's something that we can actually like easily or somewhat easily interact with the world's quite complex but we have to make some simplifications so if I know that I have a flat wall you could try to model it on like every single atom to describe the whole wall That's obviously way too complex so instead I just create a big triangle a bunch of big triangles color it white in this case and that's my wall so this turns out to be a very easy thing for like humans to sort of work with you just sort of move around the corners of these triangles but for various reasons it's actually kind of tricky for a computer to operate in that regime you because the way that we want to sort of create these computer algorithms to reconstruct scenes they will sort of start with some mesh and try to deform it or modify it until it reaches some Target shape and that you have to ask what question like the question of what do you start with so you could start with like a sphere and then you sort of slowly deform it over time until you get say a water bottle that I'm interested in now in this case it's probably going to do a pretty good job but you've got one problem here which is like there's this Loop right now there's no way I can modify a sphere such that I still get that hole so you run into these sort of edge cases and problems when you try to use meshes in the uh sort of computer like having the computers operate with them so for that reason we're gonna approach the problem from a very different approach one that's maybe harder for humans to interact with but easier for our machines to interact with and this is more of the approach of like let's try to model every atom in the scene now in practice it's not every atom in the scene but it's like bigger chunks of space in the scene so I think the closest analog is something like uh Minecraft so Minecraft you model the world with cubes right and you can draw them you can kind of make whatever shape you want people make rather crazy objects but these are pretty Big Blocks so let's just imagine Minecraft but now we make those blocks smaller and smaller and we can represent most things in the world so we call this a volumetric representation because we're modeling the volume every place in space so Nerf is a volumetric method that can represent the like realistic representations of saints where some of the Novelties of Nerf come that come in that sort of differentiate it from something like Minecraft is sort of along two Dimensions the First Dimension is um you want these really small cubes or you want to be able to represent small regions of space turns out if you try to do it with something explicit like a bunch of Cubes it just doesn't fit on the computer anymore so you have to come up with a different way to describe the scene so that you can represent a very large scene so I'm not going to go too much into the details but a lot of the uh like the original Nerf method will essentially use the same technology that machine learning uses a neural network to represent all these little essentially cubes in space and then more recent methods kind of blend that with the minecrafty approach of having actual cubes and it turns out you can get some speed ups and that's where you see a lot of these methods that can work really fast kind of blend these different methods together so that's one difference the other difference is you like the real world the color of a pointed space isn't constant if I move around it'll change colors right if you look at this water bottle from one angle versus the other you get Reflections in what we call specularities on these objects so we want to make sure that as we represent all these little pieces in space they have a way to change their color depending on the direction I'm looking at it and that color that we're changing is one way to think about it is it's the reflectance or the radiance Radiance is the color emitted from a particle in a certain direction and hence the name neural Radiance field so it's a field so a field being a space full of particles that are represented by these particles with different radians and we're going to use a neural network to actually store that information so that's a neural Radiance field okay so that that makes sense to me how about you Jared yeah yeah so to me that's so it sounds like it's a very you know fundamental difference in the representation you know like you said like so a triangle mesh you know you've got vertices you've got triangles you're representing sort of like these solid surfaces you know like you know there's empty space there's a surface where that triangle is and there's empty space behind it you know and you're linking these triangles together represent that surface or the boundary of objects you know that being one representation another representation you know I work a lot with this point clouds you know there there is no sort of representation of the surface it's just a bunch of sort of independent samples of here's just a point in space um and there you know it may not be on some sort of regular grid it couldn't sort of be random or a sparse or um there's going to be patterns to it but that's you know there's no representation of the surface whereas you're saying here with this you know volumetric representation this is sort of a third you know different representation that's that's representing um you know sort of you know the density or the um the radiance um at this at you know all these points in space but um yeah that makes sense about having to you know a lot of the you know issues of it just you know trying to do a brute force and store all of that information at once you know is is way too much and having to deal with you know various ways of of compressing or reducing that data um to make it make it practical oh that's cool Yeah you mentioned Point clouds like one way to think about this is this is just a really really dense Point Cloud um with the exception that it the points can change color depending on the direction you look at it yeah yeah I think that that view depended like the scene change Sheen as you change the view of the camera is part of the magic I that that has kind of caught the imagination of all the creatives and just I post something online you're seeing you're seeing shadows tracing through you know or Reflections chasing through through water I think I think that's what people are seeing there this is they're changing the camera well the appearance of that part of the scene has now changed and and you just don't see if you're not seeing that any other way except for a lot of manual work of people you know doing things for like games where they have to you know do a bunch of fancy tricks with with with meshes and textures and you know post-production that you're not doing that I'm not doing any post-production on Earth and train the Nerf I render a camera animation of the scene and it just it just happens it's worth noting that these like reflection properties are something that we like hope the model predicts the correct Reflections but in many cases there are ways to cheat so using the same representation of space there are ways to make it so that it looks correct through this optimization procedure from the images that you give it as input but in practice the thing it reconstructs like the underlying density and geometry is incorrect and so this is one of the reasons why people do differentiate this versus uh existing approaches that are more focused on the actual geometry because you can't really trust the geometry in nerves and so one good example is something like a mirror a mirror you could represent the scene as having density at the surface and then depending on the direction that I look at each of these points it changes color and that would be the correct way to represent it there's another way that you could theoretically represent it though which is to represent it as a whole that you look through the world and you see the reflected uh world like at the same depth that it would be if I were like looking behind me at that location in the mirror and so the computer is kind of deciding during this optimization which one to choose and there's ways that you can try to like force it to choose one versus the other uh by like tweaking some parameters but in practice like either one satisfies the goal that you give that computer to begin with so you do get these like weird artifacts often and on an artistic sense it can be like quite compelling I think people may have seen those some videos people have made where they've done Nerfs of like mirrors and other reflective objects and you can like fly into like the mirror World um but depending on your ultimate application like that may not be desirable oh that's so cool that's so cool like your comment there about you know having it look good even though the geometry is bad I mean I see it a lot of times with with textured meshes it's like oh you can have a really low you know low quality mesh but then you texture it with some texture and all of a sudden it looks like it's amazingly detailed um this sounds like it's like that problem you know just you know to the next degree of it's uh you can have this is you know dealing with Reflections and yeah that whole thing about well is it the is the mirror actually that flat surface or is there a mirror world you know hidden behind that that space that's so cool I mean it's worth mentioning like a little bit of the history here so things like photogrammetry are interested in reconstructing uh the correct geometry of the scene and when we started the Nerf project we came at it from a completely different sphere our goal is can we create good looking images we didn't care about the underlying geometry at all which is why we were like let's just represent the world as this sort of like Cloud it's just a lot of little particles there's nothing that forces it to be it correct we just care about pretty looking pictures in the end and when we like if you look at the original Nerf papers that is sort of what we focus on like oh we can get these good looking images out of it it was sort of by accident that you can actually get pretty good geometry in many cases um and use it for other applications indeed okay so going down that that line then of uh now you've you've you've built the first NERF and you guys have iterated on it and improved it or maybe not improved but have different versions of it now and that some of the geometry is appearing to look better um do you see a a future where perhaps photogrammetry becomes this deprecated model or method to to create an accurate representation of the physical surfaces in the world and we we use Nerfs instead or or do you always find that this will be kind of a never reach that same level of accuracy that you would get you know are they are they is that kind of like a we that will supersede it or or there'll just be different methods that have different pros and cons um I mean I think they'll always exist multiple methods but one thing that kind of surprised us is we released the paper we got good looking quality results but a lot of people are interested in the the actual geometry and so what you end up seeing is a lot of the follow-up work uh particularly in the computer vision Community has focused on the geometry aspects and that's why you do see that like there's methods coming out with much better geometry and one thing to note is a lot of these methods will actually take in the some of the ideas from photogrammetry like things that we know work well in photogrammetry import those ideas into Nerf in order to improve the geometry so it's not that like one will supersede the other but it'll be sort of a combination of merging that makes sense because yeah uh yeah no I I think that's valuable I mean just because that sharing of knowledge between between the different approaches I mean I've seen that from just like the photogrammetry side like using pure just you know heuristics and algorithms talk oh let's let's you know triangulate points in space and then compare patches of pixels to see which one you know what is the depth of this point and so there's a whole you know decades and decades worth of literature about oh here's the best methods to do that you know and now some of those are being superseded by you know learned learned based approaches where you can say well no machine learning method you know neural network can do a better job at figuring out well given these two you know images you know what is the depth of a particular pixel you know and so you start you know leveraging the best tool for the job in that you know goal of you know reconstructing or understanding the scene I I think one interesting data point and this is by no means general but one thing I've experienced uh when I have a lot of images that are well captured of a scene and I have like good camera poses uh if I also have additional depth information say from a lidar scan it's often beneficial not to take it into account because even though like lidar is very very accurate any sort of misalignment which is almost uh guaranteed given that like you've got rolling shutter artifact various artifacts that happen that misalign the images from the lidar plus the surfaces of like the edges of objects with lidar can be sometimes kind of noisy not taking into the lightering account when you have lots of images uh you can actually produce better quality depth from images alone so I think that's like a bit surprising again it's not General but in in some cases that does end up happening and so I think it's worth like asking uh when we think about how we do these like photogrammetry type setups where we do care about recovering geometry like maybe there are other approaches that involve just like taking lots of photos and using nerve-based approaches now if you have something like a wall that is all the same color Nerf and photo parametry is gonna are gonna fail to produce like a a good representation of that where lidar is going to do quite well so I found lidar type deaths are helpful when you have scenes like that or if you have cases where you just don't have very many images but if you've got a lot of images a nerves can be kind of compelling one I've seen in my personal explorations of hundreds of Nerfs now I've just kind of thrown all kinds of different scenes in and as you said it doesn't know where the sky is so you end up when if you don't look at it from the right direction so you see perhaps what would be a cloud over a building and it's like the cloud is actually in front of the building if you were to look at it from the wrong direction because it just doesn't know how to represent that scene from that that angle at all um you know he just doesn't have that depth information it says oh there's something there I can't tell if it's I'm guessing this just doesn't know if it's really far away it's really close but I find that if then also I get more more images like if I'm it's an unbounded scene I get that but if I manage to like walk around a building it starts to say oh well I saw it from the side profile now and no there is no Cloud hanging out out in front of that building it's it's it's a blank space it's a you know transparent there so it is kind of kind of cool to see how you can you can change your image set to get less of that um ambiguity of of the ratings field there yeah because I mean it produced the correct image for that Viewpoint that is trained up right and so the trick is like how do you also help make it produce good images from new viewpoints and it does pretty well when you're like in the regime like the same areas that you've captured from as you deviate from that you're going to start seeing artifacts like that and then in a case like that this is where like you can step in and say well we know that the sky AI or things up high like are often very far away so you can start programming in some prior knowledge saying okay if I see something like that put it as far back as possible you'd be a little bit careful there because now let's say we have some fixed color that like the blue sky it doesn't change depending on how I look at it so I can put it infinitely far away but if I look at the ground maybe it's like a nice new concrete ground it looks the exact same color I could also put that infinitely far away so it becomes like a game of which sort of prior knowledge do you want to incorporate and any prior knowledge that you do incorporate it could potentially like hurt other aspects yeah so Jared even know that there's a lot of balance and a lot of um as I've dove into the world of computer vision finding out how much you guys just spend experimenting with different variables and just seeing how you know run the data over and over with modifications to get that desirable effect for the hopefully known scene that you're going to be representing more than once you know in the future and say okay well we know if we're doing this sort of scene we'll use these parameters and a lot of that's just trial and error see see how it ends up yeah yeah so much goes and just understand because it's you can develop algorithms you know the ideal be to develop an algorithm you know a method that can work anywhere under any environment under any circumstances but you know a lot of times having some knowledge about oh okay I'm gonna be reconstructing man-made objects and so therefore we there's going to be a lot of stuff that's going to be flat you know or oh I'm going to be in an outdoor environment where there's going to be no repetitive structure you know everything looks unique and random and I can sort of you know use that as some sort of constraint or you know help influence the algorithms that be developed that are being being developed that makes sense that makes sense all right so I have some uh Jared I have some questions around uh block Nerf which maybe one of the more novel you know or interesting ways uh that Nerfs have been used that I think I think that one circulated the internet for a good while um and uh I just I think it was interesting was was three was it 360 images 360 camera images as input um is that is that is that correct is that that how you would you see this technology for capturing large spaces to me more like a 360 camera array or um was that you know was you guys just something something that was unique that waymo had lots of just different cameras all over that um you know their their cars that capture that data so I you can kind of think of it as a 360 camera but in reality it wasn't it was the car itself has I forget exactly how many cameras somewhere around eight or nine cameras that are placed looking in all directions so in that sense like you can fuse them all together to make a 360 uh render the slight modification there and this is an important aspect of nerve is you want to view the scene from as many locations as possible a 360 camera the camera sensor is at one location looking in all Direction so while you see everything the the source is the same location uh the cars are a little bit different because the cameras are like spread across uh I think like there may have been some on the like front bumper or some on the top some on the back so there's a little bit of spreading as to where the camera sensors are so it's a little bit better information than what you'd get with the 360 camera that being said if you just walk around with that 360 camera you solve that issue mm-hmm yeah that's the same same problem we have there in photogrammetry too it's like if you only have observe the the scene from one position in space yeah you can't triangulate that depth which sounds like that's the same problem there with with Nerfs yeah and I I kind of pulled up the the waymo video the video that uh by the way I noticed that you you narrate a lot of these videos just to Glad a little humor here do you guys draw straws who who does that or is that just uh who who decides who narrates these papers that's just something I've always noticed a variety of things I mean a lot of these are like who's still awake at whatever time of night before the uh submitted to a conference um yeah but uh I just thought it was funny it's like maybe they always make Matt go up because he's he's done a few of these now um go ahead oh I was gonna say just to take a step back I mean so Jonathan's showing here these amazing visuals is you know just you know Urban uh Urban environments downtown uh San Francisco or whatever being reconstructed can you just Matt can you just take a you know take a step back and just kind of explain at a high level you know what what is what is block Nerf or what was the goal of uh block Nerf yeah so the block Nerf is a project that was done with waymo which is uh Google's self-driving car effort and their main goal is I mean essentially to make like autonomous cars to drive across all these streets they can obviously test the cars on the streets but they can only go so far right so if you want to start simulating new scenarios or whatever it might be it's useful to have a 3D representation of the world that you can like interact with and so traditionally you would potentially hire someone to try to create 3D assets of all these elements but in this particular case you've got these cars with all these cameras on them can you just automate the process that that was the goal and so you've got photogrammetry methods that could potentially work on it I had experience in Nerf stuff so it just seemed like a natural idea can we apply Nerf to the data that the waymo vehicles have already captured to just reconstruct them but this leads to a few challenges that we don't normally have or at least the original Nerf papers didn't really deal with and it's really one of scale so while there's a lot of data that is of different regions of space you have to take into account that they're taken at like different times and that different objects were around so a lot of that paper was just a matter of like pulling it together or pulling together various other approaches that have been proposed in other papers that kind of account for these different aspects so things like changes in weather things like there's a vehicle driving by how do you handle all that okay that makes sense that makes sense and what you said earlier you know talking about how you know Nerfs motivations being around generating that you know nice looking image that realistic looking image versus just you know perhaps accuracy you know just raw Brute Force accuracy of the geometry that makes sense if you're trying to you know simulate the world you know and simulate the world through Visual means having those really nice looking accurate images um yeah that seems like a great application of Nerf yeah because I mean the self-driving car companies or most of them like waymo they have lidars on them so they have a some estimate of the 3D geometry of space and so simulating lidar it's not easy but it's a little bit more straightforward um and that's something where you can actually use more crude 3D models that already exist that you can just like create a lidar sensor and just have it travel through that space visual is harder right uh what actually is fed into the camera you want to be something that looks better realistic now I think the video that you're just showing like is a step there it's not photorealistic yeah right there's issues uh and a lot of those issues come down to the fact that this data was captured over long periods of time and so you lose some of the constraints that you normally have or assumptions that you normally have that like the Cena static so when the leaves on the trees change we don't really have a great way to account for that so as a result it sort of blurs it all out and kind of predicts the average of all the different Leaf positions or when you look at this data over a long enough period of time you actually see like the tree grows right so you're looking at the trees either short or tall or no no like literally it grows over a month or two period of time right okay so um or the leaves fall off the trees that's cool we did have a past episode um about uh you know computer vision in the wild I think was the title and it was you know those challenges so Jared has a lot of experience as well with just crowdsourcing you know hundreds of thousands of images and it's just no matter what you're not going to get Perfection but it's amazing what you can pull off to get a good understanding of a scene even with all that I you know mismatch of information between images um you know we get I mean Jared was even using a mix of cameras that we didn't even know what the camera they were a scrape from Photobucket I think or one of the one of the big you know crowdsource image sets where you didn't know what the camera was just had images so um it's impressive what he pulled off especially also that block Nerf was was done that was that wasn't done like last week that was a while ago so you know not that that you can solve those problems necessarily today but you know I've already seen the quality of Nerfs just improve as you as this has evolved and I think people forget they say oh well this is no not as good as photogrammetry photogrammetry has been around since you know decades versus a couple years so there's a lot to figure out a lot to optimize so I just thought was a really good example of how he's kind of like you said you pulled in a lot of ideas to pull off this large Nerf uh so when when one thing I noticed that people kind of struggle with with Nerfs is is it takes an enormous amount of GPU power you know you can't just run it on a you know entry-level laptop or desktop you need that high-end GPU um can you can you talk about that like uh do you see that that that burden goes away in the future is that just the reality of the technology and and if so also how then how do you expect people to to be interacting with with nerves so we're going to be doing it on cloud-based environments where you don't have to it's just you know close hosted out there in the the cloud uh how do you see that that for that moving right they they are compute intensive and the maybe I'll briefly describe why they're compute intensive um I described that you represent the world as sort of lots of little cubes or like little little chunks of space um but since it's all being represented uh at in sort of this representation that often's like a neural network you don't actually know where it is until you ask whether there's something in that location in space so the way you actually render or predict the color of a single Pixel is you sort of like shoot a camera ray out into space and then along that Ray you take lots of little samples and each of those samples you ask is there anything here and if there is what color is it that asking process is actually feeding that sample through a neural network and popping out an answer and so if you're doing 500 samples on array 500 queries of Renault Network and you're trying to render a one megapixel image that's you're rendering like evaluating this neural network billing sometimes so very quickly it becomes quite expensive and for that reason the original Nerf papers they would take uh almost a day to or some taste in some cases more than a day just to reconstruct a single C quite expensive and for a while that was like people were sort of fighting getting it maybe like 20 faster or 30 faster but over time uh people found more and more tricks in order to make it so now you've got something like instant GP or these other fast methods I could train on the order of five minutes right so we've already gone multiple like multiple orders of magnitude uh there's different ways and different tricks that people use but one of the biggest tricks is instead of doing 500 samples along the array if you keep some information around saying that like there's nothing here don't sample this region you can get rid of most of the that computation and so as we go forward we're coming up with more and more tricks like that so that we don't have to do quite as much computation when we actually render the scene so do I think it's going to get faster I I think it will I mean it already has gotten significantly faster that being said it's still obviously uh not in the not something you can run on your laptop but I do think it'll go farther I think the other thing that's worth mentioning is uh computer graphics 3D Graphics we kind of take for granted now but it took time for them to actually be feasible to run and one of the main reasons they're feasible now is because we have something like a GPU which is literally designed to render out a bunch of these mesh triangles as fast as possible so we're sort of seeing people get more on board with these Nerf based methods I think it's totally feasible that these manufacturers will also start looking into like how do you actually optimize on a hardware level rendering these things out faster so the That Base approach I think will continue to get faster it might be a little bit slower especially if given that like Hardware modifications may be necessary but there's another approach that you can kind of look at which is uh can we convert these nerves into something else so if we forget about the actual creation of the nerve for a second let's just say the creation of the nerve stays a bit expensive and that's something that you run in the cloud or somewhere else once it's created people are coming up with more and more methods to convert it into a representation that still maintains a lot of the visual quality but is using the technologies that we've already optimized essentially converting it to a mesh or something similar to a mesh that we can run on something like a phone or normal devices without gpus and we already see that happening both in research and also in Industry uh Luma for example is a company that will do some sort of conversion like that which is why you can visualize it in your browser mm-hmm that that makes sense uh also a mobile Nerf which turned into a triangle mesh as well yeah running High FPS even on my you know my iPhone um in browser so so I just as like a interesting aside is like mobile Nerf it converts it to a triangle mesh but it's not a triangle mesh that you're used to uh what it is is it's tons and tons of triangles all overlapping each other like uh like we like to use the term triangle soup where it's just sort of like you splatter triangles into the scene and so it's not the way that people typically think of triangle meshes where a lot of people care about it being watertight so it's some volume that doesn't have any holes uh uh it's like nice and perfect that's not what these are but they can Leverage The advances in GPU technology to make it really really fast while also producing great looking renders yeah that's that's what I noticed it it looks great don't leave the scene it's not ready for a game um and also right a lot of triangles you know it's not optimized to be the best looking with the least amount of triangles and meshes and data there it's but but it works that's interesting for visualization and and I think a lot of people forget that Nerfs uh there's a lot of people say well he's what's useful with the Nerf if I don't get a mesh there's no use for this technology without meshes and that that is a definite definitely a Viewpoint from people who just need meshes they work in game engines they work in software that they're interacting with a mesh and that's that's fine but I I think that people just Overlook a whole entire like think of waymo they're doing self-driving cars right so they actually don't care about a geometry a lot of times in a scene it's the cars learning what the scene looks like and how to interact with a scene based off appearance because using camera guidance they're not they're not uh you know and you can simulate lidar data for for for some geometry but I I I think that that is a application where we don't have to worry about meshes and it could be expensive to render inexpensive to train but we're using it for these machine learning models that in the end it's worth all that investment all that time all the energy all the GPU Cycles because it's it actually is faster than someone perhaps modeling these objects in 3D in a you know in all these third-party softwares just to get one little scene for a car to learn how to drive around it um so I I like to throw that out there yeah I think I would also have sort of like two other responses to someone who says they just want meshes because they want geometry um I think that's totally valid because that's what our tooling is built to use right now but people well so the first thing is uh people are creating methods to convert nerves to meshes so it's not like using a nerve means that you don't get something like a mesh as an output right not quite as good uh but we're getting better and there's a lot of research in that space so I wouldn't be surprised if you can get pretty good meshes out of a Nerf based model soon um and the the second thing is people want meshes because that's what their tools currently use right if you got the unity game engine or whatever game engine you import a mesh I don't think that's always going to be the case you've already seen people hack together ways to import Nerf style models into these existing game engines and I think it's only a matter of time before these game engines implement it at more of a core level so once that is the case then do you actually need the mesh or you just need the mesh because that's what your current tooling uses yeah I agree there it's a it's a necessity right now but is it a mandatory requirement maybe not and uh just I you know I've been spending a lot of time interacting with Nvidia who obviously are spending a lot of time and energy and you know research Cycles on this technology and they haven't said anything publicly but you can tell that that they're working on something with the Omniverse off of what they've said and I can only imagine that they're going to have some sort of you know server-based rendering of Nerfs and not convert it to a mesh necessarily because you know that the whole Omniverse they have a lot of synthetic data rendering and things like that they use for machine learning and I can see how this just easily integrates in that whole ecosystem where you don't need the mesh in that tool set but like you said some other tool sets it's it's the reality of today I I think there's also like a just general trend of a lot of tool sets migrating to the cloud and once you migrate to the cloud then they can do whatever they want in the back end right like they can use all the compute they want and yeah so yeah all right well I just want to make sure you have enough time here uh because we're coming to the end of our of our time with you but I want to make sure we talk about um a project you're currently involved in called Nerf studio and I'm gonna just if people are on the YouTube version of this I will again have some visuals of just outputs from Nerf Studio but uh can you just give us a quick overview what Nerf studio is to start out with and I'll show some cool interesting results yeah so it's an open source project um that essentially tries to bring together a bunch of different Nerf methods so Nerf came out a little over two years ago now and since then there's been a lot of follow-ups uh the most notable ones are the ones that make it like real time and fast but there's also been a lot of things to improve the quality in a variety of different scenarios um and the challenge is if you go out there and look and you just want to use Nerf uh say you're a researcher or a programmer it's not entirely clear where you start you could grab like the thing that I would recommend number one is don't grab the original Nerf code base there's like zero reason you should use that anymore yet a lot of people still use it and we still get like GitHub issues and people trying to mess around with it but it's out of date uh these things have improved since then and I've personally experienced this uh we talked about block Nerf earlier block Nerf I just combined a bunch of different methods together right and there was a few methods that I hadn't had the chance to combine yet and so when I went back to uh Berkeley to work on my next project I wanted to do essentially block Nerf but on a uh academic uh level compute rather than like Google level compute and I think it's possible now that we have things like instant NGP which allows things to be quite quick and so I started hacking around I wanted to reconstruct the campus here at Berkeley and very quickly I realized I was repeating all the same things I was doing like uh combining these different methods that I had to do when I was at waymo and it seemed kind of like a waste of time and just thinking there's got to be other people that are also trying to combine all these different methods together what if we just do it the correct way or a more of a correct way so that when people want to use these methods in the future they can just piece them together are more like Lego blocks and get good results at the same time can we make the the UI and usability easier so that folks that are maybe less into the research side of things can use this as a tool um that combines the the best of the best that we have in nurse at the moment and so I with a few other grad students I'll highlight Ethan Weber and Yvonne we worked together to create this and we released it a few months ago and yeah it's been exciting to see other people use it um we released one model with it the nerfacto model and so this is just an example where we combined a few different methods so you asked I don't know if you asked how it like compares to instant NGP but like if you compare it to something like instant GP the new model is essentially instant NGP but like with other things added on top of it so it's not like it's uh trying to compete with it it's using those ideas and Concepts and just bringing in other ideas and Concepts that have happened in the space sense so for example now there's like a method that lets you render stuff in the background a lot clearer why not include that with instant NGP right that was my first the first comparison I took the same image set I was like I always want to see the difference between de facto and instant NGP and it's just like oh my goodness it's like the background mountains just snapped into Clarity which which excited me because I always felt like the bat you get something to get further and further away they almost look like they've been water colored into the background as opposed to just even out of focus they just like smeared that that's not the case yeah yeah so the goal going forward though is as new methods and new techniques come out and like people sort of agree that this is like a good direction to go we want to incorporate them and make an easy place for people to like contribute their own modules uh but also sort of leave like a history of what sort of uh the different approaches one can take and I think what's a little bit different than maybe treating this as like a traditional research project is that we're kind of doing this all in the open um the goal isn't to write some paper and I think there's great reasons to write papers about different methods in this area but I think there's equally good reasons to try to do these things more publicly and not just try to figure out ways to like differentiate yourself from others because I think you've got to accept that like there's a lot of good ideas in this area if we combine them all we can get something better and so as a result we've seen people uh outside of our core team starting to contribute to the repo which is the thing that we like that was our ultimate goal uh because the hope is to make this something that's like sustainable that people across industry and Academia can help contribute to to improve it over time all right that's that's such a valuable effort I mean like I know just in my in my work during my PhD like okay every grad student they've got their own project that they're working on and oh I'm gonna try to use someone else's code sure I might find it somewhere and then I try to download it compile it oh it doesn't work you know and so just you know that effort of just you know reproducing everyone else's work to try to you know build you know benefit your own you know that's I really appreciate these products like for me in the photogrammetry space you know dealing with you know a call map or open mvg or visual sfm so some of these projects where it's like oh I don't need to reinvent an entire photogrammetry pipeline I can take one that's already out there and then build upon it and extend it or just use it just to see like hey what what sort of results do I get out of it so this project like Nerf Studio where you're saying you're able to take you know all these sort of state-of-the-art methods make it modular so that you know both researchers as well as Engineers can you know give it a try you know extend it if they want um that that's such a valuable valuable effort yeah I find this interesting because it's it's a project that excites Jared you know a a an actual engineer someone who can jump in the code base and then someone like me who just checks on it every now and then stays active in the Discord and one day you're just saying hey you know pip install or pip upgrade your code base and also and I got these new features these new methods I can play with and I don't I don't have to I don't have to add to the I don't have to be committing to the GitHub repo I don't even have to build the source code from scratch necessarily like I did with instant NGP you guys made it easy and very very tangible you know way to use it um so it's it's kind of crosses the I would say armchair you know quarterback just want to go in there and play with it and see what it can do and then yeah Jared who's like no maybe we want to take some serious looks at modifying this and adding to this this this stack of technology so how would someone get find this I mean just I I have I can bring it up on my screen but um like what is this um yes I mean you should be able to just Google Nerf studio and hopefully uh the SEO is really high enough uh the the website is Nerf dot studio um easy I'll make sure I link it in all the show notes to All these uh you know the podcasts and YouTube video but uh do you do you suggest they go here uh which I have my screen if you're on the YouTube you can see I kind of have the web page pulled up you start here and not not at the GitHub level yeah I think starting well I mean I you can quickly get to the GitHub from here but yeah hopefully you don't have to go to the GitHub if you don't want to like like we want to be accessible for people who don't need to do anything coding related on the left side we've got something for uh installation if you walk through those steps hopefully you could be up and running uh uh takes a little bit of time to install but uh once it's not not that long though so I you know I hope to make a guide on getting this installed for the people who've never even touched command line uh you know just take it down to the very simplest core there but um it's also worth noting that um we do want to make this more accessible to people who like don't have gpus so in the top left there's a button that says collab clicking that will actually launch a Google hosted program that like gives you essentially a free compute now it's not going to be like top tier compute so it's a little bit slow going through it but it will allow you to like replicate these results without shelling uh hundreds of dollars for your own fancy Nvidia GPU if you didn't get a hold of one that's you know yeah there was at least on the 40 90s I think there's a thread of people just looking for when they get post you know get get go on sale so well anyways I you know that I've taken enough of your time today um just want to wrap this up so again it's Nerf studio uh I know that there's also a wonderful Discord Channel where people are engaged and asking questions and answering them and I mean that's one way to get a hold of you you Matt you just you're in there very actively I noticed uh responding to people explaining things in that we're all grateful for that it's it's rare that there's a research or some sort of you know this level of of Novel technology that you can just go and ask questions on it on a channel and get the actual researchers to respond you know if it's a big company a big Corporation a lot of times you're not dealing with the people closest to the software it's some sort of Representative who's asking you a question then go in and try to answer it so that's that's quite exciting um house house can we follow your work Matt um are you are you uh are you active on social or your website that's a better place to follow yeah so I mean my my personal website's just my last name.com um but for all the Nerf Studio stuff we've got at Nerf Studio team on Twitter where we post updates and renders um I think that's probably the best bet great I'll make sure those are all Linked In the show notes as well so people can can find different places to to see what you're you're up to and follow this journey at Nerf studio and uh as always again just thank you Matt and uh if you if you are listening or watching this before sub uh sorry no November 17th uh Jared and I actually gonna host a live uh computer of a computer vision decoded event in Durham North Carolina where where Jared his head headquarters is and we're actually going to show off just derfs and talk about a little bit and uh Matt you don't know it but we're gonna we're gonna use Nerf Studio because I I think it is it will be great for that Community to say hey here's Nerfs if you don't know about it or you're interested in it go look at this we're also going to use the record 3D you know both workflow you have so we can just capture someone and within within you know a few minutes maybe the time it takes a chance for the data initiate Atlanta code we can have a Nerf training which to me is really exciting too because you know it's like one thing to say okay we took pictures and in 30 minutes after we get all the pose estimations we get to start trading well now we can skip that so um really excited we'll make sure we share share how that goes as well with you Matt so um thanks Matt thanks Jared for being on this episode and I look forward to seeing you guys all in the next one yeah thanks for having me on thank you\n",
        "\"\"\"\n",
        "import textwrap\n",
        "\n",
        "print(textwrap.fill(text, replace_whitespace=False, drop_whitespace=False))"
      ],
      "metadata": {
        "id": "3WCYbBLNHTdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8b2b25-2bc1-49d8-8c51-36be532a7ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summarize this youtube transcript with bullet points: \"welcome to \n",
            "computer vision decoded where we discuss topics in the quickly \n",
            "evolving world of computer vision in today's episode we're going to \n",
            "dive into one of the hottest Topics in computer vision that's neural \n",
            "Radiance Fields whereas most people know it Nerfs as always I'm joined\n",
            " by our chief scientist of everypoint Jared heinle and today I'm \n",
            "extremely excited to have our first guest on the show Matt tancic \n",
            "Matthew is currently pursuing a PhD in the computer science and \n",
            "electrical engineering department at UC Berkeley last but not least \n",
            "he's building Nerf Studio a collaboration friendly studio for nurse \n",
            "with a few other students at UC Berkeley thank you two for joining uh \n",
            "just start off um I just want to get a little bit of a background Matt\n",
            " what what set you on the Journey of of pursuing computer vision and \n",
            "um you know your whole background that led you up to all this this \n",
            "Nerf research uh yeah that's a good question so um I guess I first got\n",
            " a computer when I was in maybe Middle School an old hand-me-down \n",
            "laptop from my parents and wasn't allowed to play or download it \n",
            "couldn't download games right but the one thing that it was able to \n",
            "run was blender so that was sort of like the first program I actually \n",
            "had access to and started playing around with and I'd say that's kind \n",
            "of where it started um first internship I got was using blender full \n",
            "time lots of reconstructing scenes from point clouds captured from \n",
            "lidar and yeah and then going throughout undergrad sort of switched to\n",
            " the more academic route taking classes wasn't really sure what I'd do\n",
            " after that but realized that you could actually contribute to these \n",
            "Cool Tools like blender and build off of them so that kind of got me \n",
            "interested in what you actually needed to do to contribute which is \n",
            "learn how to program they're in these various 3D Graphics methods and \n",
            "ultimately ended up here at Berkeley working on my PhD where worked \n",
            "with a great group of folks on building out Nerf and the various \n",
            "variants sets all right I mean look at your background prior to \n",
            "Berkeley you were at MIT was that correct yep all right that's a huge \n",
            "difference uh East Coast Cold versus you know the warm weather in \n",
            "Berkeley I guess kind of warm depending what time of year um all right\n",
            " and it sounds it's interesting I don't come from the computer science\n",
            " background that you two come from but it sounds like you know you \n",
            "have similar backgrounds that Jared what can you can you talk about \n",
            "yours too because I think that's kind of interesting how people end up\n",
            " in this this really it's kind of a hot field right now but you know \n",
            "you don't just fall into computer vision as a research yeah yeah no \n",
            "it's similar like with me it was for me it was high school getting my \n",
            "first graphing calculator you know the TI-83 or whatever and so there \n",
            "uh I did put games on it you know Tetris and you know Space Invaders \n",
            "or whatever it was you know and then very quickly was like okay how \n",
            "can I make games and then trying to figure out programming oh how can \n",
            "I make 3D games and then that led me on the path of oh can I do 3D \n",
            "programming and then I you know did my computer science degree got \n",
            "interested in Ray tracing research just to you know render you know \n",
            "really nice looking images and then from there was like okay well I \n",
            "can render images and how can you take images and turn that into \n",
            "computer vision so that was a big game changer for me realizing that \n",
            "there was a whole world of computer vision that uh I just I'd never \n",
            "seen before but yeah I love loving the the you know that that Graphics\n",
            " that Imaging that blender uh background that's really cool yeah \n",
            "there's a lot of different paths to get where you are yeah um all \n",
            "right so Jared do you want to tee up tee up one of the first questions\n",
            " or I can take it from here no I was gonna say I mean so for me um I \n",
            "guess you know so my background I deal with 3D reconstruction you know\n",
            " a lot of image based um 3D reconstruction but Nerfs you know that's \n",
            "not my I haven't I haven't done research into Nerfs you know like you \n",
            "have and so I guess just to start that off can you explain to us at \n",
            "the highest level you know what is a Nerf and how is it different from\n",
            " something that I'm familiar with more like a photogrammetry based \n",
            "output you know whether that's a point cloud or a textured mesh um \n",
            "yeah what what is a Nerf what does that even mean yeah so I'm going to\n",
            " start by explaining what a Nerf isn't um so what a derf isn't is a \n",
            "mesh for people who are not super familiar with Graphics it's \n",
            "basically the way that almost everything that you see in 3d is \n",
            "represented so the world video games TV shows when they want to use 3D\n",
            " rendering they'll use a mesh which is just a scene that is made up of\n",
            " a bunch of triangles where each triangle has a different color and if\n",
            " you think about why we use meshes it's because it's something that we\n",
            " can actually like easily or somewhat easily interact with the world's\n",
            " quite complex but we have to make some simplifications so if I know \n",
            "that I have a flat wall you could try to model it on like every single\n",
            " atom to describe the whole wall That's obviously way too complex so \n",
            "instead I just create a big triangle a bunch of big triangles color it\n",
            " white in this case and that's my wall so this turns out to be a very \n",
            "easy thing for like humans to sort of work with you just sort of move \n",
            "around the corners of these triangles but for various reasons it's \n",
            "actually kind of tricky for a computer to operate in that regime you \n",
            "because the way that we want to sort of create these computer \n",
            "algorithms to reconstruct scenes they will sort of start with some \n",
            "mesh and try to deform it or modify it until it reaches some Target \n",
            "shape and that you have to ask what question like the question of what\n",
            " do you start with so you could start with like a sphere and then you \n",
            "sort of slowly deform it over time until you get say a water bottle \n",
            "that I'm interested in now in this case it's probably going to do a \n",
            "pretty good job but you've got one problem here which is like there's \n",
            "this Loop right now there's no way I can modify a sphere such that I \n",
            "still get that hole so you run into these sort of edge cases and \n",
            "problems when you try to use meshes in the uh sort of computer like \n",
            "having the computers operate with them so for that reason we're gonna \n",
            "approach the problem from a very different approach one that's maybe \n",
            "harder for humans to interact with but easier for our machines to \n",
            "interact with and this is more of the approach of like let's try to \n",
            "model every atom in the scene now in practice it's not every atom in \n",
            "the scene but it's like bigger chunks of space in the scene so I think\n",
            " the closest analog is something like uh Minecraft so Minecraft you \n",
            "model the world with cubes right and you can draw them you can kind of\n",
            " make whatever shape you want people make rather crazy objects but \n",
            "these are pretty Big Blocks so let's just imagine Minecraft but now we\n",
            " make those blocks smaller and smaller and we can represent most \n",
            "things in the world so we call this a volumetric representation \n",
            "because we're modeling the volume every place in space so Nerf is a \n",
            "volumetric method that can represent the like realistic \n",
            "representations of saints where some of the Novelties of Nerf come \n",
            "that come in that sort of differentiate it from something like \n",
            "Minecraft is sort of along two Dimensions the First Dimension is um \n",
            "you want these really small cubes or you want to be able to represent \n",
            "small regions of space turns out if you try to do it with something \n",
            "explicit like a bunch of Cubes it just doesn't fit on the computer \n",
            "anymore so you have to come up with a different way to describe the \n",
            "scene so that you can represent a very large scene so I'm not going to\n",
            " go too much into the details but a lot of the uh like the original \n",
            "Nerf method will essentially use the same technology that machine \n",
            "learning uses a neural network to represent all these little \n",
            "essentially cubes in space and then more recent methods kind of blend \n",
            "that with the minecrafty approach of having actual cubes and it turns \n",
            "out you can get some speed ups and that's where you see a lot of these\n",
            " methods that can work really fast kind of blend these different \n",
            "methods together so that's one difference the other difference is you \n",
            "like the real world the color of a pointed space isn't constant if I \n",
            "move around it'll change colors right if you look at this water bottle\n",
            " from one angle versus the other you get Reflections in what we call \n",
            "specularities on these objects so we want to make sure that as we \n",
            "represent all these little pieces in space they have a way to change \n",
            "their color depending on the direction I'm looking at it and that \n",
            "color that we're changing is one way to think about it is it's the \n",
            "reflectance or the radiance Radiance is the color emitted from a \n",
            "particle in a certain direction and hence the name neural Radiance \n",
            "field so it's a field so a field being a space full of particles that \n",
            "are represented by these particles with different radians and we're \n",
            "going to use a neural network to actually store that information so \n",
            "that's a neural Radiance field okay so that that makes sense to me how\n",
            " about you Jared yeah yeah so to me that's so it sounds like it's a \n",
            "very you know fundamental difference in the representation you know \n",
            "like you said like so a triangle mesh you know you've got vertices \n",
            "you've got triangles you're representing sort of like these solid \n",
            "surfaces you know like you know there's empty space there's a surface \n",
            "where that triangle is and there's empty space behind it you know and \n",
            "you're linking these triangles together represent that surface or the \n",
            "boundary of objects you know that being one representation another \n",
            "representation you know I work a lot with this point clouds you know \n",
            "there there is no sort of representation of the surface it's just a \n",
            "bunch of sort of independent samples of here's just a point in space \n",
            "um and there you know it may not be on some sort of regular grid it \n",
            "couldn't sort of be random or a sparse or um there's going to be \n",
            "patterns to it but that's you know there's no representation of the \n",
            "surface whereas you're saying here with this you know volumetric \n",
            "representation this is sort of a third you know different \n",
            "representation that's that's representing um you know sort of you know\n",
            " the density or the um the radiance um at this at you know all these \n",
            "points in space but um yeah that makes sense about having to you know \n",
            "a lot of the you know issues of it just you know trying to do a brute \n",
            "force and store all of that information at once you know is is way too\n",
            " much and having to deal with you know various ways of of compressing \n",
            "or reducing that data um to make it make it practical oh that's cool \n",
            "Yeah you mentioned Point clouds like one way to think about this is \n",
            "this is just a really really dense Point Cloud um with the exception \n",
            "that it the points can change color depending on the direction you \n",
            "look at it yeah yeah I think that that view depended like the scene \n",
            "change Sheen as you change the view of the camera is part of the magic\n",
            " I that that has kind of caught the imagination of all the creatives \n",
            "and just I post something online you're seeing you're seeing shadows \n",
            "tracing through you know or Reflections chasing through through water \n",
            "I think I think that's what people are seeing there this is they're \n",
            "changing the camera well the appearance of that part of the scene has \n",
            "now changed and and you just don't see if you're not seeing that any \n",
            "other way except for a lot of manual work of people you know doing \n",
            "things for like games where they have to you know do a bunch of fancy \n",
            "tricks with with with meshes and textures and you know post-production\n",
            " that you're not doing that I'm not doing any post-production on Earth\n",
            " and train the Nerf I render a camera animation of the scene and it \n",
            "just it just happens it's worth noting that these like reflection \n",
            "properties are something that we like hope the model predicts the \n",
            "correct Reflections but in many cases there are ways to cheat so using\n",
            " the same representation of space there are ways to make it so that it\n",
            " looks correct through this optimization procedure from the images \n",
            "that you give it as input but in practice the thing it reconstructs \n",
            "like the underlying density and geometry is incorrect and so this is \n",
            "one of the reasons why people do differentiate this versus uh existing\n",
            " approaches that are more focused on the actual geometry because you \n",
            "can't really trust the geometry in nerves and so one good example is \n",
            "something like a mirror a mirror you could represent the scene as \n",
            "having density at the surface and then depending on the direction that\n",
            " I look at each of these points it changes color and that would be the\n",
            " correct way to represent it there's another way that you could \n",
            "theoretically represent it though which is to represent it as a whole \n",
            "that you look through the world and you see the reflected uh world \n",
            "like at the same depth that it would be if I were like looking behind \n",
            "me at that location in the mirror and so the computer is kind of \n",
            "deciding during this optimization which one to choose and there's ways\n",
            " that you can try to like force it to choose one versus the other uh \n",
            "by like tweaking some parameters but in practice like either one \n",
            "satisfies the goal that you give that computer to begin with so you do\n",
            " get these like weird artifacts often and on an artistic sense it can \n",
            "be like quite compelling I think people may have seen those some \n",
            "videos people have made where they've done Nerfs of like mirrors and \n",
            "other reflective objects and you can like fly into like the mirror \n",
            "World um but depending on your ultimate application like that may not \n",
            "be desirable oh that's so cool that's so cool like your comment there \n",
            "about you know having it look good even though the geometry is bad I \n",
            "mean I see it a lot of times with with textured meshes it's like oh \n",
            "you can have a really low you know low quality mesh but then you \n",
            "texture it with some texture and all of a sudden it looks like it's \n",
            "amazingly detailed um this sounds like it's like that problem you know\n",
            " just you know to the next degree of it's uh you can have this is you \n",
            "know dealing with Reflections and yeah that whole thing about well is \n",
            "it the is the mirror actually that flat surface or is there a mirror \n",
            "world you know hidden behind that that space that's so cool I mean \n",
            "it's worth mentioning like a little bit of the history here so things \n",
            "like photogrammetry are interested in reconstructing uh the correct \n",
            "geometry of the scene and when we started the Nerf project we came at \n",
            "it from a completely different sphere our goal is can we create good \n",
            "looking images we didn't care about the underlying geometry at all \n",
            "which is why we were like let's just represent the world as this sort \n",
            "of like Cloud it's just a lot of little particles there's nothing that\n",
            " forces it to be it correct we just care about pretty looking pictures\n",
            " in the end and when we like if you look at the original Nerf papers \n",
            "that is sort of what we focus on like oh we can get these good looking\n",
            " images out of it it was sort of by accident that you can actually get\n",
            " pretty good geometry in many cases um and use it for other \n",
            "applications indeed okay so going down that that line then of uh now \n",
            "you've you've you've built the first NERF and you guys have iterated \n",
            "on it and improved it or maybe not improved but have different \n",
            "versions of it now and that some of the geometry is appearing to look \n",
            "better um do you see a a future where perhaps photogrammetry becomes \n",
            "this deprecated model or method to to create an accurate \n",
            "representation of the physical surfaces in the world and we we use \n",
            "Nerfs instead or or do you always find that this will be kind of a \n",
            "never reach that same level of accuracy that you would get you know \n",
            "are they are they is that kind of like a we that will supersede it or \n",
            "or there'll just be different methods that have different pros and \n",
            "cons um I mean I think they'll always exist multiple methods but one \n",
            "thing that kind of surprised us is we released the paper we got good \n",
            "looking quality results but a lot of people are interested in the the \n",
            "actual geometry and so what you end up seeing is a lot of the follow-\n",
            "up work uh particularly in the computer vision Community has focused \n",
            "on the geometry aspects and that's why you do see that like there's \n",
            "methods coming out with much better geometry and one thing to note is \n",
            "a lot of these methods will actually take in the some of the ideas \n",
            "from photogrammetry like things that we know work well in \n",
            "photogrammetry import those ideas into Nerf in order to improve the \n",
            "geometry so it's not that like one will supersede the other but it'll \n",
            "be sort of a combination of merging that makes sense because yeah uh \n",
            "yeah no I I think that's valuable I mean just because that sharing of \n",
            "knowledge between between the different approaches I mean I've seen \n",
            "that from just like the photogrammetry side like using pure just you \n",
            "know heuristics and algorithms talk oh let's let's you know \n",
            "triangulate points in space and then compare patches of pixels to see \n",
            "which one you know what is the depth of this point and so there's a \n",
            "whole you know decades and decades worth of literature about oh here's\n",
            " the best methods to do that you know and now some of those are being \n",
            "superseded by you know learned learned based approaches where you can \n",
            "say well no machine learning method you know neural network can do a \n",
            "better job at figuring out well given these two you know images you \n",
            "know what is the depth of a particular pixel you know and so you start\n",
            " you know leveraging the best tool for the job in that you know goal \n",
            "of you know reconstructing or understanding the scene I I think one \n",
            "interesting data point and this is by no means general but one thing \n",
            "I've experienced uh when I have a lot of images that are well captured\n",
            " of a scene and I have like good camera poses uh if I also have \n",
            "additional depth information say from a lidar scan it's often \n",
            "beneficial not to take it into account because even though like lidar \n",
            "is very very accurate any sort of misalignment which is almost uh \n",
            "guaranteed given that like you've got rolling shutter artifact various\n",
            " artifacts that happen that misalign the images from the lidar plus \n",
            "the surfaces of like the edges of objects with lidar can be sometimes \n",
            "kind of noisy not taking into the lightering account when you have \n",
            "lots of images uh you can actually produce better quality depth from \n",
            "images alone so I think that's like a bit surprising again it's not \n",
            "General but in in some cases that does end up happening and so I think\n",
            " it's worth like asking uh when we think about how we do these like \n",
            "photogrammetry type setups where we do care about recovering geometry \n",
            "like maybe there are other approaches that involve just like taking \n",
            "lots of photos and using nerve-based approaches now if you have \n",
            "something like a wall that is all the same color Nerf and photo \n",
            "parametry is gonna are gonna fail to produce like a a good \n",
            "representation of that where lidar is going to do quite well so I \n",
            "found lidar type deaths are helpful when you have scenes like that or \n",
            "if you have cases where you just don't have very many images but if \n",
            "you've got a lot of images a nerves can be kind of compelling one I've\n",
            " seen in my personal explorations of hundreds of Nerfs now I've just \n",
            "kind of thrown all kinds of different scenes in and as you said it \n",
            "doesn't know where the sky is so you end up when if you don't look at \n",
            "it from the right direction so you see perhaps what would be a cloud \n",
            "over a building and it's like the cloud is actually in front of the \n",
            "building if you were to look at it from the wrong direction because it\n",
            " just doesn't know how to represent that scene from that that angle at\n",
            " all um you know he just doesn't have that depth information it says \n",
            "oh there's something there I can't tell if it's I'm guessing this just\n",
            " doesn't know if it's really far away it's really close but I find \n",
            "that if then also I get more more images like if I'm it's an unbounded\n",
            " scene I get that but if I manage to like walk around a building it \n",
            "starts to say oh well I saw it from the side profile now and no there \n",
            "is no Cloud hanging out out in front of that building it's it's it's a\n",
            " blank space it's a you know transparent there so it is kind of kind \n",
            "of cool to see how you can you can change your image set to get less \n",
            "of that um ambiguity of of the ratings field there yeah because I mean\n",
            " it produced the correct image for that Viewpoint that is trained up \n",
            "right and so the trick is like how do you also help make it produce \n",
            "good images from new viewpoints and it does pretty well when you're \n",
            "like in the regime like the same areas that you've captured from as \n",
            "you deviate from that you're going to start seeing artifacts like that\n",
            " and then in a case like that this is where like you can step in and \n",
            "say well we know that the sky AI or things up high like are often very\n",
            " far away so you can start programming in some prior knowledge saying \n",
            "okay if I see something like that put it as far back as possible you'd\n",
            " be a little bit careful there because now let's say we have some \n",
            "fixed color that like the blue sky it doesn't change depending on how \n",
            "I look at it so I can put it infinitely far away but if I look at the \n",
            "ground maybe it's like a nice new concrete ground it looks the exact \n",
            "same color I could also put that infinitely far away so it becomes \n",
            "like a game of which sort of prior knowledge do you want to \n",
            "incorporate and any prior knowledge that you do incorporate it could \n",
            "potentially like hurt other aspects yeah so Jared even know that \n",
            "there's a lot of balance and a lot of um as I've dove into the world \n",
            "of computer vision finding out how much you guys just spend \n",
            "experimenting with different variables and just seeing how you know \n",
            "run the data over and over with modifications to get that desirable \n",
            "effect for the hopefully known scene that you're going to be \n",
            "representing more than once you know in the future and say okay well \n",
            "we know if we're doing this sort of scene we'll use these parameters \n",
            "and a lot of that's just trial and error see see how it ends up yeah \n",
            "yeah so much goes and just understand because it's you can develop \n",
            "algorithms you know the ideal be to develop an algorithm you know a \n",
            "method that can work anywhere under any environment under any \n",
            "circumstances but you know a lot of times having some knowledge about \n",
            "oh okay I'm gonna be reconstructing man-made objects and so therefore \n",
            "we there's going to be a lot of stuff that's going to be flat you know\n",
            " or oh I'm going to be in an outdoor environment where there's going \n",
            "to be no repetitive structure you know everything looks unique and \n",
            "random and I can sort of you know use that as some sort of constraint \n",
            "or you know help influence the algorithms that be developed that are \n",
            "being being developed that makes sense that makes sense all right so I\n",
            " have some uh Jared I have some questions around uh block Nerf which \n",
            "maybe one of the more novel you know or interesting ways uh that Nerfs\n",
            " have been used that I think I think that one circulated the internet \n",
            "for a good while um and uh I just I think it was interesting was was \n",
            "three was it 360 images 360 camera images as input um is that is that \n",
            "is that correct is that that how you would you see this technology for\n",
            " capturing large spaces to me more like a 360 camera array or um was \n",
            "that you know was you guys just something something that was unique \n",
            "that waymo had lots of just different cameras all over that um you \n",
            "know their their cars that capture that data so I you can kind of \n",
            "think of it as a 360 camera but in reality it wasn't it was the car \n",
            "itself has I forget exactly how many cameras somewhere around eight or\n",
            " nine cameras that are placed looking in all directions so in that \n",
            "sense like you can fuse them all together to make a 360 uh render the \n",
            "slight modification there and this is an important aspect of nerve is \n",
            "you want to view the scene from as many locations as possible a 360 \n",
            "camera the camera sensor is at one location looking in all Direction \n",
            "so while you see everything the the source is the same location uh the\n",
            " cars are a little bit different because the cameras are like spread \n",
            "across uh I think like there may have been some on the like front \n",
            "bumper or some on the top some on the back so there's a little bit of \n",
            "spreading as to where the camera sensors are so it's a little bit \n",
            "better information than what you'd get with the 360 camera that being \n",
            "said if you just walk around with that 360 camera you solve that issue\n",
            " mm-hmm yeah that's the same same problem we have there in \n",
            "photogrammetry too it's like if you only have observe the the scene \n",
            "from one position in space yeah you can't triangulate that depth which\n",
            " sounds like that's the same problem there with with Nerfs yeah and I \n",
            "I kind of pulled up the the waymo video the video that uh by the way I\n",
            " noticed that you you narrate a lot of these videos just to Glad a \n",
            "little humor here do you guys draw straws who who does that or is that\n",
            " just uh who who decides who narrates these papers that's just \n",
            "something I've always noticed a variety of things I mean a lot of \n",
            "these are like who's still awake at whatever time of night before the \n",
            "uh submitted to a conference um yeah but uh I just thought it was \n",
            "funny it's like maybe they always make Matt go up because he's he's \n",
            "done a few of these now um go ahead oh I was gonna say just to take a \n",
            "step back I mean so Jonathan's showing here these amazing visuals is \n",
            "you know just you know Urban uh Urban environments downtown uh San \n",
            "Francisco or whatever being reconstructed can you just Matt can you \n",
            "just take a you know take a step back and just kind of explain at a \n",
            "high level you know what what is what is block Nerf or what was the \n",
            "goal of uh block Nerf yeah so the block Nerf is a project that was \n",
            "done with waymo which is uh Google's self-driving car effort and their\n",
            " main goal is I mean essentially to make like autonomous cars to drive\n",
            " across all these streets they can obviously test the cars on the \n",
            "streets but they can only go so far right so if you want to start \n",
            "simulating new scenarios or whatever it might be it's useful to have a\n",
            " 3D representation of the world that you can like interact with and so\n",
            " traditionally you would potentially hire someone to try to create 3D \n",
            "assets of all these elements but in this particular case you've got \n",
            "these cars with all these cameras on them can you just automate the \n",
            "process that that was the goal and so you've got photogrammetry \n",
            "methods that could potentially work on it I had experience in Nerf \n",
            "stuff so it just seemed like a natural idea can we apply Nerf to the \n",
            "data that the waymo vehicles have already captured to just reconstruct\n",
            " them but this leads to a few challenges that we don't normally have \n",
            "or at least the original Nerf papers didn't really deal with and it's \n",
            "really one of scale so while there's a lot of data that is of \n",
            "different regions of space you have to take into account that they're \n",
            "taken at like different times and that different objects were around \n",
            "so a lot of that paper was just a matter of like pulling it together \n",
            "or pulling together various other approaches that have been proposed \n",
            "in other papers that kind of account for these different aspects so \n",
            "things like changes in weather things like there's a vehicle driving \n",
            "by how do you handle all that okay that makes sense that makes sense \n",
            "and what you said earlier you know talking about how you know Nerfs \n",
            "motivations being around generating that you know nice looking image \n",
            "that realistic looking image versus just you know perhaps accuracy you\n",
            " know just raw Brute Force accuracy of the geometry that makes sense \n",
            "if you're trying to you know simulate the world you know and simulate \n",
            "the world through Visual means having those really nice looking \n",
            "accurate images um yeah that seems like a great application of Nerf \n",
            "yeah because I mean the self-driving car companies or most of them \n",
            "like waymo they have lidars on them so they have a some estimate of \n",
            "the 3D geometry of space and so simulating lidar it's not easy but \n",
            "it's a little bit more straightforward um and that's something where \n",
            "you can actually use more crude 3D models that already exist that you \n",
            "can just like create a lidar sensor and just have it travel through \n",
            "that space visual is harder right uh what actually is fed into the \n",
            "camera you want to be something that looks better realistic now I \n",
            "think the video that you're just showing like is a step there it's not\n",
            " photorealistic yeah right there's issues uh and a lot of those issues\n",
            " come down to the fact that this data was captured over long periods \n",
            "of time and so you lose some of the constraints that you normally have\n",
            " or assumptions that you normally have that like the Cena static so \n",
            "when the leaves on the trees change we don't really have a great way \n",
            "to account for that so as a result it sort of blurs it all out and \n",
            "kind of predicts the average of all the different Leaf positions or \n",
            "when you look at this data over a long enough period of time you \n",
            "actually see like the tree grows right so you're looking at the trees \n",
            "either short or tall or no no like literally it grows over a month or \n",
            "two period of time right okay so um or the leaves fall off the trees \n",
            "that's cool we did have a past episode um about uh you know computer \n",
            "vision in the wild I think was the title and it was you know those \n",
            "challenges so Jared has a lot of experience as well with just \n",
            "crowdsourcing you know hundreds of thousands of images and it's just \n",
            "no matter what you're not going to get Perfection but it's amazing \n",
            "what you can pull off to get a good understanding of a scene even with\n",
            " all that I you know mismatch of information between images um you \n",
            "know we get I mean Jared was even using a mix of cameras that we \n",
            "didn't even know what the camera they were a scrape from Photobucket I\n",
            " think or one of the one of the big you know crowdsource image sets \n",
            "where you didn't know what the camera was just had images so um it's \n",
            "impressive what he pulled off especially also that block Nerf was was \n",
            "done that was that wasn't done like last week that was a while ago so \n",
            "you know not that that you can solve those problems necessarily today \n",
            "but you know I've already seen the quality of Nerfs just improve as \n",
            "you as this has evolved and I think people forget they say oh well \n",
            "this is no not as good as photogrammetry photogrammetry has been \n",
            "around since you know decades versus a couple years so there's a lot \n",
            "to figure out a lot to optimize so I just thought was a really good \n",
            "example of how he's kind of like you said you pulled in a lot of ideas\n",
            " to pull off this large Nerf uh so when when one thing I noticed that \n",
            "people kind of struggle with with Nerfs is is it takes an enormous \n",
            "amount of GPU power you know you can't just run it on a you know \n",
            "entry-level laptop or desktop you need that high-end GPU um can you \n",
            "can you talk about that like uh do you see that that that burden goes \n",
            "away in the future is that just the reality of the technology and and \n",
            "if so also how then how do you expect people to to be interacting with\n",
            " with nerves so we're going to be doing it on cloud-based environments\n",
            " where you don't have to it's just you know close hosted out there in \n",
            "the the cloud uh how do you see that that for that moving right they \n",
            "they are compute intensive and the maybe I'll briefly describe why \n",
            "they're compute intensive um I described that you represent the world \n",
            "as sort of lots of little cubes or like little little chunks of space \n",
            "um but since it's all being represented uh at in sort of this \n",
            "representation that often's like a neural network you don't actually \n",
            "know where it is until you ask whether there's something in that \n",
            "location in space so the way you actually render or predict the color \n",
            "of a single Pixel is you sort of like shoot a camera ray out into \n",
            "space and then along that Ray you take lots of little samples and each\n",
            " of those samples you ask is there anything here and if there is what \n",
            "color is it that asking process is actually feeding that sample \n",
            "through a neural network and popping out an answer and so if you're \n",
            "doing 500 samples on array 500 queries of Renault Network and you're \n",
            "trying to render a one megapixel image that's you're rendering like \n",
            "evaluating this neural network billing sometimes so very quickly it \n",
            "becomes quite expensive and for that reason the original Nerf papers \n",
            "they would take uh almost a day to or some taste in some cases more \n",
            "than a day just to reconstruct a single C quite expensive and for a \n",
            "while that was like people were sort of fighting getting it maybe like\n",
            " 20 faster or 30 faster but over time uh people found more and more \n",
            "tricks in order to make it so now you've got something like instant GP\n",
            " or these other fast methods I could train on the order of five \n",
            "minutes right so we've already gone multiple like multiple orders of \n",
            "magnitude uh there's different ways and different tricks that people \n",
            "use but one of the biggest tricks is instead of doing 500 samples \n",
            "along the array if you keep some information around saying that like \n",
            "there's nothing here don't sample this region you can get rid of most \n",
            "of the that computation and so as we go forward we're coming up with \n",
            "more and more tricks like that so that we don't have to do quite as \n",
            "much computation when we actually render the scene so do I think it's \n",
            "going to get faster I I think it will I mean it already has gotten \n",
            "significantly faster that being said it's still obviously uh not in \n",
            "the not something you can run on your laptop but I do think it'll go \n",
            "farther I think the other thing that's worth mentioning is uh computer\n",
            " graphics 3D Graphics we kind of take for granted now but it took time\n",
            " for them to actually be feasible to run and one of the main reasons \n",
            "they're feasible now is because we have something like a GPU which is \n",
            "literally designed to render out a bunch of these mesh triangles as \n",
            "fast as possible so we're sort of seeing people get more on board with\n",
            " these Nerf based methods I think it's totally feasible that these \n",
            "manufacturers will also start looking into like how do you actually \n",
            "optimize on a hardware level rendering these things out faster so the \n",
            "That Base approach I think will continue to get faster it might be a \n",
            "little bit slower especially if given that like Hardware modifications\n",
            " may be necessary but there's another approach that you can kind of \n",
            "look at which is uh can we convert these nerves into something else so\n",
            " if we forget about the actual creation of the nerve for a second \n",
            "let's just say the creation of the nerve stays a bit expensive and \n",
            "that's something that you run in the cloud or somewhere else once it's\n",
            " created people are coming up with more and more methods to convert it\n",
            " into a representation that still maintains a lot of the visual \n",
            "quality but is using the technologies that we've already optimized \n",
            "essentially converting it to a mesh or something similar to a mesh \n",
            "that we can run on something like a phone or normal devices without \n",
            "gpus and we already see that happening both in research and also in \n",
            "Industry uh Luma for example is a company that will do some sort of \n",
            "conversion like that which is why you can visualize it in your browser\n",
            " mm-hmm that that makes sense uh also a mobile Nerf which turned into \n",
            "a triangle mesh as well yeah running High FPS even on my you know my \n",
            "iPhone um in browser so so I just as like a interesting aside is like \n",
            "mobile Nerf it converts it to a triangle mesh but it's not a triangle \n",
            "mesh that you're used to uh what it is is it's tons and tons of \n",
            "triangles all overlapping each other like uh like we like to use the \n",
            "term triangle soup where it's just sort of like you splatter triangles\n",
            " into the scene and so it's not the way that people typically think of\n",
            " triangle meshes where a lot of people care about it being watertight \n",
            "so it's some volume that doesn't have any holes uh uh it's like nice \n",
            "and perfect that's not what these are but they can Leverage The \n",
            "advances in GPU technology to make it really really fast while also \n",
            "producing great looking renders yeah that's that's what I noticed it \n",
            "it looks great don't leave the scene it's not ready for a game um and \n",
            "also right a lot of triangles you know it's not optimized to be the \n",
            "best looking with the least amount of triangles and meshes and data \n",
            "there it's but but it works that's interesting for visualization and \n",
            "and I think a lot of people forget that Nerfs uh there's a lot of \n",
            "people say well he's what's useful with the Nerf if I don't get a mesh\n",
            " there's no use for this technology without meshes and that that is a \n",
            "definite definitely a Viewpoint from people who just need meshes they \n",
            "work in game engines they work in software that they're interacting \n",
            "with a mesh and that's that's fine but I I think that people just \n",
            "Overlook a whole entire like think of waymo they're doing self-driving\n",
            " cars right so they actually don't care about a geometry a lot of \n",
            "times in a scene it's the cars learning what the scene looks like and \n",
            "how to interact with a scene based off appearance because using camera\n",
            " guidance they're not they're not uh you know and you can simulate \n",
            "lidar data for for for some geometry but I I I think that that is a \n",
            "application where we don't have to worry about meshes and it could be \n",
            "expensive to render inexpensive to train but we're using it for these \n",
            "machine learning models that in the end it's worth all that investment\n",
            " all that time all the energy all the GPU Cycles because it's it \n",
            "actually is faster than someone perhaps modeling these objects in 3D \n",
            "in a you know in all these third-party softwares just to get one \n",
            "little scene for a car to learn how to drive around it um so I I like \n",
            "to throw that out there yeah I think I would also have sort of like \n",
            "two other responses to someone who says they just want meshes because \n",
            "they want geometry um I think that's totally valid because that's what\n",
            " our tooling is built to use right now but people well so the first \n",
            "thing is uh people are creating methods to convert nerves to meshes so\n",
            " it's not like using a nerve means that you don't get something like a\n",
            " mesh as an output right not quite as good uh but we're getting better\n",
            " and there's a lot of research in that space so I wouldn't be \n",
            "surprised if you can get pretty good meshes out of a Nerf based model \n",
            "soon um and the the second thing is people want meshes because that's \n",
            "what their tools currently use right if you got the unity game engine \n",
            "or whatever game engine you import a mesh I don't think that's always \n",
            "going to be the case you've already seen people hack together ways to \n",
            "import Nerf style models into these existing game engines and I think \n",
            "it's only a matter of time before these game engines implement it at \n",
            "more of a core level so once that is the case then do you actually \n",
            "need the mesh or you just need the mesh because that's what your \n",
            "current tooling uses yeah I agree there it's a it's a necessity right \n",
            "now but is it a mandatory requirement maybe not and uh just I you know\n",
            " I've been spending a lot of time interacting with Nvidia who \n",
            "obviously are spending a lot of time and energy and you know research \n",
            "Cycles on this technology and they haven't said anything publicly but \n",
            "you can tell that that they're working on something with the Omniverse\n",
            " off of what they've said and I can only imagine that they're going to\n",
            " have some sort of you know server-based rendering of Nerfs and not \n",
            "convert it to a mesh necessarily because you know that the whole \n",
            "Omniverse they have a lot of synthetic data rendering and things like \n",
            "that they use for machine learning and I can see how this just easily \n",
            "integrates in that whole ecosystem where you don't need the mesh in \n",
            "that tool set but like you said some other tool sets it's it's the \n",
            "reality of today I I think there's also like a just general trend of a\n",
            " lot of tool sets migrating to the cloud and once you migrate to the \n",
            "cloud then they can do whatever they want in the back end right like \n",
            "they can use all the compute they want and yeah so yeah all right well\n",
            " I just want to make sure you have enough time here uh because we're \n",
            "coming to the end of our of our time with you but I want to make sure \n",
            "we talk about um a project you're currently involved in called Nerf \n",
            "studio and I'm gonna just if people are on the YouTube version of this\n",
            " I will again have some visuals of just outputs from Nerf Studio but \n",
            "uh can you just give us a quick overview what Nerf studio is to start \n",
            "out with and I'll show some cool interesting results yeah so it's an \n",
            "open source project um that essentially tries to bring together a \n",
            "bunch of different Nerf methods so Nerf came out a little over two \n",
            "years ago now and since then there's been a lot of follow-ups uh the \n",
            "most notable ones are the ones that make it like real time and fast \n",
            "but there's also been a lot of things to improve the quality in a \n",
            "variety of different scenarios um and the challenge is if you go out \n",
            "there and look and you just want to use Nerf uh say you're a \n",
            "researcher or a programmer it's not entirely clear where you start you\n",
            " could grab like the thing that I would recommend number one is don't \n",
            "grab the original Nerf code base there's like zero reason you should \n",
            "use that anymore yet a lot of people still use it and we still get \n",
            "like GitHub issues and people trying to mess around with it but it's \n",
            "out of date uh these things have improved since then and I've \n",
            "personally experienced this uh we talked about block Nerf earlier \n",
            "block Nerf I just combined a bunch of different methods together right\n",
            " and there was a few methods that I hadn't had the chance to combine \n",
            "yet and so when I went back to uh Berkeley to work on my next project \n",
            "I wanted to do essentially block Nerf but on a uh academic uh level \n",
            "compute rather than like Google level compute and I think it's \n",
            "possible now that we have things like instant NGP which allows things \n",
            "to be quite quick and so I started hacking around I wanted to \n",
            "reconstruct the campus here at Berkeley and very quickly I realized I \n",
            "was repeating all the same things I was doing like uh combining these \n",
            "different methods that I had to do when I was at waymo and it seemed \n",
            "kind of like a waste of time and just thinking there's got to be other\n",
            " people that are also trying to combine all these different methods \n",
            "together what if we just do it the correct way or a more of a correct \n",
            "way so that when people want to use these methods in the future they \n",
            "can just piece them together are more like Lego blocks and get good \n",
            "results at the same time can we make the the UI and usability easier \n",
            "so that folks that are maybe less into the research side of things can\n",
            " use this as a tool um that combines the the best of the best that we \n",
            "have in nurse at the moment and so I with a few other grad students \n",
            "I'll highlight Ethan Weber and Yvonne we worked together to create \n",
            "this and we released it a few months ago and yeah it's been exciting \n",
            "to see other people use it um we released one model with it the \n",
            "nerfacto model and so this is just an example where we combined a few \n",
            "different methods so you asked I don't know if you asked how it like \n",
            "compares to instant NGP but like if you compare it to something like \n",
            "instant GP the new model is essentially instant NGP but like with \n",
            "other things added on top of it so it's not like it's uh trying to \n",
            "compete with it it's using those ideas and Concepts and just bringing \n",
            "in other ideas and Concepts that have happened in the space sense so \n",
            "for example now there's like a method that lets you render stuff in \n",
            "the background a lot clearer why not include that with instant NGP \n",
            "right that was my first the first comparison I took the same image set\n",
            " I was like I always want to see the difference between de facto and \n",
            "instant NGP and it's just like oh my goodness it's like the background\n",
            " mountains just snapped into Clarity which which excited me because I \n",
            "always felt like the bat you get something to get further and further \n",
            "away they almost look like they've been water colored into the \n",
            "background as opposed to just even out of focus they just like smeared\n",
            " that that's not the case yeah yeah so the goal going forward though \n",
            "is as new methods and new techniques come out and like people sort of \n",
            "agree that this is like a good direction to go we want to incorporate \n",
            "them and make an easy place for people to like contribute their own \n",
            "modules uh but also sort of leave like a history of what sort of uh \n",
            "the different approaches one can take and I think what's a little bit \n",
            "different than maybe treating this as like a traditional research \n",
            "project is that we're kind of doing this all in the open um the goal \n",
            "isn't to write some paper and I think there's great reasons to write \n",
            "papers about different methods in this area but I think there's \n",
            "equally good reasons to try to do these things more publicly and not \n",
            "just try to figure out ways to like differentiate yourself from others\n",
            " because I think you've got to accept that like there's a lot of good \n",
            "ideas in this area if we combine them all we can get something better \n",
            "and so as a result we've seen people uh outside of our core team \n",
            "starting to contribute to the repo which is the thing that we like \n",
            "that was our ultimate goal uh because the hope is to make this \n",
            "something that's like sustainable that people across industry and \n",
            "Academia can help contribute to to improve it over time all right \n",
            "that's that's such a valuable effort I mean like I know just in my in \n",
            "my work during my PhD like okay every grad student they've got their \n",
            "own project that they're working on and oh I'm gonna try to use \n",
            "someone else's code sure I might find it somewhere and then I try to \n",
            "download it compile it oh it doesn't work you know and so just you \n",
            "know that effort of just you know reproducing everyone else's work to \n",
            "try to you know build you know benefit your own you know that's I \n",
            "really appreciate these products like for me in the photogrammetry \n",
            "space you know dealing with you know a call map or open mvg or visual \n",
            "sfm so some of these projects where it's like oh I don't need to \n",
            "reinvent an entire photogrammetry pipeline I can take one that's \n",
            "already out there and then build upon it and extend it or just use it \n",
            "just to see like hey what what sort of results do I get out of it so \n",
            "this project like Nerf Studio where you're saying you're able to take \n",
            "you know all these sort of state-of-the-art methods make it modular so\n",
            " that you know both researchers as well as Engineers can you know give\n",
            " it a try you know extend it if they want um that that's such a \n",
            "valuable valuable effort yeah I find this interesting because it's \n",
            "it's a project that excites Jared you know a a an actual engineer \n",
            "someone who can jump in the code base and then someone like me who \n",
            "just checks on it every now and then stays active in the Discord and \n",
            "one day you're just saying hey you know pip install or pip upgrade \n",
            "your code base and also and I got these new features these new methods\n",
            " I can play with and I don't I don't have to I don't have to add to \n",
            "the I don't have to be committing to the GitHub repo I don't even have\n",
            " to build the source code from scratch necessarily like I did with \n",
            "instant NGP you guys made it easy and very very tangible you know way \n",
            "to use it um so it's it's kind of crosses the I would say armchair you\n",
            " know quarterback just want to go in there and play with it and see \n",
            "what it can do and then yeah Jared who's like no maybe we want to take\n",
            " some serious looks at modifying this and adding to this this this \n",
            "stack of technology so how would someone get find this I mean just I I\n",
            " have I can bring it up on my screen but um like what is this um yes I\n",
            " mean you should be able to just Google Nerf studio and hopefully uh \n",
            "the SEO is really high enough uh the the website is Nerf dot studio um\n",
            " easy I'll make sure I link it in all the show notes to All these uh \n",
            "you know the podcasts and YouTube video but uh do you do you suggest \n",
            "they go here uh which I have my screen if you're on the YouTube you \n",
            "can see I kind of have the web page pulled up you start here and not \n",
            "not at the GitHub level yeah I think starting well I mean I you can \n",
            "quickly get to the GitHub from here but yeah hopefully you don't have \n",
            "to go to the GitHub if you don't want to like like we want to be \n",
            "accessible for people who don't need to do anything coding related on \n",
            "the left side we've got something for uh installation if you walk \n",
            "through those steps hopefully you could be up and running uh uh takes \n",
            "a little bit of time to install but uh once it's not not that long \n",
            "though so I you know I hope to make a guide on getting this installed \n",
            "for the people who've never even touched command line uh you know just\n",
            " take it down to the very simplest core there but um it's also worth \n",
            "noting that um we do want to make this more accessible to people who \n",
            "like don't have gpus so in the top left there's a button that says \n",
            "collab clicking that will actually launch a Google hosted program that\n",
            " like gives you essentially a free compute now it's not going to be \n",
            "like top tier compute so it's a little bit slow going through it but \n",
            "it will allow you to like replicate these results without shelling uh \n",
            "hundreds of dollars for your own fancy Nvidia GPU if you didn't get a \n",
            "hold of one that's you know yeah there was at least on the 40 90s I \n",
            "think there's a thread of people just looking for when they get post \n",
            "you know get get go on sale so well anyways I you know that I've taken\n",
            " enough of your time today um just want to wrap this up so again it's \n",
            "Nerf studio uh I know that there's also a wonderful Discord Channel \n",
            "where people are engaged and asking questions and answering them and I\n",
            " mean that's one way to get a hold of you you Matt you just you're in \n",
            "there very actively I noticed uh responding to people explaining \n",
            "things in that we're all grateful for that it's it's rare that there's\n",
            " a research or some sort of you know this level of of Novel technology\n",
            " that you can just go and ask questions on it on a channel and get the\n",
            " actual researchers to respond you know if it's a big company a big \n",
            "Corporation a lot of times you're not dealing with the people closest \n",
            "to the software it's some sort of Representative who's asking you a \n",
            "question then go in and try to answer it so that's that's quite \n",
            "exciting um house house can we follow your work Matt um are you are \n",
            "you uh are you active on social or your website that's a better place \n",
            "to follow yeah so I mean my my personal website's just my last \n",
            "name.com um but for all the Nerf Studio stuff we've got at Nerf Studio\n",
            " team on Twitter where we post updates and renders um I think that's \n",
            "probably the best bet great I'll make sure those are all Linked In the\n",
            " show notes as well so people can can find different places to to see \n",
            "what you're you're up to and follow this journey at Nerf studio and uh\n",
            " as always again just thank you Matt and uh if you if you are \n",
            "listening or watching this before sub uh sorry no November 17th uh \n",
            "Jared and I actually gonna host a live uh computer of a computer \n",
            "vision decoded event in Durham North Carolina where where Jared his \n",
            "head headquarters is and we're actually going to show off just derfs \n",
            "and talk about a little bit and uh Matt you don't know it but we're \n",
            "gonna we're gonna use Nerf Studio because I I think it is it will be \n",
            "great for that Community to say hey here's Nerfs if you don't know \n",
            "about it or you're interested in it go look at this we're also going \n",
            "to use the record 3D you know both workflow you have so we can just \n",
            "capture someone and within within you know a few minutes maybe the \n",
            "time it takes a chance for the data initiate Atlanta code we can have \n",
            "a Nerf training which to me is really exciting too because you know \n",
            "it's like one thing to say okay we took pictures and in 30 minutes \n",
            "after we get all the pose estimations we get to start trading well now\n",
            " we can skip that so um really excited we'll make sure we share share \n",
            "how that goes as well with you Matt so um thanks Matt thanks Jared for\n",
            " being on this episode and I look forward to seeing you guys all in \n",
            "the next one yeah thanks for having me on thank you\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GX_JOtk0HVpS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}