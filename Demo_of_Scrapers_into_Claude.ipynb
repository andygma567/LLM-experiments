{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andygma567/LLM-experiments/blob/main/Demo_of_Scrapers_into_Claude.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbuN-8oPXnFh"
      },
      "source": [
        "- [Claude](https://www.anthropic.com/product) can work on 1hour videos (200k token input)\n",
        "\n",
        "- [Chat GPT 3.5] (https://chat.openai.com/) (4k token input)\n",
        "\n",
        "- ~~[Llama2](https://labs.perplexity.ai/) (3-4k characters / 512 words)~~\n",
        "\n",
        "- ~~[Mosaics's 30B MPT-chat](https://huggingface.co/spaces/mosaicml/mpt-30b-chat\n",
        ") (8k token input)~~\n",
        "\n",
        "- ~~[Bard](https://bard.google.com/?utm_source=welcome-email&utm_medium=email&utm_campaign=en) (I'll look up the context length later)~~\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQnbmB70zqon"
      },
      "source": [
        "## Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk2d90cCdF4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e55dd8b-8f7e-473c-ab6f-a66939c9b9b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.0/77.0 kB 1.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 28.5 MB/s eta 0:00:00\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install -U -q langchain\n",
        "# needed for scraping youtube\n",
        "pip install -q youtube-transcript-api pytube\n",
        "# for reading urls with langchain\n",
        "pip install -q unstructured\n",
        "# for the demo\n",
        "pip install -q gradio\n",
        "# for openai\n",
        "pip install -q openai tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "tmnNNc-DrrQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n75sbCqQN2PP"
      },
      "source": [
        "# Create a gradio interface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import (UnstructuredURLLoader, WebBaseLoader, YoutubeLoader)\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "import gradio as gr\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "from langchain.chains import (\n",
        "    StuffDocumentsChain,\n",
        "    LLMChain,\n",
        "    ReduceDocumentsChain,\n",
        "    MapReduceDocumentsChain,\n",
        ")\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Set up the MapReduceDocumentsChain\n",
        "# Instantiate the LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Set up the text splitter and document loader\n",
        "text_splitter = TokenTextSplitter(chunk_size=4000, chunk_overlap=0)\n",
        "\n",
        "# Setting up the MapReduceDocumentsChain\n",
        "# Setting up the map step with LLMChain\n",
        "prompt_map = PromptTemplate.from_template(\"Summarize this content: {context}\")\n",
        "llm_chain_map = LLMChain(llm=llm, prompt=prompt_map)\n",
        "\n",
        "# Setting up the reduce step\n",
        "prompt_reduce = PromptTemplate.from_template(\"Combine these summaries: {context}\")\n",
        "llm_chain_reduce = LLMChain(llm=llm, prompt=prompt_reduce)\n",
        "combine_documents_chain = StuffDocumentsChain(llm_chain=llm_chain_reduce)\n",
        "reduce_documents_chain = ReduceDocumentsChain(combine_documents_chain=combine_documents_chain, token_max=4000)\n",
        "\n",
        "# Creating the MapReduceDocumentsChain\n",
        "map_reduce_chain = MapReduceDocumentsChain(llm_chain=llm_chain_map, reduce_documents_chain=reduce_documents_chain)\n",
        "\n",
        "def scrape(url, document_loader, use_api):\n",
        "    # Load the documents based on the selected document loader\n",
        "    if document_loader == \"YoutubeLoader\":\n",
        "        loader = YoutubeLoader.from_youtube_url(url.strip(), add_video_info=True)\n",
        "        prompt = PromptTemplate.from_template('Summarize this youtube transcript with bullet points: \"{text}\"')\n",
        "    elif document_loader == \"UnstructuredURLLoader\":\n",
        "        loader = UnstructuredURLLoader(urls=[url.strip()])\n",
        "        prompt = PromptTemplate.from_template('Summarize this website article with bullet points: \"{text}\"')\n",
        "    else:\n",
        "        loader = WebBaseLoader(web_path=[url.strip()])\n",
        "        prompt = PromptTemplate.from_template('Summarize this website article with bullet points: \"{text}\"')\n",
        "\n",
        "    docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "\n",
        "    # If use_api is True, use MapReduceDocumentsChain to get the summary\n",
        "    if use_api:\n",
        "        print(len(docs))\n",
        "        # Run the MapReduceDocumentsChain\n",
        "        outputs = map_reduce_chain.run(docs)\n",
        "        # Return the summary\n",
        "        return outputs\n",
        "\n",
        "    # If use_api is False, return the scraped content as is\n",
        "    else:\n",
        "        return prompt.format(text=docs[0].page_content.strip())\n",
        "\n",
        "# Start the gradio demo\n",
        "interface = gr.Interface(fn=scrape,\n",
        "                        inputs=[gr.Textbox(placeholder=\"https://sites.google.com/view/mnovackmath/home\", label=\"Website url\"),\n",
        "                                gr.Radio(label=\"Webscraper\", choices=[\"YoutubeLoader\",\"UnstructuredURLLoader\", \"WebBaseLoader\"], value=\"YoutubeLoader\"),\n",
        "                                gr.Checkbox(label=\"Use API\"),  # Adding the checkbox here\n",
        "                                ],\n",
        "                        outputs=[gr.Textbox(label=\"Scrape results\", show_copy_button=True, max_lines=10)],\n",
        "                        title=f\"URL scraper and a MapReduceDocumentsChain\",\n",
        "                        description=\"Works best with Claude2 or ChatGPT\")\n",
        "\n",
        "# Launch the interface\n",
        "# Enabling the queue is required for inference times > 60 seconds: https://gradio.app/key-features/#queuing\n",
        "interface.queue()\n",
        "interface.launch(share=True)"
      ],
      "metadata": {
        "id": "WEjH0vOzo624",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1aa1783-c13a-4ff7-df08-e9adf3fba6da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}