{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "n75sbCqQN2PP"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andygma567/LLM-experiments/blob/main/Test_Scrapers_into_Claude.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Claude can work on 1hour videos (200k token input):\n",
        "https://www.anthropic.com/product\n",
        "\n",
        "- The next largest context that I could find is Mosaics's 30B MPT-chat (8k token input):\n",
        "https://huggingface.co/spaces/mosaicml/mpt-30b-chat\n",
        "https://huggingface.co/mosaicml/mpt-30b-chat\n",
        "\n",
        "- Chat GPT 3.5 (4k token input)\n",
        "\n",
        "---\n",
        "\n",
        "I also find that using the webscraper is better that using `ctrl+a` on a webpage."
      ],
      "metadata": {
        "id": "YbuN-8oPXnFh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQnbmB70zqon"
      },
      "source": [
        "## Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Mk2d90cCdF4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46cf83ac-36c4-4fba-9757-c733bd58fe38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 15.3 MB/s eta 0:00:00\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install -U -q langchain\n",
        "# needed for scraping youtube\n",
        "pip install -q youtube-transcript-api pytube\n",
        "# for reading urls with langchain\n",
        "pip install -q unstructured"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the youtube url"
      ],
      "metadata": {
        "id": "dgeBDyxxyu1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import YoutubeLoader\n",
        "\n",
        "URL = '''\n",
        "\n",
        "https://www.youtube.com/watch?v=BVErL_Ez9LI&list=WL&index=10\n",
        "\n",
        "'''\n",
        "\n",
        "loader = YoutubeLoader.from_youtube_url(URL.strip(), add_video_info=True,)\n",
        "docs = loader.load()\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"Summarize the following youtube transcript: {transcript}\",)\n",
        "prompt.format(transcript=docs[0].page_content)"
      ],
      "metadata": {
        "id": "IYCOcCjTxdcV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "7c0dbc5c-942a-4c5a-efdb-a139fdfd533e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Summarize the following youtube transcript: when people adopt test stream development they often make two extremely common mistakes one of them I spoke about recently that test driven development is all about unit testing they think that when you test does make a difference but this is wrong it does the other is that test driven development is all about maximizing testing and code coverage 100 is the only acceptable Target but will be pragmatic in except 80 percent this is wrong too not because a hundred percent is too much but because it isn't enough and that's our topic for today [Music] hi I'm Dave Farley of continuous delivery welcome to my channel if you haven't been here before please do hit subscribe and if you enjoy the content today hit like as well nearly everyone when first exposed to the idea of test room development assumes that coverage is the goal but actually this is almost completely missing the point but this is a nevertheless an understandable mistake but I still think it's a mistake after all when you look at teams that practice test driven development they have great test coverage so it seems like a reasonable assumption that this was their target from the start except that it wasn't and chasing coverage for the sake of coverage leads us to all sorts of problems good code coverage from our tests is a side effect of how we work not the reason that we do it this is confusing and it is a mistake that nearly all organizations adopting test driven development make but it's still a mistake in fact it's a serious enough mistake to actually prevent the outcome that we really want being able to build better software faster how on Earth does this make sense surely good coverage metrics are better than poor ones well before we answer that let me say thank you to our sponsors we're fortunate to be sponsored by equal experts trisentis and transfig all of these companies offer products and services that are well aligned with the topics that we discuss on this channel every week and have been supportive of the continuous delivery channel for a long time so if you're looking for excellence in continuous delivery and in software engineering click on the links in the description below to support them and and to say thank you do check them out so how can a better score on coverage not mean better testing well there are two parts to this the technicalities of testing and the social aspect of adopting an automated testing strategy the technicalities are pretty obvious really it's all down to the quality of the tests and specifically about the quality of the assertions that they make code coverage tools don't tell us anything about that at all code coverage works by tracking which code was executed so it's very easy to write a test that executes all of the code but does the wrong things whether we get the assertions rung by accident or on purpose I once worked with an organization that set coverage as a goal for all of their teams several hundred people to achieve 80 test coverage they incentivize the teams to hit their coverage targets even going as far to link individual programmers bonus payments with test coverage at one point guess what they hit their coverage targets and we're very proud of their coverage metrics as a result unfortunately when they looked a little bit closer over 30 percent of their tests had no assertions in them at all so this company had paid people to spend their valuable time writing tests that did nothing at all other than earn the developers in question a bonus the code could be doing anything and all that the coverage results said was yes this code has run here is my test now cool when it runs it passes and I get 100 code coverage but notice I'm not asserting anything so this test will always pass even though in this case there's no code at all other than an empty method so this test does nothing useful actually this test does nothing useful even if I do make the code do something useful the test is still passing and my coverage is still at 100 percent just as before and that stays true even when I change my ad numbers function here to actually multiply the numbers instead this is obviously a useful route if your goal is to cheat rather than to do a good job coverage is so easy to cheat this is what the team that I mentioned earlier did they simply gained the coverage metrics rather than write useful tests because they were being measured on coverage they could achieve that with a lot less effort by simply leaving out the assertions after all to make an assertion is a lot more annoying work it means understanding what the code is meant to do and then it comes with all of those pesky annoying breakages when you change things it's even worse than that though it really doesn't take much effort to write useless tests that give you 100 coverage but are much more difficult to detect because they do make assertions here's a more reasonable looking test this one's actually asserting something so cool all these right with the world okay if I run it the test passes cool then let's just take a look at the coverage 100 result nothing to see here please move along swiftly actually what is this code really doing well that looks okay but I wonder what this result thing is doing don't there we go there's that sheet another useless test but this time making useless assertions and still giving us 100 code coverage so there is a very big problem here it's easier to write tests that don't work than tests that do but working tests bear no relationship at all to test coverage this is really a problem of who guards the guards or in this case who tests the tests people have tried to address this problem technically in particular with something called mutation testing mutation tests modify the code in small ways the idea here being that we should we should force the test to fail if it now passes the tests aren't doing their job properly the problem with mutation testing at any real scale anyway is that we need the test to fail for the right sorts of reasons it's not as simple as replacing bits of the code with garbage that doesn't work at all we need it to work well enough for the tests to run but also to supply a wrong result what sorts of changes can you make to the code automatically that will force a failure like that this isn't a simple problem it's easy to imagine this being quite tricky but people have tried there's a Brute Force solution which is a little easier and that's to change the test instead of changing The Code by inverting the assertions that are made in the test if something is meant to assert that something is true make it assert that it's false Instead This is relatively easy to do and will give you a crude picture of where you stand with your tests I confess that I've never tried this for myself I have some other reservations but I can certainly imagine this being useful as a one-off or occasional check of a suspicious code base but I don't think this really gets to the real problem and neither does measuring coverage at all test room development solves this problem in a completely different way by requiring us to write the test before we write the code in a way that we can run it and see it fail in a predictable way our aim is to effectively design the test to fail in the way that we will be that will be helpful to us and then seeing that it does in fact fail in that helpful way this does allow us to test the test the fact that coverage has no relationship to the value of the test is a technical problem really but the real problem with using coverage as a metric isn't directly this technical difficulty I don't think it's more about the cultural impact so technical fixes like mutation testing aren't really addressing the problem they're not the real answer what we really want is useful tests that give us enough coverage so that we can be confident in releasing our changes when they all pass notice that I didn't say enough coverage to prove that our software is good we can never do that no amount of tests is enough as you've already seen in my simple examples we can always miss something very easily always get the test Wrong by accident or on purpose and so on so there's no proof here strictly that's not quite correct for some simple very constrained kinds of problems it is possible to prove those sorts of solutions mathematically but this is a unicorn that has been hunted for decades and has Obsessed some of the smartest computer scientists during that period practically in reality this very quickly becomes an unrealistic expectation for most systems software systems for most use cases in general are not provable in this way so if that's the case what's the point of testing and what has this got to do with coverage testing is sample a sampling process we take discrete samples of the behavior of our system and hope that that sampling is good enough to find most of the problems this is perhaps messier than a strict mathematical proof but it is how the real world works and actually it's also how maths works when it's dealing with the real world too things are fuzzy and statistical rather than precise so if tests are only samples of the behavior of our system it's sensible to have lots of samples but also a lot of good samples setting coverage as a Target does nothing to help with us identify the good samples in fact it's worse than that guess it's setting coverage as a Target often disincentivizes good testing what data there is that I'm aware of says that my experience of the organization that I mentioned earlier that incentivize the 80 coverage Target is common enough to be normal this wasn't an exceptionally bad organization this is what happens when you set up a development process with competing demands coverage on one hand and oh yeah Feature Feature Feature on the other under these conflicting pressures people will gain the coverage every time in fact they'd be rather stupid not to because that's much easier to fake than feature production even when the features are crap because they aren't being tested properly so if we want great coverage then what we really need is to change the culture change the mindset of the developers and of the organization in general chasing coverage gives us the false impression that testing is something outside of development something extra that we need to measure to force reluctant developers to comply with as long as developers see this as a chore rather than as one of the best tools in their tool kit they're missing the point and missing the value and a building worth software slower testing is at the foundation of good software development it isn't an afterthought it isn't something that somebody else's job treating it like this has been a huge mistake for the software development industry test and specifically test driven development test first automated testing is like a carpenter measuring the wood before they cut it you wouldn't trust a carpenter who didn't use a variety of measuring devices try squares rulers and miters these are the tools of the trade the tests of tester and development are exactly the same kind of tools they allow us to build better results I work as a consultant and part of my job is to quickly establish a reasonable picture of somebody else's development approach so that I can offer them my advice on how and where to improve I do sometimes ask people what their test coverage is like not because it's accurate but because it tells me something subjective about the nature of the organization or team so here's the Farley scale for test coverage not to ten percent not doing automated testing yet this is probably means that they think that they once had a test somewhere but they can't remember where it is now 10 to 20 they're probably overestimating and probably in reality they're in category one twenty to sixty percent no test driven development they either have lots of nasty coupled complex unit tests that are fragile highly coupled and and Flaky or a few nasty coupled flaky unreliable functional tests usually written who's by somebody who got a bit enthusiastic and thought that writing some automated tests would be a good idea but who now spends nearly all of their time for lonely trying to keep them working against the in the face of a sea of changes from people who don't care or don't see the results from these tests the tests provide limited value because they seemingly random significant fraction of them are broken all of the time sixty to eighty percent coverage probably an overestimate almost certainly based on unit tests written after the code almost certainly these are fragile and over coupled to the specifics of the code too if the system has been around for any amount of time it's almost certainly difficult to work on or to change because of these over coupled tests that are that you can't change anything without breaking 80 to 95 coverage they're either practicing genuine tester and development or they are chasing coverage and have tests without any assertions 95 to 100 they've missed the point and are trying too hard they probably have some decent test driven development because tdd tends to make you a bit test obsessed which is a good thing but they haven't yet seen that there's a law of diminishing returns at Play there comes a point when it's not really worth testing every single line of code they will almost certainly have some tests that are only there to hit the magic 100 Target and some of these tests are probably unusually complicated because they're unit testing every last wrinkle of the system some of which are difficult to get to okay so the Folly scale is a bit jokey this is certainly not to be relied on but this is or something very close is what goes through my head when people start talking about test coverage if your team practices tdd you will get excellent test coverage as a side effect but which much much more important than that if your team practices tdd they will better understand the problems that they're solving amplify their talent as designers because tdd encourages us to write testable code and testable code is better quality code not based on some naive measure of test coverage but because testable code is more modular more cohesive and has a better separation of concerns it has Parts delineated from one another by lines seems in the code that represent points of abstraction and has parts that are coupled in ways that better allow us to change things easily and with confidence tdd is not just about unit testing and it's certainly not about chasing test coverage but as a side effect of working this way you will get great test coverage but also great tests thank you very much thanks for watching and if you enjoy our stuff on the continuous delivery Channel please do consider supporting our work by joining our patreon community there's details to that in the description below too thank you and bye foreign [Music]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load a website url"
      ],
      "metadata": {
        "id": "Ba_2a0-5QGna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.document_loaders import (UnstructuredURLLoader, \\\n",
        "                                        WebBaseLoader, \\\n",
        "                                        )\n",
        "URL = '''\n",
        "\n",
        "https://medium.com/geekculture/enhancing-kubeflow-with-mlflow-8983373d0cac\n",
        "\n",
        "'''\n",
        "\n",
        "loader = UnstructuredURLLoader(urls=[URL.strip()])\n",
        "# loader = WebBaseLoader(web_path=[URL.strip()])\n",
        "\n",
        "docs = loader.load()\n",
        "prompt = PromptTemplate.from_template(\"Summarize this website article: {text}\",)\n",
        "prompt.format(text=docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "3eYr1KcuQO3A",
        "outputId": "ec0f5652-2c9c-4786-c497-1be0241d17dd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Summarize this website article: Enhancing Kubeflow with MLFlow\\n\\nWajeeh Ul Hassan·Follow\\n\\nPublished inGeek Culture·4 min read·Mar 6, 2021\\n\\n--\\n\\nListen\\n\\nShare\\n\\nEveryone who have understanding in machine learning understands that machine learning model development is different from traditional software engineering problems. Many tools have to the market trying to solve this problem. Before we start, check out my previous article on why we need MLOps, link to the article: https://medium.com/@wajeehulhassan/ai-is-electricity-and-mlops-is-the-transmission-line-7960e7e8c7fb\\n\\nKubeflow and MLFlow are two most renown tools in the domain. Kubeflow and MLFlow both are great tools for model deployment while Kubeflow is far more richer and provides us more components. MLFlow can be used on a local machine and on Kubernetes cluster as well but Kubeflow runs only on Kubernetes, since Kubeflow was made keeping in mind the deployment of scalable machine learning models. We can deploy MLFlow on Kubernetes as well but in this article I am going to show you how we can install MLFlow on Kubeflow and enhance the functionality of Kubeflow.\\n\\nKubeflow itself was never meant to replace any machine learning tool, rather it was built to create a scalable environment where all the tools can work in conjunction and create a smooth workflow pipeline.\\n\\nPrerequisite\\n\\nKubernetes\\n\\nKubeflow\\n\\nHelm 3\\n\\nYAML\\n\\nMLFlow\\n\\nDocker\\n\\nAssuming that you already have a running Kuberflow on premise or in the cloud (GCP, AWS, Azure). First we will make sure that all the pods are running and for doing so run the command:kubectl get pods -n kubeflow\\n\\nYou can see that the pods in my machine are about 2 months old, that’s when I recreated the Kubeflow cluster on Minikube and installed MLFlow in it.\\n\\nYou can also see that I already have MLFLow installed on my Kubeflow cluster, you won’t get the MLFlow pod when you install Kubeflow since it does not come packaged with Kubeflow.\\n\\nMLFlow components:\\n\\nMLFlow have three main components:\\n\\nMLFLow Tracking\\n\\nMLFlow Projects\\n\\nMLFlow Models\\n\\nKubeflow also provides us different components. MLFlow is comparatively easier to use, while Kubeflow provides us entrerprise grade workflow management, MLFlow lags behind in that area compared to Kubeflow. In this article we would only discuss MLFlow. We will save Kubeflow for later articles.\\n\\nFor installation of MLFlow on Kubeflow cluster which could allow us to write metadata from different docker containers and view metadata centrally. The main components of MLFlow can be found in the architecture diagram below:\\n\\nMinio (comes pre-installed with Kubeflow)\\n\\nMLFlow tracking server — (we will install this component in Kubeflow clulster)\\n\\nWe can specify MLFlow log metrics and MLFLow log parameters programmatically. We can also specify the locations of MLFlow log artifact and local artifact storage.\\n\\nInstallation of MLFLow Tracking Server on Kubeflow (Kubernetes):\\n\\nSince default use of MLFlow is for local machine, therefore MLFlow server stores MLFlow runs to either local files or SQLAlchemy compatible database but in our case we will be installing it in a Kubeflow cluster so we would be using remote server for backend.\\n\\nFor storage MLFLow has two components:\\n\\nBackend storage - (MLFlow tracking server stores experiment and run metadata as well as tags, metrics, and parameters for runs in the backend storage). Backend storage can be stored as file store or database-backed store. Here we will use file store which is part of docker image. Note: If the container restarts, the data will be lost. For longer storage we can use NFS server or database.\\n\\nArtifact storage — it has to be an storage system that can store larger files like which can be a bucket on the cloud or even NFS File system. If you want to use NFS, you might want to configure NFS first. Follow the link to install and configure NFS https://www.kubeflow.org/docs/other-guides/kubeflow-on-multinode-cluster/. In this article we will not be using NFS.\\n\\nMinio is hardware agnostic, and can make our artifact storage cloud independent. For artifact storage we will be using Minio.\\n\\nOur docker file should install mlflow, awscli, boto3, and it should expose post 5000 for communication with MLFlow tracking server. It should also have different environment variables for configuring cloud storage.\\n\\nWe have utilised a script that would run with the docker container as an entry-point.\\n\\nThe run script would create the required directory for mlflow which will be used as the backend storage, while we would also definie the s3 bucket which would be used for storing the artefact. The s3 bucket in our case is minio storage system, and minio is using the storage system that has been given to Kubeflow during its installation. The script runs the host at 0.0.0.0 so any ip can access the server at port 5000.\\n\\nNow we will build the docker container by running the build.sh script. That build script will build the Dockerfile and name it “lightbend/mlflow:1.0”.\\n\\nThat Dockerfile, run and build script can be found in the Github repository: https://github.com/wajeehulhassanvii/mlflow_on_kubeflow/\\n\\nAfter docker container has started. The chart can be installed with Helm. Helm will install MLFlow deployment server, Kubernetes service which will expose the MLFlow deployment, and Virtual Service which will expose MLFlow service to users through the Istio Ingress gateway.\\n\\nRun the following command in terminal for installing MLFlow helm chart in Kubeflow Kubernetes cluster.\\n\\nhelm install < location of helm chart>\\n\\nLink to the Github repository: https://github.com/wajeehulhassanvii/mlflow_on_kubeflow\\n\\nIn later articles, we will dive more into MLOps tools.\\n\\nMlflow\\n\\nKubeflow\\n\\nMlops\\n\\nMachine Learning\\n\\nData Science\\n\\n--\\n\\n--\\n\\nFollow\\n\\nWritten by Wajeeh Ul Hassan\\n\\n29 Followers\\n\\nWriter for\\n\\nGeek Culture\\n\\n#MLOps, Machine Learning Engineer, former Full Stack Engineer\\n\\nFollow\\n\\nMore from Wajeeh Ul Hassan and Geek Culture\\n\\nWajeeh Ul Hassan\\n\\nAI is electricity and MLOps is the transmission lineWhat is MLOps and why do we even need it?\\n\\n6 min read·Feb 9, 2021\\n\\n--\\n\\nJacob Bennett\\n\\nin\\n\\nGeek Culture\\n\\nThe 5 paid subscriptions I actually use in 2023 as a software engineerTools I use that are cheaper than Netflix\\n\\n4 min read·Mar 25\\n\\n--\\n\\n35\\n\\nArslan Ahmad\\n\\nin\\n\\nGeek Culture\\n\\nLoad Balancer vs. Reverse Proxy vs. API GatewayUnderstanding the Key Components for Efficient, Secure, and Scalable Web Applications.\\n\\n12 min read·May 17\\n\\n--\\n\\nSung Kim\\n\\nin\\n\\nGeek Culture\\n\\nList of Open Sourced Fine-Tuned Large Language Models (LLM)An incomplete list of open-sourced fine-tuned Large Language Models (LLM) you can run locally on your computer\\n\\n31 min read·Mar 30\\n\\n--\\n\\n17\\n\\nSee all from Wajeeh Ul Hassan\\n\\nSee all from Geek Culture\\n\\nRecommended from Medium\\n\\nDeepThinkers\\n\\nin\\n\\nSFU Professional Computer Science\\n\\nMLflow\\u200a—\\u200aa modern MLOps tool for data project collaborationA comprehensive introduction to MLflow and its capabilities\\n\\n16 min read·Feb 11\\n\\n--\\n\\nFelipe Melo\\n\\nin\\n\\nDev Genius\\n\\nMLflow\\u200a—\\u200aan extended “Hello World”An extended tour on concepts and examples of MLflow Tracking, Projects, Models and Model Registry\\n\\n20 min read·Feb 11\\n\\n--\\n\\nLists\\n\\nPredictive Modeling w/ Python18 stories·213 saves\\n\\nPractical Guides to Machine Learning10 stories·225 saves\\n\\nNatural Language Processing470 stories·95 saves\\n\\nNew_Reading_List174 stories·52 saves\\n\\nAssaf Pinhasi\\n\\nin\\n\\nFeature Stores for ML\\n\\nFeature pipelines and feature stores\\u200a—\\u200adeep dive into system engineering and analytical tradeoffsIntroduction\\n\\n17 min read·Dec 8, 2022\\n\\n--\\n\\nDounpct\\n\\nMlflow with Helm and serve Train Model on kubernetesPart 5: Install Mlflow on GKE Cluster with helm\\n\\n8 min read·Jun 17\\n\\n--\\n\\nLove Sharma\\n\\nin\\n\\nByteByteGo System Design Alliance\\n\\nSystem Design Blueprint: The Ultimate GuideDeveloping a robust, scalable, and efficient system can be daunting. However, understanding the key concepts and components can make the…\\n\\n9 min read·Apr 20\\n\\n--\\n\\n52\\n\\nBaşak Tuğçe Eskili\\n\\nin\\n\\nMarvelous MLOps\\n\\nThe Minimum Set of Must-Haves for MLOpsIn the previous article, we introduced MLOps maturity assessment. That assessment can also be interpreted as MLOps standards, a checklist…\\n\\n5 min read·Apr 11\\n\\n--\\n\\nSee more recommendations\\n\\nHelp\\n\\nStatus\\n\\nWriters\\n\\nBlog\\n\\nCareers\\n\\nPrivacy\\n\\nTerms\\n\\nAbout\\n\\nText to speech\\n\\nTeams'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text loading and splitting for ChatGPT"
      ],
      "metadata": {
        "id": "dXOU_nkhnAac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WybqcIpscpsO",
        "outputId": "1a9b8f87-d88c-41d0-9014-d226a39478f6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain import PromptTemplate\n",
        "from langchain.document_loaders import (UnstructuredURLLoader, \\\n",
        "                                        WebBaseLoader, \\\n",
        "                                        )\n",
        "\n",
        "text_splitter = TokenTextSplitter(chunk_size=4000, chunk_overlap=0)\n",
        "\n",
        "URL = '''\n",
        "\n",
        "https://medium.com/geekculture/enhancing-kubeflow-with-mlflow-8983373d0cac\n",
        "\n",
        "'''\n",
        "\n",
        "loader = UnstructuredURLLoader(urls=[URL.strip()])\n",
        "# loader = WebBaseLoader(web_path=[URL.strip()])\n",
        "\n",
        "docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "prompt = PromptTemplate.from_template(\"Summarize this website article with bullet points: {text}\",)\n",
        "prompt.format(text=docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Oqex1nxbnJAd",
        "outputId": "a84eb5dc-f1e4-4fca-fde9-471b7f90d1ac"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Summarize this website article with bullet points: Enhancing Kubeflow with MLFlow\\n\\nWajeeh Ul Hassan·Follow\\n\\nPublished inGeek Culture·4 min read·Mar 6, 2021\\n\\n--\\n\\nListen\\n\\nShare\\n\\nEveryone who have understanding in machine learning understands that machine learning model development is different from traditional software engineering problems. Many tools have to the market trying to solve this problem. Before we start, check out my previous article on why we need MLOps, link to the article: https://medium.com/@wajeehulhassan/ai-is-electricity-and-mlops-is-the-transmission-line-7960e7e8c7fb\\n\\nKubeflow and MLFlow are two most renown tools in the domain. Kubeflow and MLFlow both are great tools for model deployment while Kubeflow is far more richer and provides us more components. MLFlow can be used on a local machine and on Kubernetes cluster as well but Kubeflow runs only on Kubernetes, since Kubeflow was made keeping in mind the deployment of scalable machine learning models. We can deploy MLFlow on Kubernetes as well but in this article I am going to show you how we can install MLFlow on Kubeflow and enhance the functionality of Kubeflow.\\n\\nKubeflow itself was never meant to replace any machine learning tool, rather it was built to create a scalable environment where all the tools can work in conjunction and create a smooth workflow pipeline.\\n\\nPrerequisite\\n\\nKubernetes\\n\\nKubeflow\\n\\nHelm 3\\n\\nYAML\\n\\nMLFlow\\n\\nDocker\\n\\nAssuming that you already have a running Kuberflow on premise or in the cloud (GCP, AWS, Azure). First we will make sure that all the pods are running and for doing so run the command:kubectl get pods -n kubeflow\\n\\nYou can see that the pods in my machine are about 2 months old, that’s when I recreated the Kubeflow cluster on Minikube and installed MLFlow in it.\\n\\nYou can also see that I already have MLFLow installed on my Kubeflow cluster, you won’t get the MLFlow pod when you install Kubeflow since it does not come packaged with Kubeflow.\\n\\nMLFlow components:\\n\\nMLFlow have three main components:\\n\\nMLFLow Tracking\\n\\nMLFlow Projects\\n\\nMLFlow Models\\n\\nKubeflow also provides us different components. MLFlow is comparatively easier to use, while Kubeflow provides us entrerprise grade workflow management, MLFlow lags behind in that area compared to Kubeflow. In this article we would only discuss MLFlow. We will save Kubeflow for later articles.\\n\\nFor installation of MLFlow on Kubeflow cluster which could allow us to write metadata from different docker containers and view metadata centrally. The main components of MLFlow can be found in the architecture diagram below:\\n\\nMinio (comes pre-installed with Kubeflow)\\n\\nMLFlow tracking server — (we will install this component in Kubeflow clulster)\\n\\nWe can specify MLFlow log metrics and MLFLow log parameters programmatically. We can also specify the locations of MLFlow log artifact and local artifact storage.\\n\\nInstallation of MLFLow Tracking Server on Kubeflow (Kubernetes):\\n\\nSince default use of MLFlow is for local machine, therefore MLFlow server stores MLFlow runs to either local files or SQLAlchemy compatible database but in our case we will be installing it in a Kubeflow cluster so we would be using remote server for backend.\\n\\nFor storage MLFLow has two components:\\n\\nBackend storage - (MLFlow tracking server stores experiment and run metadata as well as tags, metrics, and parameters for runs in the backend storage). Backend storage can be stored as file store or database-backed store. Here we will use file store which is part of docker image. Note: If the container restarts, the data will be lost. For longer storage we can use NFS server or database.\\n\\nArtifact storage — it has to be an storage system that can store larger files like which can be a bucket on the cloud or even NFS File system. If you want to use NFS, you might want to configure NFS first. Follow the link to install and configure NFS https://www.kubeflow.org/docs/other-guides/kubeflow-on-multinode-cluster/. In this article we will not be using NFS.\\n\\nMinio is hardware agnostic, and can make our artifact storage cloud independent. For artifact storage we will be using Minio.\\n\\nOur docker file should install mlflow, awscli, boto3, and it should expose post 5000 for communication with MLFlow tracking server. It should also have different environment variables for configuring cloud storage.\\n\\nWe have utilised a script that would run with the docker container as an entry-point.\\n\\nThe run script would create the required directory for mlflow which will be used as the backend storage, while we would also definie the s3 bucket which would be used for storing the artefact. The s3 bucket in our case is minio storage system, and minio is using the storage system that has been given to Kubeflow during its installation. The script runs the host at 0.0.0.0 so any ip can access the server at port 5000.\\n\\nNow we will build the docker container by running the build.sh script. That build script will build the Dockerfile and name it “lightbend/mlflow:1.0”.\\n\\nThat Dockerfile, run and build script can be found in the Github repository: https://github.com/wajeehulhassanvii/mlflow_on_kubeflow/\\n\\nAfter docker container has started. The chart can be installed with Helm. Helm will install MLFlow deployment server, Kubernetes service which will expose the MLFlow deployment, and Virtual Service which will expose MLFlow service to users through the Istio Ingress gateway.\\n\\nRun the following command in terminal for installing MLFlow helm chart in Kubeflow Kubernetes cluster.\\n\\nhelm install < location of helm chart>\\n\\nLink to the Github repository: https://github.com/wajeehulhassanvii/mlflow_on_kubeflow\\n\\nIn later articles, we will dive more into MLOps tools.\\n\\nMlflow\\n\\nKubeflow\\n\\nMlops\\n\\nMachine Learning\\n\\nData Science\\n\\n--\\n\\n--\\n\\nFollow\\n\\nWritten by Wajeeh Ul Hassan\\n\\n29 Followers\\n\\nWriter for\\n\\nGeek Culture\\n\\n#MLOps, Machine Learning Engineer, former Full Stack Engineer\\n\\nFollow\\n\\nMore from Wajeeh Ul Hassan and Geek Culture\\n\\nWajeeh Ul Hassan\\n\\nAI is electricity and MLOps is the transmission lineWhat is MLOps and why do we even need it?\\n\\n6 min read·Feb 9, 2021\\n\\n--\\n\\nJacob Bennett\\n\\nin\\n\\nGeek Culture\\n\\nThe 5 paid subscriptions I actually use in 2023 as a software engineerTools I use that are cheaper than Netflix\\n\\n4 min read·Mar 25\\n\\n--\\n\\n35\\n\\nArslan Ahmad\\n\\nin\\n\\nGeek Culture\\n\\nLoad Balancer vs. Reverse Proxy vs. API GatewayUnderstanding the Key Components for Efficient, Secure, and Scalable Web Applications.\\n\\n12 min read·May 17\\n\\n--\\n\\nSung Kim\\n\\nin\\n\\nGeek Culture\\n\\nList of Open Sourced Fine-Tuned Large Language Models (LLM)An incomplete list of open-sourced fine-tuned Large Language Models (LLM) you can run locally on your computer\\n\\n31 min read·Mar 30\\n\\n--\\n\\n17\\n\\nSee all from Wajeeh Ul Hassan\\n\\nSee all from Geek Culture\\n\\nRecommended from Medium\\n\\nDeepThinkers\\n\\nin\\n\\nSFU Professional Computer Science\\n\\nMLflow\\u200a—\\u200aa modern MLOps tool for data project collaborationA comprehensive introduction to MLflow and its capabilities\\n\\n16 min read·Feb 11\\n\\n--\\n\\nFelipe Melo\\n\\nin\\n\\nDev Genius\\n\\nMLflow\\u200a—\\u200aan extended “Hello World”An extended tour on concepts and examples of MLflow Tracking, Projects, Models and Model Registry\\n\\n20 min read·Feb 11\\n\\n--\\n\\nLists\\n\\nPredictive Modeling w/ Python18 stories·213 saves\\n\\nPractical Guides to Machine Learning10 stories·225 saves\\n\\nNatural Language Processing470 stories·95 saves\\n\\nNew_Reading_List174 stories·52 saves\\n\\nAssaf Pinhasi\\n\\nin\\n\\nFeature Stores for ML\\n\\nFeature pipelines and feature stores\\u200a—\\u200adeep dive into system engineering and analytical tradeoffsIntroduction\\n\\n17 min read·Dec 8, 2022\\n\\n--\\n\\nDounpct\\n\\nMlflow with Helm and serve Train Model on kubernetesPart 5: Install Mlflow on GKE Cluster with helm\\n\\n8 min read·Jun 17\\n\\n--\\n\\nLove Sharma\\n\\nin\\n\\nByteByteGo System Design Alliance\\n\\nSystem Design Blueprint: The Ultimate GuideDeveloping a robust, scalable, and efficient system can be daunting. However, understanding the key concepts and components can make the…\\n\\n9 min read·Apr 20\\n\\n--\\n\\n52\\n\\nBaşak Tuğçe Eskili\\n\\nin\\n\\nMarvelous MLOps\\n\\nThe Minimum Set of Must-Haves for MLOpsIn the previous article, we introduced MLOps maturity assessment. That assessment can also be interpreted as MLOps standards, a checklist…\\n\\n5 min read·Apr 11\\n\\n--\\n\\nSee more recommendations\\n\\nHelp\\n\\nStatus\\n\\nWriters\\n\\nBlog\\n\\nCareers\\n\\nPrivacy\\n\\nTerms\\n\\nAbout\\n\\nText to speech\\n\\nTeams'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gOYO3CTopX__"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}