{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "n75sbCqQN2PP"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andygma567/LLM-experiments/blob/main/Test_LLM_Palm2_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text bison appears to work better than chat when I've tried it directly in the makersuite UI: see \"https://api.python.langchain.com/en/latest/_modules/langchain/chat_models/google_palm.html#ChatGooglePalm\", with beautiful soup, and the load_summary prompt\n",
        "\n",
        "There's an issue because it can't do map reduce\n",
        "\n",
        "Matt's website with webbasedloader gives an error for 2.5k+ chars but 2k chars works\n",
        "\n",
        "The robot Framework can work if it's broken into very small 1000 char size pieces. For some reason, 12000 characters works but 13000 does not work. 2k chars also works for this.\n",
        "\n",
        "It's so strange that the PALM API depends on the number of chars\n",
        "\n",
        "For some websites it just doesn't work for example it has trouble with the \"Medium\" blog post even in the makersuite. I tried it with 500 and 2000 chars. Nothing worked."
      ],
      "metadata": {
        "id": "MglvyLhjNm_H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQnbmB70zqon"
      },
      "source": [
        "## Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mk2d90cCdF4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bebf3dc-2299-434f-ae68-473d21ac48e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.9/122.9 kB 1.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 113.3/113.3 kB 4.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 6.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.0/90.0 kB 5.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.4/49.4 kB 2.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 9.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 12.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 21.5 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 41.4 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 44.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.9/19.9 MB 47.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.7/65.7 kB 7.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.2/294.2 kB 26.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.4/75.4 kB 8.5 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 kB 4.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.7/138.7 kB 10.5 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 4.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.5/59.5 kB 6.2 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.9/129.9 kB 13.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.4/50.4 kB 4.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.5/46.5 kB 4.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.7/43.7 kB 4.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.0/41.0 kB 3.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.0/41.0 kB 4.2 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.5/87.5 kB 9.4 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 kB 7.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 5.3 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 6.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74.5/74.5 kB 6.5 MB/s eta 0:00:00\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install -U -q google-generativeai # PALM API library\n",
        "pip install -U -q langchain\n",
        "pip install -q unstructured # for reading urls with langchain\n",
        "pip install -q transformers # needed by the summary chain\n",
        "pip install -q gradio # for the demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fc0TqP2AvX-"
      },
      "source": [
        "# Set up the langchain PALM integration\n",
        "\n",
        "To get started, you'll need to [create an API key](https://developers.generativeai.google/tutorials/setup). I'm using the [langchain integration](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.google_palm.ChatGooglePalm.html#langchain.chat_models.google_palm.ChatGooglePalm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "YE1x5qv-hka3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.llms.google_palm import GooglePalm\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "MY_API_KEY = 'AIzaSyBCopn5tdSQBN659Z_0GqvY5S-E7ywnh-4'\n",
        "os.environ['GOOGLE_API_KEY'] = MY_API_KEY\n",
        "\n",
        "llm = GooglePalm(temperature=0,\n",
        "                 max_output_tokens=1024,\n",
        "                 )\n",
        "chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrap a custom LLM"
      ],
      "metadata": {
        "id": "uZfMEfPVmDZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from typing import Any, List, Mapping, Optional\n",
        "# from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "# from langchain.llms.base import LLM\n",
        "# import google.generativeai as palm\n",
        "# from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "# palm.configure(api_key=MY_API_KEY)\n",
        "\n",
        "# class PALMLLM(LLM):\n",
        "#     temperature: float\n",
        "\n",
        "#     @property\n",
        "#     def _llm_type(self) -> str:\n",
        "#         return \"PALM\"\n",
        "\n",
        "#     def _call(\n",
        "#         self,\n",
        "#         prompt: str,\n",
        "#         stop: Optional[List[str]] = None,\n",
        "#         run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "#     ) -> str:\n",
        "#         if stop is not None:\n",
        "#             raise ValueError(\"stop kwargs are not permitted.\")\n",
        "#         completion = palm.generate_text(\n",
        "#             model='models/text-bison-001',\n",
        "#             prompt=summarize_prompt.to_string(),\n",
        "#             temperature=self.temperature,\n",
        "#             # The maximum length of the response\n",
        "#             max_output_tokens=1024,\n",
        "#             top_k=40,\n",
        "#             top_p=0.95,\n",
        "#         )\n",
        "#         return completion.result\n",
        "\n",
        "#     @property\n",
        "#     def _identifying_params(self) -> Mapping[str, Any]:\n",
        "#         \"\"\"Get the identifying parameters.\"\"\"\n",
        "#         return {\"temperature\": self.temperature}\n",
        "\n",
        "# llm = PALMLLM(temperature=0)\n",
        "# chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\")"
      ],
      "metadata": {
        "id": "BVLBbzymmGQM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try Summarization\n",
        "\n",
        "See this example: https://python.langchain.com/docs/modules/chains/popular/summarize\n",
        "\n",
        "[Reference for PALM2 models](https://developers.generativeai.google/models/language#:~:text=Note%3A%20For%20the%20PaLM%202,about%2060%2D80%20English%20words)."
      ],
      "metadata": {
        "id": "I0GCnN_9QuiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and split data"
      ],
      "metadata": {
        "id": "dgeBDyxxyu1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# PALM2 has a roughly 8k token input\n",
        "# but the PALM API can only take about 20k bytes\n",
        "# 1 bytes ~ 1 char\n",
        "# 4 char ~ 1 token\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)"
      ],
      "metadata": {
        "id": "0PPlheHJRFNA"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "from langchain.document_loaders import (UnstructuredURLLoader, \\\n",
        "                                        WebBaseLoader, \\\n",
        "                                        )\n",
        "urls = [\n",
        "    # This only works with map_reduce\n",
        "    # \"https://sites.google.com/view/mnovackmath/home\",\n",
        "    # This works better with stuff\n",
        "    # \"https://robotframework.org/robotframework/latest/RobotFrameworkUserGuide.html\",\n",
        "    # This doesn't work at all - it's because of the PALM model\n",
        "    # \"https://betterprogramming.pub/building-a-multi-document-reader-and-chatbot-with-langchain-and-chatgpt-d1864d47e339\",\n",
        "    \"https://www.gradio.app/docs/textbox\"\n",
        "    ]\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "# loader = WebBaseLoader(web_path=urls)\n",
        "\n",
        "docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "# The replace_whitespace = True is better for UnstructuredURLLoader\n",
        "# and False is better for the WebBaseLoader\n",
        "print(f\"Total number of documents: {len(docs)}\\n\")\n",
        "print(f\"Num chars per doc: {len(docs[0].page_content)}\\n\")\n",
        "print(textwrap.fill(docs[0].page_content, max_lines=10))"
      ],
      "metadata": {
        "id": "IYCOcCjTxdcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "445a95d5-dfa8-4fa7-a565-c6f57172831f"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of documents: 16\n",
            "\n",
            "Num chars per doc: 1825\n",
            "\n",
            "Building Demos  Interface  Flagging  Combining Interfaces  Blocks\n",
            "Block Layouts  ChatInterfaceNEW  Themes  Components  AnnotatedImage\n",
            "Audio  BarPlot  Button  Chatbot  Checkbox  CheckboxGroup  ClearButton\n",
            "Code  ColorPicker  Dataframe  Dataset  Dropdown  DuplicateButton  File\n",
            "Gallery  HTML  HighlightedText  Image  Interpretation  JSON  Label\n",
            "LinePlot  Markdown  Model3D  Number  Plot  Radio  ScatterPlot  Slider\n",
            "State  Textbox  Timeseries  UploadButton  Video  Helpers  Error  load\n",
            "Examples  Progress  update  make_waveform  EventData  Warning  Info\n",
            "Routes  Request  mount_gradio_app  Python Client  Client  Job\n",
            "JavaScript Client  New to Gradio? Start here: Getting Started [...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run a summarization chain"
      ],
      "metadata": {
        "id": "f68S5bXkyx3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "summarize_prompt = chain.llm_chain.prompt.format_prompt(text=docs[0].page_content)\n",
        "print(len(summarize_prompt.to_string()))\n",
        "print(textwrap.fill(summarize_prompt.to_string()))\n",
        "print()\n",
        "import google.generativeai as palm\n",
        "palm.configure(api_key=MY_API_KEY)\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model='models/text-bison-001',\n",
        "    prompt=summarize_prompt.to_string(),\n",
        "    temperature=0,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=1024,\n",
        "    top_k=40,\n",
        "    top_p=0.95,\n",
        ")\n",
        "print(textwrap.fill(completion.result))"
      ],
      "metadata": {
        "id": "euypYbcd7EZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import langchain\n",
        "import textwrap\n",
        "\n",
        "# langchain.debug=True\n",
        "response = chain.run(docs[:4])\n",
        "print(textwrap.fill(response,\n",
        "                    replace_whitespace=False,))\n",
        "\n",
        "# result for docs[:10]\n",
        "# Token indices sequence length is longer than the specified maximum sequence length for this model (1112 > 1024). Running this sequence through the model will result in indexing errors\n",
        "# Robot Framework is a Python-based, extensible keyword-driven\n",
        "# automation framework for acceptance testing, ATDD, BDD and RPA. It is\n",
        "# platform and application independent. It provides a simple library API\n",
        "# for creating customized test libraries.\n",
        "# CPU times: user 229 ms, sys: 19.9 ms, total: 249 ms\n",
        "# Wall time: 34.6 s"
      ],
      "metadata": {
        "id": "m5d57TZa2b1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "c19d7b5a-9d84-4282-cac1-b793fedae61f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             outputs = (\n\u001b[0;32m--> 252\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# Other keys are assumed to be needed for LLM prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mother_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         output, extra_return_dict = self.combine_docs(\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mother_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/map_reduce.py\u001b[0m in \u001b[0;36mcombine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mThis\u001b[0m \u001b[0mreducing\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdone\u001b[0m \u001b[0mrecursively\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneeded\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmany\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \"\"\"\n\u001b[0;32m--> 210\u001b[0;31m         map_results = self.llm_chain.apply(\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;31m# FYI - this is parallelized and so it is fast.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_variable_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mcreate_outputs\u001b[0;34m(self, llm_result)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_result\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLLMResult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;34m\"\"\"Create outputs from response.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         result = [\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0;31m# Get the text of the top generated string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             {\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;31m# Get the text of the top generated string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             {\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;34m\"full_generation\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             }\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/schema/output_parser.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mStructured\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \"\"\"\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a gradio interface\n",
        "\n",
        "I wonder if I should write pytests for this code..."
      ],
      "metadata": {
        "id": "n75sbCqQN2PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.llms.google_palm import GooglePalm\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import (UnstructuredURLLoader, \\\n",
        "                                        WebBaseLoader, \\\n",
        "                                        )\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import gradio as gr\n",
        "import textwrap\n",
        "\n",
        "# set up\n",
        "MY_API_KEY = 'AIzaSyBCopn5tdSQBN659Z_0GqvY5S-E7ywnh-4'\n",
        "os.environ['GOOGLE_API_KEY'] = MY_API_KEY\n",
        "\n",
        "llm = GooglePalm(temperature=0,\n",
        "                 max_output_tokens=1024,\n",
        "                 )\n",
        "\n",
        "# The numbers of how much to stuff and how much to map_reduce were chosen arbitrarily\n",
        "def summarize(input_text, document_loader, slide_val):\n",
        "    if document_loader==\"UnstructuredURLLoader\":\n",
        "        loader = UnstructuredURLLoader(urls=[input_text])\n",
        "    else:\n",
        "        loader = WebBaseLoader(web_path=[input_text])\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
        "    docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "    chain = load_summarize_chain(llm=llm, chain_type=\"stuff\")\n",
        "\n",
        "    output_text = chain.run(docs[:slide_val])\n",
        "\n",
        "    scraped_website_str = f\"\"\"\n",
        "    Total number of documents: {len(docs)}\n",
        "    Num chars per doc: {len(docs[0].page_content)}\n",
        "\n",
        "    {docs[0].page_content}\n",
        "    \"\"\"\n",
        "\n",
        "    return output_text, scraped_website_str"
      ],
      "metadata": {
        "id": "uGQNXVhxPVl7"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "interface = gr.Interface(fn=summarize,\n",
        "                        inputs=[gr.Textbox(placeholder=\"https://sites.google.com/view/mnovackmath/home\", default=\"\", label=\"Website url\"),\n",
        "                                gr.Radio(label=\"Webscraper\", choices=[\"UnstructuredURLLoader\", \"WebBaseLoader\"], value=\"WebBaseLoader\"),\n",
        "                                gr.Slider(minimum=1, maximum=9, value=6, step=1, label=\"Num of chars in (2k increments)\"),\n",
        "                                ],\n",
        "                        outputs=[gr.Textbox(label=\"Summary\"),\n",
        "                                 gr.Textbox(label=\"Results of url scrape\"),\n",
        "                                 ],\n",
        "                        title=f\"url PALM Summarizer with a stuff chain\",\n",
        "                        description=\"Choose a webscraper and chain type to generate a summary from a url.\")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()# debug=True) #, share=True)"
      ],
      "metadata": {
        "id": "h1zHlZA424Hl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "outputId": "51091b9d-fcde-4d26-c749-b76a6646c93f"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-92-5d5f8d49266f>:4: GradioUnusedKwargWarning: You have unused kwarg parameters in Textbox, please remove them: {'default': ''}\n",
            "  inputs=[gr.Textbox(placeholder=\"https://sites.google.com/view/mnovackmath/home\", default=\"\", label=\"Website url\"),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7877, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    }
  ]
}