{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "n75sbCqQN2PP"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andygma567/LLM-experiments/blob/main/Test_LLM_Palm2_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text bison appears to work better than chat when I've tried it directly in the makersuite UI: see \"https://api.python.langchain.com/en/latest/_modules/langchain/chat_models/google_palm.html#ChatGooglePalm\", with beautiful soup, and the load_summary prompt\n",
        "\n",
        "There's an issue because it can't do map reduce\n",
        "\n",
        "Matt's website with webbasedloader gives an error for 2.5k+ chars but 2k chars works\n",
        "\n",
        "The robot Framework can work if it's broken into very small 1000 char size pieces. For some reason, 12000 characters works but 13000 does not work. 2k chars also works for this.\n",
        "\n",
        "It's so strange that the PALM API depends on the number of chars\n",
        "\n",
        "For some websites it just doesn't work for example it has trouble with the \"Medium\" blog post even in the makersuite. I tried it with 500 and 2000 chars. Nothing worked."
      ],
      "metadata": {
        "id": "MglvyLhjNm_H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQnbmB70zqon"
      },
      "source": [
        "## Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mk2d90cCdF4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bebf3dc-2299-434f-ae68-473d21ac48e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.9/122.9 kB 1.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 113.3/113.3 kB 4.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 6.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.0/90.0 kB 5.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.4/49.4 kB 2.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 9.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 12.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 21.5 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 41.4 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 44.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.9/19.9 MB 47.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.7/65.7 kB 7.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.2/294.2 kB 26.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.4/75.4 kB 8.5 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 kB 4.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.7/138.7 kB 10.5 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 4.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.5/59.5 kB 6.2 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.9/129.9 kB 13.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.4/50.4 kB 4.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.5/46.5 kB 4.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.7/43.7 kB 4.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.0/41.0 kB 3.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.0/41.0 kB 4.2 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.5/87.5 kB 9.4 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 kB 7.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 5.3 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 6.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74.5/74.5 kB 6.5 MB/s eta 0:00:00\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install -U -q google-generativeai # PALM API library\n",
        "pip install -U -q langchain\n",
        "pip install -q unstructured # for reading urls with langchain\n",
        "pip install -q transformers # needed by the summary chain\n",
        "pip install -q gradio # for the demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fc0TqP2AvX-"
      },
      "source": [
        "# Set up the langchain PALM integration\n",
        "\n",
        "To get started, you'll need to [create an API key](https://developers.generativeai.google/tutorials/setup). I'm using the [langchain integration](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.google_palm.ChatGooglePalm.html#langchain.chat_models.google_palm.ChatGooglePalm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YE1x5qv-hka3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.llms.google_palm import GooglePalm\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "MY_API_KEY = 'AIzaSyBCopn5tdSQBN659Z_0GqvY5S-E7ywnh-4'\n",
        "os.environ['GOOGLE_API_KEY'] = MY_API_KEY\n",
        "\n",
        "llm = GooglePalm(temperature=0,\n",
        "                 max_output_tokens=1024,\n",
        "                 )\n",
        "chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrap a custom LLM"
      ],
      "metadata": {
        "id": "uZfMEfPVmDZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from typing import Any, List, Mapping, Optional\n",
        "# from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "# from langchain.llms.base import LLM\n",
        "# import google.generativeai as palm\n",
        "# from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "# palm.configure(api_key=MY_API_KEY)\n",
        "\n",
        "# class PALMLLM(LLM):\n",
        "#     temperature: float\n",
        "\n",
        "#     @property\n",
        "#     def _llm_type(self) -> str:\n",
        "#         return \"PALM\"\n",
        "\n",
        "#     def _call(\n",
        "#         self,\n",
        "#         prompt: str,\n",
        "#         stop: Optional[List[str]] = None,\n",
        "#         run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "#     ) -> str:\n",
        "#         if stop is not None:\n",
        "#             raise ValueError(\"stop kwargs are not permitted.\")\n",
        "#         completion = palm.generate_text(\n",
        "#             model='models/text-bison-001',\n",
        "#             prompt=summarize_prompt.to_string(),\n",
        "#             temperature=self.temperature,\n",
        "#             # The maximum length of the response\n",
        "#             max_output_tokens=1024,\n",
        "#             top_k=40,\n",
        "#             top_p=0.95,\n",
        "#         )\n",
        "#         return completion.result\n",
        "\n",
        "#     @property\n",
        "#     def _identifying_params(self) -> Mapping[str, Any]:\n",
        "#         \"\"\"Get the identifying parameters.\"\"\"\n",
        "#         return {\"temperature\": self.temperature}\n",
        "\n",
        "# llm = PALMLLM(temperature=0)\n",
        "# chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\")"
      ],
      "metadata": {
        "id": "BVLBbzymmGQM"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try Summarization\n",
        "\n",
        "See this example: https://python.langchain.com/docs/modules/chains/popular/summarize\n",
        "\n",
        "[Reference for PALM2 models](https://developers.generativeai.google/models/language#:~:text=Note%3A%20For%20the%20PaLM%202,about%2060%2D80%20English%20words)."
      ],
      "metadata": {
        "id": "I0GCnN_9QuiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and split data"
      ],
      "metadata": {
        "id": "dgeBDyxxyu1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# PALM2 has a roughly 8k token input\n",
        "# but the PALM API can only take about 20k bytes\n",
        "# 1 bytes ~ 1 char\n",
        "# 4 char ~ 1 token\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=19000, chunk_overlap=0)"
      ],
      "metadata": {
        "id": "0PPlheHJRFNA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "from langchain.document_loaders import (UnstructuredURLLoader, \\\n",
        "                                        WebBaseLoader, \\\n",
        "                                        )\n",
        "urls = [\n",
        "    # This only works with map_reduce\n",
        "    # \"https://sites.google.com/view/mnovackmath/home\",\n",
        "    # This works better with stuff\n",
        "    # \"https://robotframework.org/robotframework/latest/RobotFrameworkUserGuide.html\",\n",
        "    # This doesn't work at all - it's because of the PALM model\n",
        "    # \"https://betterprogramming.pub/building-a-multi-document-reader-and-chatbot-with-langchain-and-chatgpt-d1864d47e339\",\n",
        "    \"https://wandb.ai/a-sh0ts/langchain_callback_demo/reports/Prompt-Engineering-LLMs-with-LangChain-and-W-B--VmlldzozNjk1NTUw\",\n",
        "    ]\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "# loader = WebBaseLoader(web_path=urls)\n",
        "\n",
        "docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "# The replace_whitespace = True is better for UnstructuredURLLoader\n",
        "# and False is better for the WebBaseLoader\n",
        "print(f\"Total number of documents: {len(docs)}\\n\")\n",
        "print(f\"Num chars per doc: {len(docs[0].page_content)}\\n\")\n",
        "print(textwrap.fill(docs[0].page_content, max_lines=10))"
      ],
      "metadata": {
        "id": "IYCOcCjTxdcV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "7a8104dc-ab34-416d-f35b-acd2617c1cb0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of documents: 0\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f18b7b2ae35a>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# and False is better for the WebBaseLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total number of documents: {len(docs)}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Num chars per doc: {len(docs[0].page_content)}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextwrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run a summarization chain"
      ],
      "metadata": {
        "id": "f68S5bXkyx3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "summarize_prompt = chain.llm_chain.prompt.format_prompt(text=docs[0].page_content)\n",
        "print(len(summarize_prompt.to_string()))\n",
        "print(textwrap.fill(summarize_prompt.to_string()))\n",
        "print()\n",
        "import google.generativeai as palm\n",
        "palm.configure(api_key=MY_API_KEY)\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model='models/text-bison-001',\n",
        "    prompt=summarize_prompt.to_string(),\n",
        "    temperature=0,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=1024,\n",
        "    top_k=40,\n",
        "    top_p=0.95,\n",
        ")\n",
        "print(textwrap.fill(completion.result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "id": "euypYbcd7EZL",
        "outputId": "f06eb8cf-f100-4bfd-da6a-e77bfd335b75"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2032\n",
            "Write a concise summary of the following:   \"Open in app  Sign up\n",
            "Sign In  Write  Sign up  Sign In  Building a Multi-Document Reader and\n",
            "Chatbot With LangChain and ChatGPT  The best part? The chatbot will\n",
            "remember your chat history  Sami Maameri·Follow  Published inBetter\n",
            "Programming·17 min read·May 20  --  Listen  Share  Many AI products\n",
            "are coming out these days that allow you to interact with your own\n",
            "private PDFs and documents. But how do they work? And how do you build\n",
            "one? Behind the scenes, it’s actually pretty easy.  Let’s dive in!\n",
            "We’ll start with a simple chatbot that can interact with just one\n",
            "document and finish up with a more advanced chatbot that can interact\n",
            "with multiple different documents and document types, as well as\n",
            "maintain a record of the chat history, so you can ask it things in the\n",
            "context of recent conversations.  How Does It Work?  Interacting With\n",
            "a Single PDF  Interacting With a Single PDF Using Embeddings and\n",
            "Vector Stores  Adding Chat History  Interacting With Multiple\n",
            "Documents  Improvements  Summary  How Does It Work?  At a basic level,\n",
            "how does a document chatbot work? At its core, it’s just the same as\n",
            "ChatGPT. On ChatGPT, you can copy a bunch of text into the prompt, and\n",
            "then ask ChatGPT to summarise the text for you or generate some\n",
            "answers based on the text.  Interacting with a single document, such\n",
            "as a PDF, Microsoft Word, or text file, works similarly. We extract\n",
            "all of the text from the document, pass it into an LLM prompt, such as\n",
            "ChatGPT, and then ask questions about the text. This is the same way\n",
            "the ChatGPT example above works.  Interacting with multiple documents\n",
            "Where it gets a little more interesting is when the document is very\n",
            "large, or there are many documents we want to interact with. Passing\n",
            "in all of the information from these documents into a request to an\n",
            "LLM (Large Language Model) is impossible since these requests usually\n",
            "have size (token) limits, and so would only succeed if we tried to\n",
            "pass in too much information.\"   CONCISE SUMMARY:\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-160-d0403531ad69>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextwrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/textwrap.py\u001b[0m in \u001b[0;36mfill\u001b[0;34m(text, width, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[1;32m    398\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshorten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/textwrap.py\u001b[0m in \u001b[0;36mfill\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mwrapped\u001b[0m \u001b[0mparagraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \"\"\"\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/textwrap.py\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mconverted\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix_sentence_endings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fix_sentence_endings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/textwrap.py\u001b[0m in \u001b[0;36m_split_chunks\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_split_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_munge_whitespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/textwrap.py\u001b[0m in \u001b[0;36m_munge_whitespace\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \"\"\"\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_tabs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpandtabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_whitespace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0municode_whitespace_trans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'expandtabs'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import langchain\n",
        "import textwrap\n",
        "\n",
        "# langchain.debug=True\n",
        "response = chain.run(docs[:1])\n",
        "print(textwrap.fill(response,\n",
        "                    replace_whitespace=False,))\n",
        "\n",
        "# result for docs[:10]\n",
        "# Token indices sequence length is longer than the specified maximum sequence length for this model (1112 > 1024). Running this sequence through the model will result in indexing errors\n",
        "# Robot Framework is a Python-based, extensible keyword-driven\n",
        "# automation framework for acceptance testing, ATDD, BDD and RPA. It is\n",
        "# platform and application independent. It provides a simple library API\n",
        "# for creating customized test libraries.\n",
        "# CPU times: user 229 ms, sys: 19.9 ms, total: 249 ms\n",
        "# Wall time: 34.6 s"
      ],
      "metadata": {
        "id": "m5d57TZa2b1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "47f12d65-ec22-4fdc-d6e5-78a818dbb778"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robot Framework is a Python-based, extensible keyword-driven\n",
            "automation framework for acceptance testing, ATDD, BDD and RPA. It is\n",
            "platform and application independent.\n",
            "CPU times: user 115 ms, sys: 9.27 ms, total: 124 ms\n",
            "Wall time: 3.9 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nToken indices sequence length is longer than the specified maximum sequence length for this model (1112 > 1024). Running this sequence through the model will result in indexing errors\\nRobot Framework is a Python-based, extensible keyword-driven\\nautomation framework for acceptance testing, ATDD, BDD and RPA. It is\\nplatform and application independent. It provides a simple library API\\nfor creating customized test libraries.\\nCPU times: user 229 ms, sys: 19.9 ms, total: 249 ms\\nWall time: 34.6 s\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a gradio interface\n",
        "\n",
        "I wonder if I should write pytests for this code..."
      ],
      "metadata": {
        "id": "n75sbCqQN2PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.llms.google_palm import GooglePalm\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import (UnstructuredURLLoader, \\\n",
        "                                        WebBaseLoader, \\\n",
        "                                        )\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import gradio as gr\n",
        "import textwrap\n",
        "\n",
        "# set up\n",
        "MY_API_KEY = 'AIzaSyBCopn5tdSQBN659Z_0GqvY5S-E7ywnh-4'\n",
        "os.environ['GOOGLE_API_KEY'] = MY_API_KEY\n",
        "\n",
        "llm = GooglePalm(temperature=0,\n",
        "                 max_output_tokens=1024,\n",
        "                 )\n",
        "\n",
        "# This only reads the first document - stuff\n",
        "# or first handful of documents - map_reduce\n",
        "def summarize(input_text, document_loader, chain_type):\n",
        "    chain = load_summarize_chain(llm=llm, chain_type=chain_type)\n",
        "    if document_loader==\"UnstructuredURLLoader\":\n",
        "        loader = UnstructuredURLLoader(urls=[input_text])\n",
        "    else:\n",
        "        loader = WebBaseLoader(web_path=[input_text])\n",
        "\n",
        "    output_text=\"No summary available :-(\"\n",
        "\n",
        "    if chain_type==\"stuff\":\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=12000, chunk_overlap=0)\n",
        "        docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "        output_text = chain.run(docs[:1])\n",
        "    elif chain_type==\"map_reduce\":\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
        "        docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "        output_text = chain.run(docs[:15])\n",
        "\n",
        "    scraped_website_str = f\"\"\"\n",
        "    Total number of documents: {len(docs)}\n",
        "    Num chars per doc: {len(docs[0].page_content)}\n",
        "\n",
        "    {docs[0].page_content}\n",
        "    \"\"\"\n",
        "\n",
        "    return output_text, scraped_website_str"
      ],
      "metadata": {
        "id": "uGQNXVhxPVl7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "interface = gr.Interface(fn=summarize,\n",
        "                        inputs=[gr.Textbox(placeholder=\"https://sites.google.com/view/mnovackmath/home\", default=\"\", label=\"Website url\"),\n",
        "                                gr.Radio(label=\"Webscraper\", choices=[\"UnstructuredURLLoader\", \"WebBaseLoader\"], value=\"WebBaseLoader\"),\n",
        "                                gr.Radio(label=\"Chain Type\", choices=[\"map_reduce\", \"stuff\"], value=\"stuff\"),\n",
        "                                ],\n",
        "                        outputs=[\"text\",\n",
        "                                 gr.Textbox(max_lines=15, label=\"Results of url scrape\"),\n",
        "                                 ],\n",
        "                        title=f\"url PALM Summarizer (takes roughly 20 sec)\",\n",
        "                        description=\"Choose a webscraper and chain type to generate a summary from a url.\")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()# debug=True) #, share=True)"
      ],
      "metadata": {
        "id": "h1zHlZA424Hl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "outputId": "01287b6c-a0ba-4f8a-9d1e-4c4f97385b38"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-8fd49f723529>:4: GradioUnusedKwargWarning: You have unused kwarg parameters in Textbox, please remove them: {'default': ''}\n",
            "  inputs=[gr.Textbox(placeholder=\"https://sites.google.com/view/mnovackmath/home\", default=\"\", label=\"Website url\"),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7867, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}