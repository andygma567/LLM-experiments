{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andygma567/LLM-experiments/blob/main/Test_mlflow_%2B_Palm2_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a test to integration mlflow with my Langchain chains. Later I'd like to study more about this [web scraping with an LLM example](https://python.langchain.com/docs/use_cases/web_scraping/)"
      ],
      "metadata": {
        "id": "0cQhIs_sc-pU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQnbmB70zqon"
      },
      "source": [
        "## Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Mk2d90cCdF4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bed877e-3894-4b1a-8d08-ed7612a4f3b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.9/122.9 kB 1.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 113.3/113.3 kB 3.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 18.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.4/49.4 kB 5.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 5.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 358.9/358.9 kB 11.2 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/7.6 MB 15.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 22.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 39.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 41.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.3/18.3 MB 39.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.5/83.5 kB 7.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 188.8/188.8 kB 17.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.0/226.0 kB 19.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148.1/148.1 kB 11.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.2/80.2 kB 7.2 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 7.3 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.1/143.1 kB 13.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 5.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pydantic==1.* in /usr/local/lib/python3.10/dist-packages (1.10.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.*) (4.5.0)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 681.2/681.2 kB 6.8 MB/s eta 0:00:00\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install -U -q google-generativeai # PALM API library\n",
        "pip install -U -q langchain\n",
        "pip install -q unstructured # for reading urls with langchain\n",
        "pip install -q transformers # needed by the summary chain\n",
        "\n",
        "# mlflow things\n",
        "pip install -q mlflow\n",
        "pip install pydantic==1.* # test if this works with pydantic 2 later\n",
        "pip install -q pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write some environment files\n",
        "\n",
        "These are for in case I am not working inside of colab. For personal projects this is probably overkill."
      ],
      "metadata": {
        "id": "ustzhQDpn3le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a requirements.txt file\n",
        "# I don't use pip freeze > requirements.txt because\n",
        "# colab installs a ton of extra libraries that I don't actually need\n",
        "text = \"\"\"\n",
        "pandas>=1.5\n",
        "mlflow\n",
        "transformers\n",
        "langchain\n",
        "unstructured\n",
        "pydantic==1.*\n",
        "pyngrok\n",
        "google-generativeai\n",
        "\"\"\"\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oW6L8kiea6B",
        "outputId": "f7b70b6c-ebef-4313-e7a5-13cf6104c654"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "geopandas                        0.13.2\n",
            "pandas                           1.5.3\n",
            "pandas-datareader                0.10.0\n",
            "pandas-gbq                       0.17.9\n",
            "sklearn-pandas                   2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It would be interesting to see if I could install using my requirements.txt file"
      ],
      "metadata": {
        "id": "eNlheGZOnI37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a conda environment yaml - I used ChatGPT\n",
        "# I might not actually need this but I'll include it just to be safe\n",
        "# By default, conda is not install in the colab notebook - because colab runs\n",
        "# docker images\n",
        "text = \"\"\"\n",
        "name: myenv\n",
        "channels:\n",
        "  - defaults\n",
        "dependencies:\n",
        "  - python>=3.10\n",
        "  - pip\n",
        "  - pip:\n",
        "    - -r requirements.txt\n",
        "\"\"\"\n",
        "with open(\"conda.yaml\", \"w\") as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "Z1ofsqOuacIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write an MLproject file\n",
        "# it doesn't have much use because I don't have a main python script but it\n",
        "# could be useful in the future...\n",
        "text= '''\n",
        "name: mlflow + langchain experiment\n",
        "\n",
        "conda_env: conda_environment.yaml\n",
        "\n",
        "entry_points:\n",
        "  main:\n",
        "    command: \"python3 print('hello')\"\n",
        "'''\n",
        "with open(\"MLproject\", \"w\") as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "gGbkI8tLbXGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fc0TqP2AvX-"
      },
      "source": [
        "# Set up the langchain PALM integration\n",
        "\n",
        "To get started, you'll need to [create an API key](https://developers.generativeai.google/tutorials/setup). I'm using the [langchain integration](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.google_palm.ChatGooglePalm.html#langchain.chat_models.google_palm.ChatGooglePalm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YE1x5qv-hka3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.llms.google_palm import GooglePalm\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "MY_API_KEY = 'AIzaSyBCopn5tdSQBN659Z_0GqvY5S-E7ywnh-4'\n",
        "os.environ['GOOGLE_API_KEY'] = MY_API_KEY\n",
        "\n",
        "llm = GooglePalm(temperature=0,\n",
        "                 max_output_tokens=1024,\n",
        "                 )\n",
        "chain = load_summarize_chain(llm=llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try Summarization\n",
        "\n",
        "[Lang chain summarization example](https://python.langchain.com/docs/use_cases/summarization)\n",
        "\n",
        "[Reference for PALM2 models](https://developers.generativeai.google/models/language#:~:text=Note%3A%20For%20the%20PaLM%202,about%2060%2D80%20English%20words)."
      ],
      "metadata": {
        "id": "I0GCnN_9QuiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and split data"
      ],
      "metadata": {
        "id": "dgeBDyxxyu1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# PALM2 has a roughly 8k token input\n",
        "# but the PALM API can only take about 20k bytes\n",
        "# 1 bytes ~ 1 char\n",
        "# 4 char ~ 1 token\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)"
      ],
      "metadata": {
        "id": "0PPlheHJRFNA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "from langchain.document_loaders import (UnstructuredURLLoader, \\\n",
        "                                        WebBaseLoader, \\\n",
        "                                        )\n",
        "urls = [\n",
        "    # this only works with webbased I think\n",
        "    \"https://sites.google.com/view/mnovackmath/home\",\n",
        "    ]\n",
        "# loader = UnstructuredURLLoader(urls=urls)\n",
        "loader = WebBaseLoader(web_path=urls)\n",
        "\n",
        "docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "# The replace_whitespace = True is better for UnstructuredURLLoader\n",
        "# and False is better for the WebBaseLoader\n",
        "print(f\"Total number of documents: {len(docs)}\\n\")\n",
        "print(f\"Num chars per doc: {len(docs[0].page_content)}\\n\")\n",
        "print(textwrap.fill(docs[0].page_content, max_lines=10))"
      ],
      "metadata": {
        "id": "IYCOcCjTxdcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26c7952f-64c2-45ac-b8d0-09d5c734d901"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of documents: 4\n",
            "\n",
            "Num chars per doc: 1998\n",
            "\n",
            "HomeSearch this siteSkip to main contentSkip to navigationMatthew\n",
            "NovackAssistant ProfessorPurdue University, Department of\n",
            "Mathematicsmdnovack \"at\" purdue \"dot\" eduAbout MeI am an assistant\n",
            "professor in the Department of Mathematics at Purdue University.  My\n",
            "research interests lie in partial differential equations, particularly\n",
            "those arising in fluid dynamics and related fields. My CV can be found\n",
            "here.  In recent years I was a postdoc at New York University, MSRI,\n",
            "and IAS.  I completed my Ph.D. at the University of Texas-Austin in\n",
            "2019.My research is partially supported by the National Science\n",
            "Foundation, Division of Mathematical Sciences, through NSF Grant [...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run a summarization chain + mlflow\n",
        "\n",
        "This is a nice reference: [LLMOps: Experiment Tracking with MLflow for Large Language Models\n",
        "](https://dagshub.com/blog/mlflow-support-for-large-language-models/)\n",
        "\n",
        "- I need to figure out how to use the `mlflow.evaluate()` later, for now I have enough to work with and the evealuate is an experimental feature anyways\n",
        "- Maybe later I can try running the [mlflow example from the docs](https://mlflow.org/docs/latest/models.html#evaluating-with-llms)"
      ],
      "metadata": {
        "id": "f68S5bXkyx3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is some `mlflow.evaluate()` code that didn't work for me earlier\n",
        "```\n",
        "# This is formatted as code\n",
        "# try to log a table using mlflow.evaluate()\n",
        "# use model type=\"text\" bc \"summarization\" generates extra metrics\n",
        "\n",
        "# Use the pandas.DataFrame constructor to create a new DataFrame from the list of strings\n",
        "# I had to check the model signature to see that the name of the input is defaulted to\n",
        "# \"input_documents\"\n",
        "\n",
        "# For some reason this mlflow.evaluate() doesn't work for me...\n",
        "# I can double check this another time\n",
        "\n",
        "# df = pd.DataFrame(data=inputs, columns=[\"input_documents\"])\n",
        "# print(df)\n",
        "\n",
        "# mlflow.evaluate(\n",
        "#     model=logged_model.model_uri,\n",
        "#     model_type=\"text\",\n",
        "#     data=df,\n",
        "#     )\n",
        "```"
      ],
      "metadata": {
        "id": "CgA_0-gPipcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pp\n",
        "\n",
        "pp(chain)\n",
        "pp(chain.llm_chain.prompt)\n",
        "print(type(chain.llm_chain.prompt.template))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRV1GrO46gtJ",
        "outputId": "9a76e9ab-cce2-43f2-ee31-dd562d29cfc5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StuffDocumentsChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, input_key='input_documents', output_key='output_text', llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:', template_format='f-string', validate_template=True), llm=GooglePalm(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<module 'google.generativeai' from '/usr/local/lib/python3.10/dist-packages/google/generativeai/__init__.py'>, google_api_key=None, model_name='models/text-bison-001', temperature=0.0, top_p=None, top_k=None, max_output_tokens=1024, n=1), output_key='text', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], output_parser=None, partial_variables={}, template='{page_content}', template_format='f-string', validate_template=True), document_variable_name='text', document_separator='\\n\\n')\n",
            "PromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:', template_format='f-string', validate_template=True)\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# my manual test\n",
        "import langchain\n",
        "import textwrap\n",
        "import mlflow\n",
        "from pprint import pp\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "mlflow.set_tracking_uri('')\n",
        "\n",
        "experiment = mlflow.set_experiment('Langchain + mlflow')\n",
        "\n",
        "# Only the first 2k characters of Matt's webpage can be passed to the API\n",
        "# otherwise it raises an error - I have never known why this is but I assume\n",
        "# it's because the PALM API is not very good\n",
        "with mlflow.start_run():\n",
        "\n",
        "    # log the prediction\n",
        "    inputs = [docs[0].page_content]\n",
        "    outputs = [chain.run(docs[:1])]\n",
        "    prompts = [chain.llm_chain.prompt.template]\n",
        "\n",
        "    model_info = mlflow.llm.log_predictions(inputs, outputs, prompts)\n",
        "\n",
        "    # see docs:\n",
        "    # https://mlflow.org/docs/latest/python_api/mlflow.langchain.html#mlflow.langchain.log_model\n",
        "    # by default this flavor can infer the signature from the chain\n",
        "    # which appears to be good enough for my uses\n",
        "\n",
        "    # but we can also explicitly pass an input example\n",
        "    # it infers a signature from the input example\n",
        "\n",
        "    # log the model, I can use the infer signature later if I want\n",
        "    logged_model = mlflow.langchain.log_model(chain,\n",
        "                                              \"langchain_summary_chain\",\n",
        "                                              input_example=docs[0].page_content\n",
        "                                              )\n",
        "\n",
        "    # I think the artifact view for comparing runs currently only works well for\n",
        "    #  table artifacts, so I need to use the mlflow.log_table() function\n",
        "    data_dict = {\n",
        "        'prompts': prompts,\n",
        "        'inputs': inputs,\n",
        "        'outputs': outputs,\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data_dict)\n",
        "    mlflow.log_table(data=df, artifact_file=\"prediction_results.json\")"
      ],
      "metadata": {
        "id": "frqC_g9f_tYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = mlflow.last_active_run()\n",
        "\n",
        "print()\n",
        "pp(run.to_dictionary()['info'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZShBWbcihzh",
        "outputId": "c92e7f60-c6a5-44f4-ca70-74d7f8a78398"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "{'artifact_uri': 'file:///content/mlruns/478786726720344450/ea24f3bdf48f493991695f9c70cbc2c9/artifacts',\n",
            " 'end_time': 1694339524357,\n",
            " 'experiment_id': '478786726720344450',\n",
            " 'lifecycle_stage': 'active',\n",
            " 'run_id': 'ea24f3bdf48f493991695f9c70cbc2c9',\n",
            " 'run_name': 'angry-elk-661',\n",
            " 'run_uuid': 'ea24f3bdf48f493991695f9c70cbc2c9',\n",
            " 'start_time': 1694339515815,\n",
            " 'status': 'FINISHED',\n",
            " 'user_id': 'root'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the UI"
      ],
      "metadata": {
        "id": "5iFFgN69KIvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.system(\"mlflow ui &\")"
      ],
      "metadata": {
        "id": "SMkC08LJKMLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3e67914-75e8-441d-f59d-0bb9a24c361f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "# Setting the authtoken (optional)\n",
        "# Get your authtoken from https://dashboard.ngrok.com/auth\n",
        "NGROK_AUTH_TOKEN = \"2Tw0NPiESsNXEJoEZgShvindbK8_3w9U4iGq7pou7V12dDbmQ\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Open an HTTPs tunnel on port 5000 for http://localhost:5000\n",
        "public_url = ngrok.connect(\"5000\")\n",
        "\n",
        "# public_url = ngrok.connect(port=\"5000\", proto=\"http\", options={\"bind_tls\": True})\n",
        "print(\"MLflow Tracking UI:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "lN4gaFoAPLsq",
        "outputId": "a9e03f23-67d7-432a-a3fe-7b89e42f0d1f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-21860f2f09c3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyngrok\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Terminate open tunnels if exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyngrok'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}