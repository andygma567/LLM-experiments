{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andygma567/LLM-experiments/blob/main/Test_mlflow_%2B_Palm2_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a test to integration mlflow with my Langchain chains. Later I'd like to study more about this [web scraping with an LLM example](https://python.langchain.com/docs/use_cases/web_scraping/)"
      ],
      "metadata": {
        "id": "0cQhIs_sc-pU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQnbmB70zqon"
      },
      "source": [
        "## Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Mk2d90cCdF4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5880933e-c458-4c24-c11f-e781004d564d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydantic==1.* in /usr/local/lib/python3.10/dist-packages (1.10.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.*) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install -U -q google-generativeai # PALM API library\n",
        "pip install -U -q langchain\n",
        "pip install -q unstructured # for reading urls with langchain\n",
        "pip install -q transformers # needed by the summary chain\n",
        "\n",
        "# mlflow things\n",
        "pip install -q mlflow\n",
        "pip install pydantic==1.* # test if this works with pydantic 2 later\n",
        "pip install -q pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write some environment files\n",
        "\n",
        "These are for in case I am not working inside of colab. For personal projects this is probably overkill."
      ],
      "metadata": {
        "id": "ustzhQDpn3le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a requirements.txt file\n",
        "# I don't use pip freeze > requirements.txt because\n",
        "# colab installs a ton of extra libraries that I don't actually need\n",
        "text = \"\"\"\n",
        "pandas>=1.5\n",
        "mlflow\n",
        "transformers\n",
        "langchain\n",
        "unstructured\n",
        "pydantic==1.*\n",
        "pyngrok\n",
        "google-generativeai\n",
        "\"\"\"\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "5oW6L8kiea6B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It would be interesting to see if I could install using my requirements.txt file"
      ],
      "metadata": {
        "id": "eNlheGZOnI37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a conda environment yaml - I used ChatGPT\n",
        "# I might not actually need this but I'll include it just to be safe\n",
        "# By default, conda is not install in the colab notebook - because colab runs\n",
        "# docker images\n",
        "text = \"\"\"\n",
        "name: myenv\n",
        "channels:\n",
        "  - defaults\n",
        "dependencies:\n",
        "  - python>=3.10\n",
        "  - pip\n",
        "  - pip:\n",
        "    - -r requirements.txt\n",
        "\"\"\"\n",
        "with open(\"conda.yaml\", \"w\") as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "Z1ofsqOuacIE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write an MLproject file\n",
        "# it doesn't have much use because I don't have a main python script but it\n",
        "# could be useful in the future...\n",
        "text= '''\n",
        "name: mlflow + langchain experiment\n",
        "\n",
        "conda_env: conda_environment.yaml\n",
        "\n",
        "entry_points:\n",
        "  main:\n",
        "    command: \"python3 print('hello')\"\n",
        "'''\n",
        "with open(\"MLproject\", \"w\") as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "gGbkI8tLbXGa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fc0TqP2AvX-"
      },
      "source": [
        "# Set up the langchain PALM integration\n",
        "\n",
        "To get started, you'll need to [create an API key](https://developers.generativeai.google/tutorials/setup). I'm using the [langchain integration](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.google_palm.ChatGooglePalm.html#langchain.chat_models.google_palm.ChatGooglePalm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YE1x5qv-hka3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.llms.google_palm import GooglePalm\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "MY_API_KEY = 'AIzaSyBCopn5tdSQBN659Z_0GqvY5S-E7ywnh-4'\n",
        "os.environ['GOOGLE_API_KEY'] = MY_API_KEY\n",
        "\n",
        "llm = GooglePalm(temperature=0,\n",
        "                 max_output_tokens=1024,\n",
        "                 )\n",
        "chain = load_summarize_chain(llm=llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try Summarization\n",
        "\n",
        "[Lang chain summarization example](https://python.langchain.com/docs/use_cases/summarization)\n",
        "\n",
        "[Reference for PALM2 models](https://developers.generativeai.google/models/language#:~:text=Note%3A%20For%20the%20PaLM%202,about%2060%2D80%20English%20words)."
      ],
      "metadata": {
        "id": "I0GCnN_9QuiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and split data"
      ],
      "metadata": {
        "id": "dgeBDyxxyu1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# PALM2 has a roughly 8k token input\n",
        "# but the PALM API can only take about 20k bytes\n",
        "# 1 bytes ~ 1 char\n",
        "# 4 char ~ 1 token\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)"
      ],
      "metadata": {
        "id": "0PPlheHJRFNA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "from langchain.document_loaders import (UnstructuredURLLoader, \\\n",
        "                                        WebBaseLoader, \\\n",
        "                                        )\n",
        "urls = [\n",
        "    # this only works with webbased I think\n",
        "    # \"https://sites.google.com/view/mnovackmath/home\",\n",
        "    \"https://sites.google.com/view/mnovack\",\n",
        "    ]\n",
        "# loader = UnstructuredURLLoader(urls=urls)\n",
        "loader = WebBaseLoader(web_path=urls)\n",
        "\n",
        "docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "# The replace_whitespace = True is better for UnstructuredURLLoader\n",
        "# and False is better for the WebBaseLoader\n",
        "print(f\"Total number of documents: {len(docs)}\\n\")\n",
        "print(f\"Num chars per doc: {len(docs[0].page_content)}\\n\")\n",
        "print(textwrap.fill(docs[0].page_content, max_lines=10))"
      ],
      "metadata": {
        "id": "IYCOcCjTxdcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d6d0daf-d44c-4342-f8fe-f7fa236a08cc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of documents: 3\n",
            "\n",
            "Num chars per doc: 1676\n",
            "\n",
            "pytest: helps you write better programs — pytest documentation\n",
            "Navigation   modules pytest-7.4 » pytest: helps you write better\n",
            "programs        Next Open Trainings  Professional Testing with Python,\n",
            "via Python Academy, March 5th to 7th 2024 (3 day in-depth training),\n",
            "Leipzig, Germany / Remote  Also see previous talks and blogposts.\n",
            "pytest: helps you write better programs¶ The pytest framework makes it\n",
            "easy to write small, readable tests, and can scale to support complex\n",
            "functional testing for applications and libraries. pytest requires:\n",
            "Python 3.7+ or PyPy3. PyPI package name: pytest  A quick example¶ #\n",
            "content of test_sample.py def inc(x):     return x + 1   def [...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run a summarization chain + mlflow\n",
        "\n",
        "This is a nice reference: [LLMOps: Experiment Tracking with MLflow for Large Language Models\n",
        "](https://dagshub.com/blog/mlflow-support-for-large-language-models/)\n",
        "\n",
        "- I need to figure out how to use the `mlflow.evaluate()` later, for now I have enough to work with and the evealuate is an experimental feature anyways\n",
        "- Maybe later I can try running the [mlflow example from the docs](https://mlflow.org/docs/latest/models.html#evaluating-with-llms)"
      ],
      "metadata": {
        "id": "f68S5bXkyx3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is some `mlflow.evaluate()` code that didn't work for me earlier\n",
        "```\n",
        "# This is formatted as code\n",
        "# try to log a table using mlflow.evaluate()\n",
        "# use model type=\"text\" bc \"summarization\" generates extra metrics\n",
        "\n",
        "# Use the pandas.DataFrame constructor to create a new DataFrame from the list of strings\n",
        "# I had to check the model signature to see that the name of the input is defaulted to\n",
        "# \"input_documents\"\n",
        "\n",
        "# For some reason this mlflow.evaluate() doesn't work for me...\n",
        "# I can double check this another time\n",
        "\n",
        "# df = pd.DataFrame(data=inputs, columns=[\"input_documents\"])\n",
        "# print(df)\n",
        "\n",
        "# mlflow.evaluate(\n",
        "#     model=logged_model.model_uri,\n",
        "#     model_type=\"text\",\n",
        "#     data=df,\n",
        "#     )\n",
        "```"
      ],
      "metadata": {
        "id": "CgA_0-gPipcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# my manual test\n",
        "import langchain\n",
        "import textwrap\n",
        "import mlflow\n",
        "from pprint import pp\n",
        "import pandas as pd\n",
        "\n",
        "mlflow.set_tracking_uri('')\n",
        "experiment = mlflow.set_experiment('Langchain + mlflow')\n",
        "\n",
        "# Only the first 2k characters of Matt's webpage can be passed to the API\n",
        "# otherwise it raises an error - I have never known why this is but I assume\n",
        "# it's because the PALM API is not very good\n",
        "\n",
        "urls = [\n",
        "    \"https://sites.google.com/view/mnovackmath/home\",\n",
        "    \"https://sites.google.com/view/mnovack\",\n",
        "    \"https://math.gmu.edu/~scarney6/index.html\", # Sean's website\n",
        "    ]\n",
        "\n",
        "for website in urls:\n",
        "    print()\n",
        "    print(website)\n",
        "    loader = WebBaseLoader(web_path=website)\n",
        "    docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "\n",
        "    with mlflow.start_run():\n",
        "        # log the number of docs\n",
        "        params = {'num_docs': len(docs),\n",
        "                  'website': website,\n",
        "                  }\n",
        "        mlflow.log_params(params)\n",
        "\n",
        "        # log the prediction\n",
        "        inputs = [docs[0].page_content]\n",
        "        outputs = [chain.run(docs[:1])]\n",
        "        prompts = [chain.llm_chain.prompt.template]\n",
        "\n",
        "        model_info = mlflow.llm.log_predictions(inputs, outputs, prompts)\n",
        "\n",
        "        # see docs:\n",
        "        # https://mlflow.org/docs/latest/python_api/mlflow.langchain.html#mlflow.langchain.log_model\n",
        "        # by default this flavor can infer the signature from the chain\n",
        "        # which appears to be good enough for my uses\n",
        "\n",
        "        # but we can also explicitly pass an input example\n",
        "        # it infers a signature from the input example\n",
        "\n",
        "        # log the model, I can use the infer signature later if I want\n",
        "        logged_model = mlflow.langchain.log_model(chain,\n",
        "                                                  \"langchain_summary_chain\",\n",
        "                                                  input_example=docs[0].page_content\n",
        "                                                  )\n",
        "\n",
        "        # I think the artifact view for comparing runs currently only works well for\n",
        "        #  table artifacts, so I need to use the mlflow.log_table() function\n",
        "        data_dict = {\n",
        "            'prompts': prompts,\n",
        "            'inputs': inputs,\n",
        "            'outputs': outputs,\n",
        "        }\n",
        "\n",
        "        df = pd.DataFrame(data_dict)\n",
        "        mlflow.log_table(data=df, artifact_file=\"prediction_results.json\")"
      ],
      "metadata": {
        "id": "frqC_g9f_tYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c7dfb0-a04b-413d-cc84-27b9d5f0fb6d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://sites.google.com/view/mnovackmath/home\n",
            "Total number of documents: 4\n",
            "\n",
            "Num chars per doc: 1998\n",
            "\n",
            "HomeSearch this siteSkip to main contentSkip to navigationMatthew\n",
            "NovackAssistant ProfessorPurdue University, Department of\n",
            "Mathematicsmdnovack \"at\" purdue \"dot\" eduAbout MeI am an assistant\n",
            "professor in the Department of Mathematics at Purdue University.  My\n",
            "research interests lie in partial differential equations, particularly\n",
            "those arising in fluid dynamics and related fields. My CV can be found\n",
            "here.  In recent years I was a postdoc at New York University, MSRI,\n",
            "and IAS.  I completed my Ph.D. at the University of Texas-Austin in\n",
            "2019.My research is partially supported by the National Science\n",
            "Foundation, Division of Mathematical Sciences, through NSF Grant [...]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023/09/10 16:57:38 INFO mlflow.tracking.llm_utils: Creating a new llm_predictions.csv for run 6064891ec39840949c40c8e4f5b084ea.\n",
            "2023/09/10 16:57:38 WARNING mlflow: MLflow does not guarantee support for Chains outside of the subclasses of LLMChain, found StuffDocumentsChain\n",
            "2023/09/10 16:57:43 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpzcjjvdtr/model, flavor: langchain), fall back to return ['langchain==0.0.285']. Set logging level to DEBUG to see the full traceback.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://sites.google.com/view/mnovack\n",
            "Total number of documents: 2\n",
            "\n",
            "Num chars per doc: 1992\n",
            "\n",
            "Michael NovackSearch this siteSkip to main contentSkip to\n",
            "navigationMichael NovackMichael  NovackPostdoctoral Research Associate\n",
            "at Carnegie Mellon UniversityEmail address: mnovack at andrew dot cmu\n",
            "dot eduPersonal InfoI am a postdoc at Carnegie Mellon University,\n",
            "where my mentors are Irene Fonseca and Giovanni Leoni . I am\n",
            "interested in the calculus of variations, geometric measure theory,\n",
            "and partial differential equations.Previously, I was a postdoc at the\n",
            "University of Texas at Austin with Francesco Maggi and the University\n",
            "of Connecticut with Xiaodong Yan . I completed my doctoral studies at\n",
            "Indiana University under the supervision of Peter Sternberg  and [...]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023/09/10 16:57:45 INFO mlflow.tracking.llm_utils: Creating a new llm_predictions.csv for run 7f0b9be6114d4c79bf04d23ed8052bb8.\n",
            "2023/09/10 16:57:45 WARNING mlflow: MLflow does not guarantee support for Chains outside of the subclasses of LLMChain, found StuffDocumentsChain\n",
            "2023/09/10 16:57:51 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmptyqoig7t/model, flavor: langchain), fall back to return ['langchain==0.0.285']. Set logging level to DEBUG to see the full traceback.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://math.gmu.edu/~scarney6/index.html\n",
            "Total number of documents: 1\n",
            "\n",
            "Num chars per doc: 1298\n",
            "\n",
            "Sean P. Carney Profile              Sean P. Carney  Postdoctoral\n",
            "Research Fellow\n",
            "Department of Mathematical Sciences, George Mason University\n",
            "Interests  Multiscale and stochastic modeling, analysis, and\n",
            "simulation  Transport and mixing in complex and turbulent flows\n",
            "Asymptotic and numerical homogenization      Education and experience\n",
            "Postdoctoral Research Fellow, Center for Mathematics and Artificial\n",
            "Intelligence, George Mason University   Hedrick Asst. Adj. Prof.\n",
            "(2020-2023), University of California, Los Angeles Ph.D. (2020)\n",
            "University of Texas at Austin, advised by Bj&oumlrn Engquist [...]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023/09/10 16:57:53 INFO mlflow.tracking.llm_utils: Creating a new llm_predictions.csv for run 50f738bfa7274fb2909c594235f6391d.\n",
            "2023/09/10 16:57:53 WARNING mlflow: MLflow does not guarantee support for Chains outside of the subclasses of LLMChain, found StuffDocumentsChain\n",
            "2023/09/10 16:57:58 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp51we83b6/model, flavor: langchain), fall back to return ['langchain==0.0.285']. Set logging level to DEBUG to see the full traceback.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.01 s, sys: 48.8 ms, total: 1.06 s\n",
            "Wall time: 20.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DhD2sI5lAN01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the UI"
      ],
      "metadata": {
        "id": "5iFFgN69KIvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.system(\"mlflow ui &\")"
      ],
      "metadata": {
        "id": "SMkC08LJKMLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c0849c-1a3a-41b6-dd04-6a172eac70bc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "# Setting the authtoken (optional)\n",
        "# Get your authtoken from https://dashboard.ngrok.com/auth\n",
        "NGROK_AUTH_TOKEN = \"2Tw0NPiESsNXEJoEZgShvindbK8_3w9U4iGq7pou7V12dDbmQ\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Open an HTTPs tunnel on port 5000 for http://localhost:5000\n",
        "public_url = ngrok.connect(\"5000\")\n",
        "\n",
        "# public_url = ngrok.connect(port=\"5000\", proto=\"http\", options={\"bind_tls\": True})\n",
        "print(\"MLflow Tracking UI:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN4gaFoAPLsq",
        "outputId": "f4a693c8-0d18-4976-8bf9-780ad1d7f8ae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-09-10T16:19:36+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow Tracking UI: NgrokTunnel: \"https://affd-34-23-32-80.ngrok-free.app\" -> \"http://localhost:5000\"\n"
          ]
        }
      ]
    }
  ]
}