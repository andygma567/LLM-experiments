{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andygma567/LLM-experiments/blob/main/Test_mlflow_%2B_Palm2_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a test to integration mlflow with my Langchain chains. Later I'd like to study more about this [web scraping with an LLM example](https://python.langchain.com/docs/use_cases/web_scraping/)"
      ],
      "metadata": {
        "id": "0cQhIs_sc-pU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQnbmB70zqon"
      },
      "source": [
        "## Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mk2d90cCdF4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31329f2d-1d90-4740-97e9-2ac8b9b336d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.9/122.9 kB 4.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 113.3/113.3 kB 12.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 20.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.4/49.4 kB 5.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 30.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 358.9/358.9 kB 34.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/7.6 MB 79.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.8/294.8 kB 28.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 123.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 78.3 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.5/18.5 MB 30.3 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.5/83.5 kB 10.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 189.5/189.5 kB 21.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.0/226.0 kB 21.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148.1/148.1 kB 16.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.2/80.2 kB 10.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 9.2 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.1/143.1 kB 17.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 7.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pydantic==1.* in /usr/local/lib/python3.10/dist-packages (1.10.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.*) (4.5.0)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 698.7/698.7 kB 16.8 MB/s eta 0:00:00\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install -U -q google-generativeai # PALM API library\n",
        "pip install -U -q langchain\n",
        "pip install -q unstructured # for reading urls with langchain\n",
        "pip install -q transformers # needed by the summary chain\n",
        "\n",
        "# mlflow things\n",
        "pip install -q mlflow\n",
        "pip install pydantic==1.* # test if this works with pydantic 2 later\n",
        "pip install -q pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write some environment files\n",
        "\n",
        "These are for in case I am not working inside of colab. For personal projects this is probably overkill."
      ],
      "metadata": {
        "id": "ustzhQDpn3le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a requirements.txt file\n",
        "# I don't use pip freeze > requirements.txt because\n",
        "# colab installs a ton of extra libraries that I don't actually need\n",
        "text = \"\"\"\n",
        "pandas>=1.5\n",
        "mlflow\n",
        "transformers\n",
        "langchain\n",
        "unstructured\n",
        "pydantic==1.*\n",
        "pyngrok\n",
        "google-generativeai\n",
        "\"\"\"\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "5oW6L8kiea6B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It would be interesting to see if I could install using my requirements.txt file"
      ],
      "metadata": {
        "id": "eNlheGZOnI37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a conda environment yaml - I used ChatGPT\n",
        "# I might not actually need this but I'll include it just to be safe\n",
        "# By default, conda is not install in the colab notebook - because colab runs\n",
        "# docker images\n",
        "text = \"\"\"\n",
        "name: myenv\n",
        "channels:\n",
        "  - defaults\n",
        "dependencies:\n",
        "  - python>=3.10\n",
        "  - pip\n",
        "  - pip:\n",
        "    - -r requirements.txt\n",
        "\"\"\"\n",
        "with open(\"conda.yaml\", \"w\") as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "Z1ofsqOuacIE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write an MLproject file\n",
        "# it doesn't have much use because I don't have a main python script but it\n",
        "# could be useful in the future...\n",
        "text= '''\n",
        "name: mlflow + langchain experiment\n",
        "\n",
        "conda_env: conda_environment.yaml\n",
        "\n",
        "entry_points:\n",
        "  main:\n",
        "    command: \"python3 print('hello')\"\n",
        "'''\n",
        "with open(\"MLproject\", \"w\") as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "gGbkI8tLbXGa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fc0TqP2AvX-"
      },
      "source": [
        "# Set up the langchain PALM integration\n",
        "\n",
        "To get started, you'll need to [create an API key](https://developers.generativeai.google/tutorials/setup). I'm using the [langchain integration](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.google_palm.ChatGooglePalm.html#langchain.chat_models.google_palm.ChatGooglePalm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YE1x5qv-hka3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.llms.google_palm import GooglePalm\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "MY_API_KEY = 'AIzaSyBCopn5tdSQBN659Z_0GqvY5S-E7ywnh-4'\n",
        "os.environ['GOOGLE_API_KEY'] = MY_API_KEY\n",
        "\n",
        "llm = GooglePalm(temperature=0,\n",
        "                 max_output_tokens=1024,\n",
        "                 )\n",
        "chain = load_summarize_chain(llm=llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try Summarization\n",
        "\n",
        "[Lang chain summarization example](https://python.langchain.com/docs/use_cases/summarization)\n",
        "\n",
        "[Reference for PALM2 models](https://developers.generativeai.google/models/language#:~:text=Note%3A%20For%20the%20PaLM%202,about%2060%2D80%20English%20words)."
      ],
      "metadata": {
        "id": "I0GCnN_9QuiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and split data"
      ],
      "metadata": {
        "id": "dgeBDyxxyu1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# PALM2 has a roughly 8k token input\n",
        "# but the PALM API can only take about 20k bytes\n",
        "# 1 bytes ~ 1 char\n",
        "# 4 char ~ 1 token\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)"
      ],
      "metadata": {
        "id": "0PPlheHJRFNA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "from langchain.document_loaders import (UnstructuredURLLoader, \\\n",
        "                                        WebBaseLoader, \\\n",
        "                                        )\n",
        "urls = [\n",
        "    # this only works with webbased I think\n",
        "    # \"https://sites.google.com/view/mnovackmath/home\",\n",
        "    \"https://sites.google.com/view/mnovack\",\n",
        "    ]\n",
        "# loader = UnstructuredURLLoader(urls=urls)\n",
        "loader = WebBaseLoader(web_path=urls)\n",
        "\n",
        "docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "# The replace_whitespace = True is better for UnstructuredURLLoader\n",
        "# and False is better for the WebBaseLoader\n",
        "print(f\"Total number of documents: {len(docs)}\\n\")\n",
        "print(f\"Num chars per doc: {len(docs[0].page_content)}\\n\")\n",
        "print(textwrap.fill(docs[0].page_content, max_lines=10))"
      ],
      "metadata": {
        "id": "IYCOcCjTxdcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95651148-efec-47f9-8067-acb47e22757c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of documents: 2\n",
            "\n",
            "Num chars per doc: 1992\n",
            "\n",
            "Michael NovackSearch this siteSkip to main contentSkip to\n",
            "navigationMichael NovackMichael  NovackPostdoctoral Research Associate\n",
            "at Carnegie Mellon UniversityEmail address: mnovack at andrew dot cmu\n",
            "dot eduPersonal InfoI am a postdoc at Carnegie Mellon University,\n",
            "where my mentors are Irene Fonseca and Giovanni Leoni . I am\n",
            "interested in the calculus of variations, geometric measure theory,\n",
            "and partial differential equations.Previously, I was a postdoc at the\n",
            "University of Texas at Austin with Francesco Maggi and the University\n",
            "of Connecticut with Xiaodong Yan . I completed my doctoral studies at\n",
            "Indiana University under the supervision of Peter Sternberg  and [...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run a summarization chain + mlflow\n",
        "\n",
        "This is a nice reference: [LLMOps: Experiment Tracking with MLflow for Large Language Models\n",
        "](https://dagshub.com/blog/mlflow-support-for-large-language-models/)\n",
        "\n",
        "- I need to figure out how to use the `mlflow.evaluate()` later, for now I have enough to work with and the evealuate is an experimental feature anyways\n",
        "- Maybe later I can try running the [mlflow example from the docs](https://mlflow.org/docs/latest/models.html#evaluating-with-llms)"
      ],
      "metadata": {
        "id": "f68S5bXkyx3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is some `mlflow.evaluate()` code that didn't work for me earlier\n",
        "```\n",
        "# This is formatted as code\n",
        "# try to log a table using mlflow.evaluate()\n",
        "# use model type=\"text\" bc \"summarization\" generates extra metrics\n",
        "\n",
        "# Use the pandas.DataFrame constructor to create a new DataFrame from the list of strings\n",
        "# I had to check the model signature to see that the name of the input is defaulted to\n",
        "# \"input_documents\"\n",
        "\n",
        "# For some reason this mlflow.evaluate() doesn't work for me...\n",
        "# I can double check this another time\n",
        "\n",
        "# df = pd.DataFrame(data=inputs, columns=[\"input_documents\"])\n",
        "# print(df)\n",
        "\n",
        "# mlflow.evaluate(\n",
        "#     model=logged_model.model_uri,\n",
        "#     model_type=\"text\",\n",
        "#     data=df,\n",
        "#     )\n",
        "```"
      ],
      "metadata": {
        "id": "CgA_0-gPipcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# my manual test\n",
        "import langchain\n",
        "import textwrap\n",
        "import mlflow\n",
        "from pprint import pp\n",
        "import pandas as pd\n",
        "\n",
        "mlflow.set_tracking_uri('')\n",
        "experiment = mlflow.set_experiment('Langchain + mlflow')\n",
        "\n",
        "# Only the first 2k characters of Matt's webpage can be passed to the API\n",
        "# otherwise it raises an error - I have never known why this is but I assume\n",
        "# it's because the PALM API is not very good\n",
        "\n",
        "urls = [\n",
        "    \"https://sites.google.com/view/mnovackmath/home\",\n",
        "    \"https://sites.google.com/view/mnovack\",\n",
        "    \"https://math.gmu.edu/~scarney6/index.html\", # Sean's website\n",
        "    ]\n",
        "\n",
        "for website in urls:\n",
        "    print()\n",
        "    print(website)\n",
        "    loader = WebBaseLoader(web_path=website)\n",
        "    docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "\n",
        "    with mlflow.start_run():\n",
        "        # log the number of docs\n",
        "        params = {'num_docs': len(docs),\n",
        "                  'website': website,\n",
        "                  }\n",
        "        mlflow.log_params(params)\n",
        "\n",
        "        # log the prediction\n",
        "        inputs = [docs[0].page_content]\n",
        "        outputs = [chain.run(docs[:1])]\n",
        "        prompts = [chain.llm_chain.prompt.template]\n",
        "\n",
        "        model_info = mlflow.llm.log_predictions(inputs, outputs, prompts)\n",
        "\n",
        "        # see docs:\n",
        "        # https://mlflow.org/docs/latest/python_api/mlflow.langchain.html#mlflow.langchain.log_model\n",
        "        # by default this flavor can infer the signature from the chain\n",
        "        # which appears to be good enough for my uses\n",
        "        # but we can also explicitly pass an input example\n",
        "        # it infers a signature from the input example\n",
        "\n",
        "        # log the model, I can use the infer signature later if I want\n",
        "        logged_model = mlflow.langchain.log_model(chain,\n",
        "                                                  \"langchain_summary_chain\",\n",
        "                                                  )\n",
        "\n",
        "        # I think the artifact view for comparing runs currently only works well for\n",
        "        #  table artifacts, so I need to use the mlflow.log_table() function\n",
        "        data_dict = {\n",
        "            'prompts': prompts,\n",
        "            'inputs': inputs,\n",
        "            'outputs': outputs,\n",
        "        }\n",
        "\n",
        "        df = pd.DataFrame(data_dict)\n",
        "        mlflow.log_table(data=df, artifact_file=\"prediction_results.json\")"
      ],
      "metadata": {
        "id": "frqC_g9f_tYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9387b432-83b8-406e-fae1-d939434bec3e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023/09/15 23:54:22 INFO mlflow.tracking.fluent: Experiment with name 'Langchain + mlflow' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "https://sites.google.com/view/mnovackmath/home\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023/09/15 23:54:24 INFO mlflow.tracking.llm_utils: Creating a new llm_predictions.csv for run b9fe22f1c93a43b293805dabead35440.\n",
            "2023/09/15 23:54:24 WARNING mlflow: MLflow does not guarantee support for Chains outside of the subclasses of LLMChain, found StuffDocumentsChain\n",
            "2023/09/15 23:54:29 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp237qkmld/model, flavor: langchain), fall back to return ['langchain==0.0.292']. Set logging level to DEBUG to see the full traceback.\n",
            "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "https://sites.google.com/view/mnovack\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023/09/15 23:54:30 INFO mlflow.tracking.llm_utils: Creating a new llm_predictions.csv for run 7dcb5a4c40f34c379b24904bd954072e.\n",
            "2023/09/15 23:54:30 WARNING mlflow: MLflow does not guarantee support for Chains outside of the subclasses of LLMChain, found StuffDocumentsChain\n",
            "2023/09/15 23:54:38 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp_zy7sywx/model, flavor: langchain), fall back to return ['langchain==0.0.292']. Set logging level to DEBUG to see the full traceback.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "https://math.gmu.edu/~scarney6/index.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023/09/15 23:54:39 INFO mlflow.tracking.llm_utils: Creating a new llm_predictions.csv for run 427d9f5d1dea40328ce3cc46df52d55a.\n",
            "2023/09/15 23:54:39 WARNING mlflow: MLflow does not guarantee support for Chains outside of the subclasses of LLMChain, found StuffDocumentsChain\n",
            "2023/09/15 23:54:42 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpgttclcme/model, flavor: langchain), fall back to return ['langchain==0.0.292']. Set logging level to DEBUG to see the full traceback.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.06 s, sys: 414 ms, total: 2.47 s\n",
            "Wall time: 20.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Register a model"
      ],
      "metadata": {
        "id": "LNW-v-9f1p8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try out programatically registering the last run\n",
        "run = mlflow.last_active_run()\n",
        "\n",
        "mv = mlflow.register_model(f\"runs:/{run.info.run_id}/langchain_summary_chain\", \"model_A\")\n",
        "print(f\"Name: {mv.name}\")\n",
        "print(f\"Version: {mv.version}\")"
      ],
      "metadata": {
        "id": "DhD2sI5lAN01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "957c5dee-7cd1-496f-f35a-418594271e40"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Successfully registered model 'model_A'.\n",
            "2023/09/15 23:55:44 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: model_A, version 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: model_A\n",
            "Version: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Created version '1' of model 'model_A'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch the registered model"
      ],
      "metadata": {
        "id": "bvbm8Cto6BfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"model_A\"\n",
        "model_version = 1\n",
        "\n",
        "model_uri = f\"models:/{model_name}/{model_version}\"\n",
        "\n",
        "model = mlflow.pyfunc.load_model(model_uri=model_uri)\n",
        "print(model)\n",
        "\n",
        "# print the dependencies\n",
        "print()\n",
        "file_path = mlflow.pyfunc.get_model_dependencies(model_uri=model_uri, format='pip')\n",
        "with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "        print(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy1G53aZ6C8w",
        "outputId": "1aab60d1-4d0d-4643-eeae-bf2c75cdf3c1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023/09/15 23:55:52 INFO mlflow.pyfunc: To install the dependencies that were used to train the model, run the following command: '%pip install -r /content/mlruns/779359387179644347/427d9f5d1dea40328ce3cc46df52d55a/artifacts/langchain_summary_chain/requirements.txt'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mlflow.pyfunc.loaded_model:\n",
            "  artifact_path: langchain_summary_chain\n",
            "  flavor: mlflow.langchain\n",
            "  run_id: 427d9f5d1dea40328ce3cc46df52d55a\n",
            "\n",
            "\n",
            "mlflow==2.7.0\n",
            "langchain==0.0.292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have a problem with creating a pandas dataframe myself. This is also probably why I couldn't get the model evaluate to work with me.\n",
        "\n",
        "I think the problem is that the summary chain is a list of docs. I have a warning from mlflow saying: `WARNING mlflow: MLflow does not guarantee support for Chains outside of the subclasses of LLMChain, found StuffDocumentsChain`\n",
        "\n",
        "From checking the mlflow docs of `mlflow.models.infer_signature()` it appears that general object types are not supported as a datatype and from reading the examples of subclasssing the pyfunc flavor - I don't see any easy way to pass along lists of objects as input.\n",
        "\n",
        "For now, I will only use the log predictions method to log the prompts and I will be unable load and use summary chains as pyfunc models (creating a custom flavor for this was also considered but it is too much effort)."
      ],
      "metadata": {
        "id": "HZzshruxVvYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_info = mlflow.models.get_model_info(model_uri)\n",
        "print(model_info.signature)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9MKJysBSaRe",
        "outputId": "a8951a7f-3abc-4c98-f846-ffe4f1af2cbc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs: \n",
            "  ['input_documents': string]\n",
            "outputs: \n",
            "  ['output_text': string]\n",
            "params: \n",
            "  None\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the UI"
      ],
      "metadata": {
        "id": "5iFFgN69KIvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.system(\"mlflow ui &\")"
      ],
      "metadata": {
        "id": "SMkC08LJKMLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "# Setting the authtoken (optional)\n",
        "# Get your authtoken from https://dashboard.ngrok.com/auth\n",
        "NGROK_AUTH_TOKEN = \"2Tw0NPiESsNXEJoEZgShvindbK8_3w9U4iGq7pou7V12dDbmQ\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Open an HTTPs tunnel on port 5000 for http://localhost:5000\n",
        "public_url = ngrok.connect(\"5000\")\n",
        "\n",
        "# public_url = ngrok.connect(port=\"5000\", proto=\"http\", options={\"bind_tls\": True})\n",
        "print(\"MLflow Tracking UI:\", public_url)"
      ],
      "metadata": {
        "id": "lN4gaFoAPLsq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}